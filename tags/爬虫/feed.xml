<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>璐濒殉漂流记</title>
    <link>https://lawulu.github.io/tags/%E7%88%AC%E8%99%AB/feed/index.xml</link>
    <description>Recent content on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/tags/%E7%88%AC%E8%99%AB/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Phantomjs和Casperjs初窥</title>
      <link>https://lawulu.github.io/post/Phantomjs%E5%92%8CCasperjs%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Phantomjs%E5%92%8CCasperjs%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;在这个浏览器最大的时代，JS越来越重的时代，做爬虫其实最好的办法是找一个Headless浏览器，必须要支持执行JS，这样做爬虫就不用去分析HTTP和JS了。我大Java有HtmlUnit，最近尝试的是传说中的PhantomJs。话说为什么没有基于Chrome dev tools的爬虫框架呢，或许可以尝试下油猴子。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;PhantomJs提供了最原始的API，Casperjs是对PhantomJs的包装，最方便的是不用不停的写if/else（成功了执行下一步），而是用then/wait等方法增加代码的可读性和可写性。不过，Casperjs官方文档直接给出，遇到问题，应该使用PhantomJs的方式先去调用，有问题也要去PhantomJs那里去提…所以，看文档还是直接上PhantomJs吧。&lt;/p&gt;

&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;

&lt;p&gt;casperjs提供了&lt;code&gt;echo&lt;/code&gt;、&lt;code&gt;waitfor&lt;/code&gt;、&lt;code&gt;click&lt;/code&gt;等原语，所以代码大概是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;casper.thenClick(&#39;li.pg_next&#39;);

casper.then(function () {
    this.waitForSelector(&#39;li.pg_next&#39;,function () {
         this.echo(this.getHTML(&#39;.post-list&#39;));
    });
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比较苦逼的是，只有通过文件和console来跟外界交互…casperjs提供了一个server示例，直接标注说有内存溢出风险，phantomjs直接把webserver标注为不可在生产上用。&lt;/p&gt;

&lt;h2 id=&#34;debug&#34;&gt;Debug&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;casperjs --ignore-ssl-errors=yes --ssl-protocol=any  sample.js  --remote-debugger-port=9000 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候，可以在Safari里面打开，在Scripts里面设置断点，就可以Debug了。看似简单，但是很多坑在里面。&lt;/p&gt;

&lt;p&gt;首先，要在console里面执行__run()，但是如果&lt;code&gt;sample.js&lt;/code&gt;如果太复杂的话，会直接把PhantomJs搞Crash。所以可以在代码里面加上&lt;code&gt;Debugger&lt;/code&gt;，这样直接能跳过Crash的地方。
另外，PhantomJs是基于Webkit的，Chrome虽然有Webkit血统，但是只使用了起渲染HTML和CSS这一层，自己搞了一套V8去执行JS，所以，最新版本的Chrome是无法Debug PhantomJs的，最好使用血缘更近的Safari。另外再吐槽一下，PhantomJs的Bug真多，动不动就Crash。&lt;/p&gt;

&lt;p&gt;而且Console实在是不方便，所以，我觉得，最好的Debug还是用capture截图和多用log输出，例如监听资源。&lt;/p&gt;

&lt;h2 id=&#34;监听资源&#34;&gt;监听资源&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; var utils = require(&#39;utils&#39;);

casper.options.onResourceReceived = function(casperObject, res) {
     var contentType = res.contentType;
     if(contentType.indexOf(&amp;quot;json&amp;quot;)!=-1){
        console.log(utils.dump(res));

     }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以打印出所有header为&lt;code&gt;application/json&lt;/code&gt;的请求，然而没什么卵用，因为一般来说，body都被Gzip了，这里比较适合的是打印出url。&lt;/p&gt;

&lt;p&gt;另外一个坑是，PhantomJs退出时候，可能有很多Ajax请求还没有完成，所以这里要手动Wait……,不然很多请求抓不到，再所以，PhantomJs抓起来就比较慢，适合一些临时的小任务。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scrapy</title>
      <link>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Thu, 15 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;最近出于兴趣看了一下Scrapy.第一次仔细研究一个Python框架,先说结论,爬虫分为两种,一种是贪婪爬虫,最常用的就是搜索引擎,看见Url就爬.另外一种是专业爬虫,类似于网络小数定期更新,价格比对.Scrapy适合做第二种.Scrapy专心做爬取逻辑,其优势是框架比较成熟,各种插件比较全.如果真正做一个工业化的产品,Scrapy远远不够,还要攒很多东西进来.其实如果对&lt;code&gt;requests&lt;/code&gt;和&lt;code&gt;beautifulsoup&lt;/code&gt;熟悉,直接上这俩就挺好的.&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;Scrapy其实是一个很轻量级或者简单的框架.几个概念：
1. &lt;code&gt;Request&lt;/code&gt;,&lt;code&gt;Response&lt;/code&gt;,顾名思义,对应的就是网络的请求和响应.
2. &lt;code&gt;Item&lt;/code&gt;或者python dict,对应的是爬取到的结果.
3. &lt;code&gt;Pipeline&lt;/code&gt;对Item的处理.
4. &lt;code&gt;Selector&lt;/code&gt;对应的是对HTML的处理
5. &lt;code&gt;Middleware&lt;/code&gt;相当于Filter,分两种,一种是针对Spider,一种针对网络连接
6. &lt;code&gt;Scheduler&lt;/code&gt;一个深度优先的队列
7. &lt;code&gt;Feed&lt;/code&gt; 官方提供的json或者json line的输出
8. &lt;code&gt;meta&lt;/code&gt; 可以在多个Spider之间传输数据&lt;/p&gt;

&lt;p&gt;其实把文档看一遍,下一个demo,很快就可以跑起来.&lt;/p&gt;

&lt;p&gt;看图其实很清楚：
&lt;img src=&#34;https://lawulu.github.io/iimg/scrapy.png&#34; alt=&#34;Scrapy Arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从StartRequest开始,不停的返回Item或者返回Request,如果返回Item,就发到Pipeline去处理,返回Request,经过Callback处理之后,直到所有的Request都变成Item.正所谓,世界是属于Request,归根结底还是属于Item的.&lt;/p&gt;

&lt;h2 id=&#34;简单示例&#34;&gt;简单示例&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import scrapy
class ZhihuSpider(scrapy.Spider):
    name = &amp;quot;zhihu&amp;quot;
    start_urls = [
        &#39;https://www.zhihu.com/explore&#39;,
    ]

    def parse(self, response):
        # self.logger.info(&amp;quot;------zzhon中国&amp;quot;)

        for feed in response.css(&#39;.recommend-feed&#39;):
            link = response.urljoin(feed.css(&#39;a::attr(href)&#39;).extract_first());
            yield scrapy.Request(link,callback=self.parse_content)
           

##!!!  print response.css(&#39;#zh-question-detail &amp;gt; div&#39;).extract()[0].encode(&#39;utf-8&#39;)

    def parse_content(self,response):
        yield {
            &#39;title&#39;:response.css(&#39;#zh-question-title &amp;gt; h2 &amp;gt; a::text&#39;).extract_first(),
            &#39;content&#39;:response.css(&#39;#zh-question-detail &amp;gt; div::text&#39;).extract(),
            &#39;link&#39;:response.url
        }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然这个示例跑起来之前,需要改一些配置文件和设置User-agent…&lt;/p&gt;

&lt;h2 id=&#34;吐槽&#34;&gt;吐槽&lt;/h2&gt;

&lt;p&gt;Java出身的程序员的吐槽… 不用在意😜&lt;/p&gt;

&lt;h3 id=&#34;日志只打印unicode问题&#34;&gt;日志只打印Unicode问题&lt;/h3&gt;

&lt;p&gt;Debug和监控起来非常不方便,我看官方把这个issue当做wont fix了,强烈推荐直接上Python3.对非英文世界来说,Python2下scrapy shell简直没了存在的必要…&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2015-10-14 23:25:54 [scrapy] DEBUG: Scraped from &amp;lt;200 https://www.zhihu.com/question/29218955/answer/41797438&amp;gt; 
{&#39;content&#39;: [u&#39;\u666e\u6d31\u5728\u8fc7\u53bb\u5e94\u8be5\u662f\u4e0d\u4e0a\u53f0\u9762\u7684\u5427\uff1f\u4e5f\u6ca1\u95ee\u662f\u4e0d\u662f\u5c31\u95ee\u4e3a\u4ec0\u4e48\u4e86\uff0c\u5982\u679c\u95ee\u9898\u6709\u9519\u8bef\uff0c\u8bf7\u6307\u6b63&#39;], &#39;link&#39;: &#39;https://www.zhihu.com/question/19258955/answer/42797438&#39;, &#39;title&#39;: u&#39;\u666e\u6d31\u7531\u539f\u6765\u4e0d\u4e0a\u53f0\u9762\u7684\u8fb9\u9500\u8336\u5230\u5982\u4eca\u53d7\u5230\u5e7f\u5927\u8336\u53cb\u559c\u7231\u7684\u8336\u7c7b\uff0c\u9664\u4e86\u7092\u4f5c\u5916\uff0c\u666e\u6d31\u7684\u5de5\u827a\u6216\u8005\u8d28\u91cf\u6709\u4e86\u5f88\u5927\u8fdb\u6b65\u5417\uff1f&#39;} 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduler太弱&#34;&gt;Scheduler太弱&lt;/h3&gt;

&lt;p&gt;自带的scheduler只能深度优先和广度优先,官方推荐是自家的scrapyd,稍微了解了下,应该是启动时候把任务分给多个进程,进程之间好像没法交互.&lt;/p&gt;

&lt;h3 id=&#34;spider-request之间很难协同&#34;&gt;Spider/Request之间很难协同&lt;/h3&gt;

&lt;p&gt;很多时候我们需要某几个Request应该在一个组里面,同一个组的Request可以共用Cookie,共用代理.这类的需求可以通过meta机制来实现…由于Scrapy的Request是无脑yield出来的,实现出来肯定很丑陋.&lt;/p&gt;

&lt;h3 id=&#34;callback-hell&#34;&gt;Callback hell&lt;/h3&gt;

&lt;p&gt;通过Callback来绑定处理方法,入口和可测试性很差,怪不得要加入Contract机制&lt;/p&gt;

&lt;h3 id=&#34;容错&#34;&gt;容错&lt;/h3&gt;

&lt;p&gt;容错机制很少,需要把Task持久化,官方提供的持久化到disk的方案感觉很初级很粗糙.&lt;/p&gt;

&lt;h2 id=&#34;splash&#34;&gt;Splash&lt;/h2&gt;

&lt;p&gt;Pyspider是用的&lt;code&gt;PhantomJS&lt;/code&gt;,Scrapy提供了一个基于Docker和Lua的Splash.
这货的问题是Splash没有保持渲染状态的方法,对于复杂的JS交互,一般的做法是一个lua脚本执行很多次,最后返回所有的结果.这样爬取的颗粒比较大,如果中间出现异常,不好处理,而且调用&lt;code&gt;splash::wait()&lt;/code&gt;总感觉不靠谱,没有提供Dom加载之后的回调吗？&lt;/p&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;Splash可以执行Lua脚本,官方封装了Splash类,可以和JS交互.这样,就可可以找到Dom中的Button,直接调用click事件,然后返回某些DOM中的Html或者Json.&lt;/p&gt;

&lt;h3 id=&#34;lua&#34;&gt;Lua&lt;/h3&gt;

&lt;p&gt;Lua 很值得一学,Nginx,游戏引擎,Redis都在用……
快速入门：&lt;a href=&#34;http://tylerneylon.com/a/learn-lua/&#34;&gt;http://tylerneylon.com/a/learn-lua/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lua稍微复杂并且可以玩出花的内置机制就是Metatable了,可以重载对字典(对象)各种操作符.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>