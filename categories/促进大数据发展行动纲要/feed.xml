<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>璐濒殉漂流记</title>
    <link>https://lawulu.github.io/categories/%E4%BF%83%E8%BF%9B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%91%E5%B1%95%E8%A1%8C%E5%8A%A8%E7%BA%B2%E8%A6%81/feed/index.xml</link>
    <description>Recent content on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/categories/%E4%BF%83%E8%BF%9B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%91%E5%B1%95%E8%A1%8C%E5%8A%A8%E7%BA%B2%E8%A6%81/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>小陈与大数据的故事</title>
      <link>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</guid>
      <description>

&lt;p&gt;&lt;em&gt;本篇为公众号&lt;code&gt;码农翻身&lt;/code&gt;的约稿&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;在学校&#34;&gt;在学校&lt;/h2&gt;

&lt;p&gt;做为一个科班出身的程序员，在小陈还不明白自己所学的的数据结构和操作系统这些专业课跟未来的工作有什么关系，甚至未来做什么也不清楚时候，小陈最喜欢做的事就是看电影，看完还舍不得删。那个时代流行的还是“为人不识武藤兰，阅遍电影也枉然”，电影还有很多是rm格式的，几百兆都算大的，但是架不住小陈对一衣带水邻国文化的热爱，只有120G的硬盘让他很快有了另一个爱好：整理硬盘，每次有新资源出来，小陈都得想办法腾点空间出来。&lt;/p&gt;

&lt;p&gt;后来教育网里一个叫Maze的软件在我校流行开来，这个软件可以在局域网内以P2P的方式完成资源共享。喜大普奔，小陈得意洋洋的宣布自己终于不用整理硬盘了，自己硬盘里面只存一部分经典的，想看什么电影从别人机器里面拖，哪怕自己临时要看一个美帝大片，删掉一部分电影，删掉之后都还能从别人机器里面拖回来。&lt;/p&gt;

&lt;p&gt;直到有一天，小陈临时删了一位老师的代表作，等想找回来时候，不知道是学校调整了网络路由还是因为高年级学生的离校，这部本来在Maze中非常常见的电影居然一个都找不到了。小陈被室友嘲笑之余，怒而卸载了Maze，并决定要开发一套更好的共享资源软件：要有一个元数据服务器，所有使用者的硬盘的读写都由元数据服务器来管理，元数据保证每份电影都至少存在2份以上，而不是像现在这样保存什么电影，全靠用户心照不宣的默契和自己的兴趣。&lt;/p&gt;

&lt;p&gt;不过小陈也面临着毕业，自然没有时间去搞这些乱七八糟的东西，为了找工作，开始恶补各种技能。这已经是2011年，大数据已经开始火起来了，小陈开始试着学习大数据的知识。搞Android培训的那波人摇身一变开始培训Hadoop了，小陈上了一次试听课之后，回来开始向室友生成，自己的创意被Google和Hadoop给剽窃了！大数据首先要解决的都是存储问问题，GFS，以及受之启发的Hadoop HDFS，完全就是照着自己的资源共享软件思路来的。&lt;/p&gt;

&lt;p&gt;其实小陈知道，Google在2003年就把GFS的论文给写出来，开发这个GFS时候武老师可能还没有出道，甚至BigData这个词还没有被清楚的定义。但是Google作为技术最领先，体量最大的搜索引擎公司，很早就面临和自己类似的问题:大量的数据(信息索引和网页快照等等)要怎么存储存储在哪里的问题。Google选择的是用大量的通用硬件组成一个可以横向扩展的集群去对存储和处理这些海量的数据。这种英雄所见略同的感觉，让小陈决定要从事大数据开发，毕竟这个越来越自我的世界，随着移动互联网的爆发，每个人制造的垃圾数据会越来越容易，而制造垃圾数据的人也会越来越多。&lt;/p&gt;

&lt;h2 id=&#34;第一份工作&#34;&gt;第一份工作&lt;/h2&gt;

&lt;p&gt;找大数据的工作并不顺利，心仪的公司嫌自己没有经验，给Offer的公司貌似没有多少数据，只是想趁着大数据热忽悠一把。阴错阳差，小陈找到一份Java开发的工作。这家公司专做某行业的软件，公司不大但发展很快，待遇还算不错。小陈第一周上班主要学习公司的技术框架，然后就懵逼了。面试时候非要分清POBOVO，现在项目所有的参数传递都是List&lt;Map&gt;；面试时候各种设计模式的问，结果这项目的大部分的业务逻辑都是写在SQL存储过程里面的，甚至JS里面也夹杂了很多SQL；自研的一个工作流框架对应的库表，各种预留字段，从field1，field2一直到field8。在小陈犹豫着要不要换家公司，技术总监的几句话给忽悠住了他：这个世界的所有事物，即所谓的对象，都是可以用List&lt;Map&gt;来表示；SQL是史上的最好的DSL，一定要充分利用，如果想写，用SQL实现一个Dijkstra算法都可以[注1]；这框架虽然不走寻常路，但是有PHP的开发速度，也利用JVM的性能和Java的生态系统。&lt;/p&gt;

&lt;p&gt;小陈不仅觉得总监的话很有道理，而且这些话跟大数据的思路也有几份相似。Google开启大数据时代最重要的就是三篇论文:Google File System,讲如何分布式存储大数据；Google MapReduce,讲如何分布式计算大数据；Google Bigtable,实现了一个数据库，以便在分布式下快速读写结构化数据。对应的开源实现是Hadoop Hdfs，Hadoop MapReduce，HBase。这个识万物为List&lt;Map&gt;的思路不就是MapReduce的思路吗，而那个预留字段的数据库设计其实有点Bigtable/HBase的风范，不管神似与否至少形似。工作一段时间，小陈感觉这糙快猛的框架确实还是很适合这个需求变化比较快的行业，对他自己来说，至少写SQL和实现业务需求的能力提高了不少。一年之后，小陈跳槽到一家三线互联网公司做大数据，凭借的就是这两项能力。&lt;/p&gt;

&lt;h2 id=&#34;大数据工作和两个领导&#34;&gt;大数据工作和两个领导&lt;/h2&gt;

&lt;p&gt;新公司工作主要基于Shell调用Hive，把数据从日志往Oracle里面导，经常还要给业务部门提数。所以，虽然在做大数据，工作内容只是从上家公司的Oracle PL/SQL换成了Hive SQL。Map-Reduce虽然直指大数据计算的本源，但是奈何这个世界太复杂，产品经理的需求太多，表达力有点欠缺。而且又很多通用的需求，例如数据之间的Join和Group，完全可以提炼出来，于是Facebook向开源社区捐献了Hive，用史上的最好的DSL作为查询语言交给熟悉SQL的人去使用，Hive引擎负责把SQL翻译成MapReduce。小陈得偿所愿进入大数据行业，但是很快就厌倦了：Shell管理起来太麻烦，跟其他系统交互起来不方便；Hive运行太慢，资源利用率低，有时候跑了十几分钟结果出来了发现自己的SQL有一个简单的错误；想推动HadoopV2的落地，让Yarn去合理的管理集群资源，领导和同事都不愿意承担风险。小陈隐约觉得自己做的东西并没有给公司带来应有的价值，只是让报表的维度增加一些。领导告诉小陈，大数据并不是解决一切问题的灵丹妙药，数据越来越多，增加更多的是无价值的数据，而大数据处理的日志等数据与业务数据相比，信息熵要低的多。&lt;/p&gt;

&lt;p&gt;就这么工作一年后，老板也对数据部门不满意，赶走了老领导，挖来一个大厂的架构师来主管整合所有数据相关业务。新领导上来就大刀阔斧，部门架构调整，把一部分数据相关的开发并了过来，整个部门一分为二，一部分往上跟业务和产品部门打交道，一部分下沉做大数据平台时候。小陈果断的选择了去做平台。&lt;/p&gt;

&lt;p&gt;如果说老领导强调的是数据完整性和正确性，数据对业务的支撑，新领导更强调数据对产品和决策的反馈，技术的先进性。按照新领导的意思，以做用户数据仓库为出发点，从数据收集、数据转换、数据模型、数据查询、数据可视化到任务调度和平台监控，要重新梳理现有每一个模块的技术架构。这个时候，大数据相关技术已经可以说是百花齐放百家争鸣，围绕着Hadoop，几乎每个领域都有一两个出色的解决方案。领导要求使用技术要向代表技术先进性，相关软件版本落后不能超过2年，鼓励使用3年以内的新项目。小陈很高兴，感觉可以大干一场了。因为态度积极，小陈很受新领导的器重，小陈先负责用其他SQL on Hadoop的如Impala/Presto来代替Hive的部分功能，提高查询效率，后来又负责引入实时计算框架Storm，虽然只是用来计算PV/UV，但好歹是实时的。总之是趟坑无数，背锅无数，忙得不亦乐乎。&lt;/p&gt;

&lt;p&gt;直到又过一年公司裁员，小陈才发现数据部门是重灾区。和领导吃散伙饭，小陈开始鸣不平，觉得咱们的平台做了这么多，大家加班这么辛苦，为什么就没人懂欣赏呢。领导却直言自己早就知道这个结局，部门折腾了一年，老板只看到数据部门人员和机器一直在增加，却经常收到其他部门各种抱怨，产品抱怨进度太慢啊，商务说数据不懂业务啊，研发说总是要配合数据做这做那啊，运营说报表没有老系统方便数据老出问题等等。相处这么久，领导已经把小陈当心腹了，直言一开始想把有些业务拿到数据这边统一处理，技术出身搞不定公司复杂的关系，阻力太大；埋头把平台理顺，而在强势老板的公司里面，自己还是没有抓住老板的需求和业务的痛点，所以对老板来说，确实没为公司带来什么价值。领导的这番肺腑之言对小陈触动很大，开始重新审视自己的过去的工作。不管怎样小陈这时候找下一份工作已经有了一份资本。&lt;/p&gt;

&lt;h2 id=&#34;在创业公司&#34;&gt;在创业公司&lt;/h2&gt;

&lt;p&gt;在全民创业的浪潮中，小陈来到一家创业公司，这里还没有什么&amp;rdquo;大数据&amp;rdquo;,但是CEO的数据要介入整个产品和运营中的承诺和希望打动了小陈，而且这里有机会从零开始有机会按照自己的思路规划整个公司的大数据业务。这个时候，大数据圈最火的是Spark，Spark以基于内存的DAG计算引擎为基础，提供了批量计算，实时计算，交互式查询，机器学习，基本上实现了One Stack To Rule Them All，而且Spark也有SparkSQL。小陈选择了围绕Spark来构建整个系统，并且拥抱云计算了，直接选择云服务，希望自己能专注于让数据产生价值。小陈工作分这么几部分:输出报表数据;给一个数据科学家打杂，帮他整理数据提取特征，实现一些算法;每天花一个小时看数据，解释数据和用数据去解释。小陈成了对公司业务和系统最了解的人，比产品和QA都要了解，并且提出了很多优化和改进。CEO很满意，年终时候大笔一挥给了更多的期权，然而没什么卵用，因为融资寒冬来的时候，公司还是倒了。&lt;/p&gt;

&lt;h2 id=&#34;在大厂&#34;&gt;在大厂&lt;/h2&gt;

&lt;p&gt;小陈已经拖家带口了，感觉自己不再年轻，在机构Offer之间犹豫了很久之后，选择了去一个大厂做螺丝钉，这里流程和工具都已经非常完善了，用的Hadoop都是自己定制甚至已经跟社区不兼容了。小陈业务时间乐于参加各种线下各种活动，积攒自己的人脉，坚持更新Blog，提升自己的名气。&lt;/p&gt;

&lt;p&gt;这个时候Google推出了Beam计算框架。其实Google抛出了三大论文之后，表面上很沉寂，自己内部的系统却一直在向前演进，例如老版的GFS已经被Colossus被代替。直到Google眼红AWS投身云计算时候，发现客户都更习惯开源的那套东西，甚至Google把自己的BigTable服务往外卖的时候，不得不兼容BigTable的山寨产品HBase的API。这就好比一个欧洲人找到一个中国建筑师建一个亚洲古代建筑，中国建筑师给建了一个唐朝风格的建筑，欧洲人说，你这建的不大对，我在日本看到的都是这样那样。Google为了争夺话语权，把自己最新的产品拿出来开源，要一统所有的计算框架，不管你底层是Storm还是Spark，按照Beam的API来处理数据，底层可以随意切换Spark和自家的Cloud Dataflow，甚至为了师出有因，大力提携Flink和Spark打擂台。小陈相信这个框架会一统江湖，为了自己的KPI和升级, 准备在公司推Apache Beam。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imply和Confluent破解</title>
      <link>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</guid>
      <description>

&lt;p&gt;其实这两个软件还是比较容易破解的，首先开发者并没有把精力放到反破解上面，相关功能做的很简单。第二没有服务器交互的反破解还是很难的，即时是有服务器交互(例如Idea)，也有人搞出各种License Server出来&lt;/p&gt;

&lt;h2 id=&#34;imply-pivot&#34;&gt;Imply Pivot&lt;/h2&gt;

&lt;p&gt;搜索&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -ri &#39;Pivot evaluation license expired&#39; .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现有个文件&lt;code&gt;./node_modules/@implydata/im-auth/build/server/utils/license-manager/license-manager.js&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;里面有这个逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        var e = path.join(this.varDir, &amp;quot;.pivot-first-run&amp;quot;);
        return Q(fs.readFile(e, {encoding: &amp;quot;utf-8&amp;quot;})).then(function (e) {
            var r = new Date(e.trim());
            if (isNaN(r)) throw new Error(&amp;quot;invalid date&amp;quot;);
            return {created: false, firstRun: r}
        }).catch(function (r) {
            var t = new Date;
            return Q(fs.writeFile(e, t.toISOString())).then(function () {
                return {created: true, firstRun: t}
            })
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到文件&lt;code&gt;find . -name &amp;quot;.imply-first-run&amp;quot;&lt;/code&gt;
把里面日期改一下就行了&lt;/p&gt;

&lt;h2 id=&#34;confluent&#34;&gt;Confluent&lt;/h2&gt;

&lt;p&gt;坑一：一开始搞反了..一直没报错 还以为没生效&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jar uf /service/app/confluent-4.0.0/share/java/confluent-control-center/control-center-4.0.0.jar io/confluent/controlcenter/license/LicenseModule.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;坑二：zsh ll不显示隐藏文件，习惯把ll = ls-al 而不是ls -ah&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; return License.baseClaims(&amp;quot;demo&amp;quot;, bfa.creationTime().toMillis() + TimeUnit.DAYS.toMillis(30L), true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以其实修改该文件的createTime就行了..&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CDN流量日志简单分析</title>
      <link>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;公司运营的很大一个成本就是CDN耗费。对比Impression和CDN消耗量就会很奇怪，不该有这么多的CDN消耗的。&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;下载并处理cdn原始日志&#34;&gt;下载并处理CDN原始日志&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;由于官方提供的合并下载日志工具在Mac下不能用，随机选取一个小时的日志：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;wget https://cdnlog.cn-hangzhou.oss.aliyun-inc.com/streaming.lawulu.com/2017_03_04/streaming.lawulu.com_2017_03_04_1200_1300.gz?spm=5176.8232200.log.d10.83JI60&amp;amp;OSSAccessKeyId=xxxxx&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Python处理分析为CSV&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;pat = ( r&#39;\[(.+)\]\s&#39; #datetime
           &#39;(\d+.\d+.\d+.\d+)\s&#39; #IP address
           &#39;-\s&#39; #proxy ip
            &#39;\d+\s&#39; #responseTime
           &#39;&amp;quot;.+&amp;quot;\s&#39; #referrer
           &#39;&amp;quot;(GET\s\S+mp4)&amp;quot;\s&#39;  # requested file
            &#39;\d+\s&#39; #status
           &#39;\d+\s&#39; #requestSIze
           &#39;(\d+)\s&#39; #responseSize
           &#39;\D+\s&#39; #HIT OR NOT
           &#39;&amp;quot;(.+)&amp;quot;\s&#39; #user agent
           &#39;&amp;quot;\S+&amp;quot;&#39; #contentType
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@see &lt;a href=&#34;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&#34;&gt;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;导入数据库&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA INFILE &#39;cdnlogs.csv&#39; 
INTO TABLE test.cdn_log
FIELDS TERMINATED BY &#39;,&#39; 
LINES TERMINATED BY &#39;\n&#39; 
(date_time,ip,url,rsp_size,ua);
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;数据校验&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;SELECT count(*)  FROM test.cdn_log 
&#39;206619&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cat streaming.lawulu.com_2017_03_04_1200_1300 | wc -l

206618
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;用sql分析&#34;&gt;用Sql分析&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) =1 )a


&#39;110643&#39;,&#39;598635187514&#39;


select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) &amp;gt;1 )a


&#39;32670&#39;, &#39;440010318426&#39;

0.4236
select 440010318426/(598635187514+440010318426) from dual 

select sum(rsp_size) from  cdn_log

1038645505940

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;由于IP,UA,URL可以唯一确定一次UV，我们公司有30%的用户占用了40%的流量，对同一个视频会不停的反复下载。
最后查明，是SDK的bug，正在下载视频时候，如果切换到后台，下载会直接停止。所以会反复下载视频，浪费流量。&lt;/p&gt;

&lt;h2 id=&#34;校验&#34;&gt;校验&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;计算ResponseSize之后和Console里面同一时间段的&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;
CURL  -i -X HEAD &#39;https://streaming.lawulu.com/default/256b9db1-a1ce-4983-8906-4568fa7a9c75-144.mp4&#39;
&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>广协的IP库到底准不住</title>
      <link>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;对于CPM广告，地域定向是非常重要的。广告协会提供了ip库，方便AdNetwork和第三方监控做地域定向。百度默认以ip138做为其&amp;rdquo;IP&amp;rdquo;关键字的第一个入口，我相信ip138的库是比较准的，那广协的IP库和ip138的库差别大吗？&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;随机从线上提取10000个ip-并使用广协库解析成对应城市&#34;&gt;随机从线上提取10000个ip，并使用广协库解析成对应城市&lt;/h3&gt;

&lt;p&gt;从某个时段原始Nginx日志中的ip(300W+)随机取10000条
使用pandas&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;csv = pd.read_csv(&#39;/tmp/ip.log&#39;)
np.savetxt(&#39;/tmp/ipSample.log&#39;,csv.sample(10000).values,fmt=&#39;%s&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;根据ip获取ip138对应的城市&#34;&gt;根据ip获取ip138对应的城市&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# get data from web
def get_data(query):
    url = &amp;quot;http://m.ip138.com/ip.asp?ip=&amp;quot; + query
    res = requests.get(url)
    print &#39;get data for &#39;+ query +&#39; result:&#39; + str(res.status_code)
    if(res.status_code != 200):
        return &#39;Failed Response&#39;
    else:
        return parse_html(res.content)

def parse_html(html):
    res = BeautifulSoup(html,&amp;quot;html.parser&amp;quot;).find_all(&amp;quot;p&amp;quot;, &amp;quot;result&amp;quot;)
    result =split(res[0])
    if len(res)&amp;gt;1:
        result=result+&amp;quot;,&amp;quot;+ split(res[1])
    return result

def split(str):
    try:
        splits=str.string.encode(&amp;quot;utf-8&amp;quot;).split(&#39;：&#39;)
        return splits[1]
    except RuntimeError:
        return &#39;Split Error:&#39;+str

with open(&#39;/tmp/ipSample.log&#39;) as source:
    with open(&#39;ip138Result.csv&#39;,&#39;aw&#39;) as result:
        for line in source:
            ip = line.strip(&#39;\n&#39;)
            ipStr=get_data(ip)
            time.sleep(0.1)
            result.write(ip+&#39;|&#39;+ipStr+&#39;\n&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;取全国60强城市-统计两个结果的差距&#34;&gt;取全国60强城市，统计两个结果的差距&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.mnw.cn/news/china/886245.html&#34;&gt;http://www.mnw.cn/news/china/886245.html&lt;/a&gt; 60强城市&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;citys=[&#39;上海&#39;,&#39;北京&#39;,&#39;广州&#39;,&#39;深圳&#39;,&#39;成都&#39;,&#39;重庆&#39;,&#39;杭州&#39;,&#39;南京&#39;,&#39;沈阳&#39;,&#39;苏州&#39;,&#39;天津&#39;,&#39;武汉&#39;,&#39;西安&#39;,&#39;长沙&#39;,&#39;大连&#39;,&#39;济南&#39;,&#39;宁波&#39;,&#39;青岛&#39;,&#39;无锡&#39;,&#39;厦门&#39;,&#39;郑州&#39;,&#39;长春&#39;,&#39;常州&#39;,&#39;哈尔滨&#39;,&#39;福州&#39;,&#39;昆明&#39;,&#39;合肥&#39;,&#39;东莞&#39;,&#39;石家庄&#39;,&#39;呼和浩特&#39;,&#39;南昌&#39;,&#39;温州&#39;,&#39;佛山&#39;,&#39;贵阳&#39;,&#39;南宁&#39;,&#39;海口&#39;,&#39;湖州&#39;,&#39;唐山&#39;,&#39;临沂&#39;,&#39;嘉兴&#39;,&#39;绍兴&#39;,&#39;南通&#39;,&#39;徐州&#39;,&#39;泉州&#39;,&#39;太原&#39;,&#39;烟台&#39;,&#39;乌鲁木齐&#39;,&#39;潍坊&#39;,&#39;珠海&#39;,&#39;洛阳&#39;,&#39;中山&#39;,&#39;兰州&#39;,&#39;金华&#39;,&#39;淮安&#39;,&#39;吉林&#39;,&#39;威海&#39;,&#39;淄博&#39;,&#39;银川&#39;,&#39;扬州&#39;,&#39;芜湖&#39;,&#39;盐城&#39;,&#39;宜昌&#39;,&#39;西宁&#39;,&#39;襄阳&#39;,&#39;绵阳&#39;]
files =[&amp;quot;ip138Result.csv&amp;quot;,&amp;quot;ipResultV2.log&amp;quot;]
import subprocess, datetime, sys
def checkNums(file,city):
    p = subprocess.Popen(&amp;quot;&amp;quot;&amp;quot; cat &amp;quot;&amp;quot;&amp;quot; + file + &amp;quot;&amp;quot;&amp;quot;| grep &amp;quot;&amp;quot;&amp;quot;+ city +&amp;quot;&amp;quot;&amp;quot;  |wc -l &amp;quot;&amp;quot;&amp;quot;, shell=True, stdout=subprocess.PIPE );
    p.wait()
    out = p.stdout.readlines()[0]
    return str(out).strip()

for city in citys:
    result = city + &amp;quot;\t&amp;quot;
    for file in files:
        result = result +checkNums(file,city)+ &amp;quot;\t&amp;quot;
    print  result
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;北京相差比较大，系统库比Ip138库多出一倍&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其他城市 上海 深圳 南京 苏州 大连 宁波等也相差在20%以上。
可以说跟IP138差别还是比较大的。&lt;/p&gt;

&lt;p&gt;但是三线城市比较准。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hive引擎往Solr写数据</title>
      <link>https://lawulu.github.io/post/Hive%E5%BC%95%E6%93%8E%E5%BE%80Solr%E5%86%99%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Hive%E5%BC%95%E6%93%8E%E5%BE%80Solr%E5%86%99%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;p&gt;公司有一批结果是通过Hive跑SQL跑出来的，但是需要最终写到Solr里面，方面别的服务使用。
灵感来自
&lt;a href=&#34;https://chimpler.wordpress.com/2013/03/20/playing-with-apache-hive-and-solr/&#34;&gt;https://chimpler.wordpress.com/2013/03/20/playing-with-apache-hive-and-solr/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Fork之后，做了一些修改：主要是针对Hive0.12和Solr新版本做的一些升级。&lt;a href=&#34;https://github.com/lawulu/hive-solr/&#34;&gt;https://github.com/lawulu/hive-solr/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>