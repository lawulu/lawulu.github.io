<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>璐濒殉漂流记</title>
    <link>https://lawulu.github.io/tags/python/feed/index.xml</link>
    <description>Recent content on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/tags/python/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>CDN流量日志简单分析</title>
      <link>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</guid>
      <description>

&lt;h3 id=&#34;缘起&#34;&gt;缘起&lt;/h3&gt;

&lt;p&gt;公司运营的很大一个成本就是CDN耗费。对比Impression和Click&lt;/p&gt;

&lt;h3 id=&#34;数据分析&#34;&gt;数据分析&lt;/h3&gt;

&lt;h4 id=&#34;下载并处理cdn原始日志&#34;&gt;下载并处理CDN原始日志&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;由于官方提供的合并下载日志工具在Mac下不能用，随机选取一个小时的日志：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;wget https://cdnlog.cn-hangzhou.oss.aliyun-inc.com/streaming.lawulu.com/2017_03_04/streaming.lawulu.com_2017_03_04_1200_1300.gz?spm=5176.8232200.log.d10.83JI60&amp;amp;OSSAccessKeyId=xxxxx&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Python处理分析为CSV&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;pat = ( r&#39;\[(.+)\]\s&#39; #datetime
           &#39;(\d+.\d+.\d+.\d+)\s&#39; #IP address
           &#39;-\s&#39; #proxy ip
            &#39;\d+\s&#39; #responseTime
           &#39;&amp;quot;.+&amp;quot;\s&#39; #referrer
           &#39;&amp;quot;(GET\s\S+mp4)&amp;quot;\s&#39;  # requested file
            &#39;\d+\s&#39; #status
           &#39;\d+\s&#39; #requestSIze
           &#39;(\d+)\s&#39; #responseSize
           &#39;\D+\s&#39; #HIT OR NOT
           &#39;&amp;quot;(.+)&amp;quot;\s&#39; #user agent
           &#39;&amp;quot;\S+&amp;quot;&#39; #contentType
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@see &lt;a href=&#34;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&#34;&gt;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;导入数据库&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA INFILE &#39;cdnlogs.csv&#39; 
INTO TABLE test.cdn_log
FIELDS TERMINATED BY &#39;,&#39; 
LINES TERMINATED BY &#39;\n&#39; 
(date_time,ip,url,rsp_size,ua);
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;数据校验&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;SELECT count(*)  FROM test.cdn_log 
&#39;206619&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cat streaming.lawulu.com_2017_03_04_1200_1300 | wc -l

206618
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;用sql分析&#34;&gt;用Sql分析&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) =1 )a


&#39;110643&#39;,&#39;598635187514&#39;


select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) &amp;gt;1 )a


&#39;32670&#39;, &#39;440010318426&#39;

0.4236
select 440010318426/(598635187514+440010318426) from dual 

select sum(rsp_size) from  cdn_log

1038645505940

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;结论&#34;&gt;结论&lt;/h3&gt;

&lt;p&gt;由于IP,UA,URL可以唯一确定一次UV，我们公司有30%的用户占用了40%的流量，对同一个视频会不停的反复下载。
最后查明，是SDK的bug，正在下载视频时候，如果切换到后台，下载会直接停止。所以会反复下载视频，浪费流量。&lt;/p&gt;

&lt;h3 id=&#34;校验&#34;&gt;校验&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;计算ResponseSize之后和Console里面同一时间段的&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;
CURL  -i -X HEAD &#39;https://streaming.lawulu.com/default/256b9db1-a1ce-4983-8906-4568fa7a9c75-144.mp4&#39;
&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Scrapy</title>
      <link>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Thu, 15 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;最近出于兴趣看了一下Scrapy.第一次仔细研究一个Python框架,先说结论,爬虫分为两种,一种是贪婪爬虫,最常用的就是搜索引擎,看见Url就爬.另外一种是专业爬虫,类似于网络小数定期更新,价格比对.Scrapy适合做第二种.Scrapy专心做爬取逻辑,其优势是框架比较成熟,各种插件比较全.如果真正做一个工业化的产品,Scrapy远远不够,还要攒很多东西进来.其实如果对&lt;code&gt;requests&lt;/code&gt;和&lt;code&gt;beautifulsoup&lt;/code&gt;熟悉,直接上这俩就挺好的.&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;Scrapy其实是一个很轻量级或者简单的框架.几个概念：
1. &lt;code&gt;Request&lt;/code&gt;,&lt;code&gt;Response&lt;/code&gt;,顾名思义,对应的就是网络的请求和响应.
2. &lt;code&gt;Item&lt;/code&gt;或者python dict,对应的是爬取到的结果.
3. &lt;code&gt;Pipeline&lt;/code&gt;对Item的处理.
4. &lt;code&gt;Selector&lt;/code&gt;对应的是对HTML的处理
5. &lt;code&gt;Middleware&lt;/code&gt;相当于Filter,分两种,一种是针对Spider,一种针对网络连接
6. &lt;code&gt;Scheduler&lt;/code&gt;一个深度优先的队列
7. &lt;code&gt;Feed&lt;/code&gt; 官方提供的json或者json line的输出
8. &lt;code&gt;meta&lt;/code&gt; 可以在多个Spider之间传输数据&lt;/p&gt;

&lt;p&gt;其实把文档看一遍,下一个demo,很快就可以跑起来.&lt;/p&gt;

&lt;p&gt;看图其实很清楚：
&lt;img src=&#34;https://lawulu.github.io/iimg/scrapy.png&#34; alt=&#34;Scrapy Arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从StartRequest开始,不停的返回Item或者返回Request,如果返回Item,就发到Pipeline去处理,返回Request,经过Callback处理之后,直到所有的Request都变成Item.正所谓,世界是属于Request,归根结底还是属于Item的.&lt;/p&gt;

&lt;h2 id=&#34;简单示例&#34;&gt;简单示例&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import scrapy
class ZhihuSpider(scrapy.Spider):
    name = &amp;quot;zhihu&amp;quot;
    start_urls = [
        &#39;https://www.zhihu.com/explore&#39;,
    ]

    def parse(self, response):
        # self.logger.info(&amp;quot;------zzhon中国&amp;quot;)

        for feed in response.css(&#39;.recommend-feed&#39;):
            link = response.urljoin(feed.css(&#39;a::attr(href)&#39;).extract_first());
            yield scrapy.Request(link,callback=self.parse_content)
           

##!!!  print response.css(&#39;#zh-question-detail &amp;gt; div&#39;).extract()[0].encode(&#39;utf-8&#39;)

    def parse_content(self,response):
        yield {
            &#39;title&#39;:response.css(&#39;#zh-question-title &amp;gt; h2 &amp;gt; a::text&#39;).extract_first(),
            &#39;content&#39;:response.css(&#39;#zh-question-detail &amp;gt; div::text&#39;).extract(),
            &#39;link&#39;:response.url
        }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然这个示例跑起来之前,需要改一些配置文件和设置User-agent…&lt;/p&gt;

&lt;h2 id=&#34;吐槽&#34;&gt;吐槽&lt;/h2&gt;

&lt;p&gt;Java出身的程序员的吐槽… 不用在意😜&lt;/p&gt;

&lt;h3 id=&#34;日志只打印unicode问题&#34;&gt;日志只打印Unicode问题&lt;/h3&gt;

&lt;p&gt;Debug和监控起来非常不方便,我看官方把这个issue当做wont fix了,强烈推荐直接上Python3.对非英文世界来说,Python2下scrapy shell简直没了存在的必要…&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2015-10-14 23:25:54 [scrapy] DEBUG: Scraped from &amp;lt;200 https://www.zhihu.com/question/29218955/answer/41797438&amp;gt; 
{&#39;content&#39;: [u&#39;\u666e\u6d31\u5728\u8fc7\u53bb\u5e94\u8be5\u662f\u4e0d\u4e0a\u53f0\u9762\u7684\u5427\uff1f\u4e5f\u6ca1\u95ee\u662f\u4e0d\u662f\u5c31\u95ee\u4e3a\u4ec0\u4e48\u4e86\uff0c\u5982\u679c\u95ee\u9898\u6709\u9519\u8bef\uff0c\u8bf7\u6307\u6b63&#39;], &#39;link&#39;: &#39;https://www.zhihu.com/question/19258955/answer/42797438&#39;, &#39;title&#39;: u&#39;\u666e\u6d31\u7531\u539f\u6765\u4e0d\u4e0a\u53f0\u9762\u7684\u8fb9\u9500\u8336\u5230\u5982\u4eca\u53d7\u5230\u5e7f\u5927\u8336\u53cb\u559c\u7231\u7684\u8336\u7c7b\uff0c\u9664\u4e86\u7092\u4f5c\u5916\uff0c\u666e\u6d31\u7684\u5de5\u827a\u6216\u8005\u8d28\u91cf\u6709\u4e86\u5f88\u5927\u8fdb\u6b65\u5417\uff1f&#39;} 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduler太弱&#34;&gt;Scheduler太弱&lt;/h3&gt;

&lt;p&gt;自带的scheduler只能深度优先和广度优先,官方推荐是自家的scrapyd,稍微了解了下,应该是启动时候把任务分给多个进程,进程之间好像没法交互.&lt;/p&gt;

&lt;h3 id=&#34;spider-request之间很难协同&#34;&gt;Spider/Request之间很难协同&lt;/h3&gt;

&lt;p&gt;很多时候我们需要某几个Request应该在一个组里面,同一个组的Request可以共用Cookie,共用代理.这类的需求可以通过meta机制来实现…由于Scrapy的Request是无脑yield出来的,实现出来肯定很丑陋.&lt;/p&gt;

&lt;h3 id=&#34;callback-hell&#34;&gt;Callback hell&lt;/h3&gt;

&lt;p&gt;通过Callback来绑定处理方法,入口和可测试性很差,怪不得要加入Contract机制&lt;/p&gt;

&lt;h3 id=&#34;容错&#34;&gt;容错&lt;/h3&gt;

&lt;p&gt;容错机制很少,需要把Task持久化,官方提供的持久化到disk的方案感觉很初级很粗糙.&lt;/p&gt;

&lt;h2 id=&#34;splash&#34;&gt;Splash&lt;/h2&gt;

&lt;p&gt;Pyspider是用的&lt;code&gt;PhantomJS&lt;/code&gt;,Scrapy提供了一个基于Docker和Lua的Splash.
这货的问题是Splash没有保持渲染状态的方法,对于复杂的JS交互,一般的做法是一个lua脚本执行很多次,最后返回所有的结果.这样爬取的颗粒比较大,如果中间出现异常,不好处理,而且调用&lt;code&gt;splash::wait()&lt;/code&gt;总感觉不靠谱,没有提供Dom加载之后的回调吗？&lt;/p&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;Splash可以执行Lua脚本,官方封装了Splash类,可以和JS交互.这样,就可可以找到Dom中的Button,直接调用click事件,然后返回某些DOM中的Html或者Json.&lt;/p&gt;

&lt;h3 id=&#34;lua&#34;&gt;Lua&lt;/h3&gt;

&lt;p&gt;Lua 很值得一学,Nginx,游戏引擎,Redis都在用……
快速入门：&lt;a href=&#34;http://tylerneylon.com/a/learn-lua/&#34;&gt;http://tylerneylon.com/a/learn-lua/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lua稍微复杂并且可以玩出花的内置机制就是Metatable了,可以重载对字典(对象)各种操作符.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>