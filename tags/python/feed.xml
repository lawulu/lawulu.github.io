<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ç’æ¿’æ®‰æ¼‚æµè®°</title>
    <link>https://lawulu.github.io/tags/python/feed/index.xml</link>
    <description>Recent content on ç’æ¿’æ®‰æ¼‚æµè®°</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/tags/python/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>CDNæµé‡æ—¥å¿—ç®€å•åˆ†æ</title>
      <link>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</guid>
      <description>

&lt;h2 id=&#34;ç¼˜èµ·&#34;&gt;ç¼˜èµ·&lt;/h2&gt;

&lt;p&gt;å…¬å¸è¿è¥çš„å¾ˆå¤§ä¸€ä¸ªæˆæœ¬å°±æ˜¯CDNè€—è´¹ã€‚å¯¹æ¯”Impressionå’ŒCDNæ¶ˆè€—é‡å°±ä¼šå¾ˆå¥‡æ€ªï¼Œä¸è¯¥æœ‰è¿™ä¹ˆå¤šçš„CDNæ¶ˆè€—çš„ã€‚&lt;/p&gt;

&lt;h2 id=&#34;æ•°æ®åˆ†æ&#34;&gt;æ•°æ®åˆ†æ&lt;/h2&gt;

&lt;h3 id=&#34;ä¸‹è½½å¹¶å¤„ç†cdnåŸå§‹æ—¥å¿—&#34;&gt;ä¸‹è½½å¹¶å¤„ç†CDNåŸå§‹æ—¥å¿—&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;ç”±äºå®˜æ–¹æä¾›çš„åˆå¹¶ä¸‹è½½æ—¥å¿—å·¥å…·åœ¨Macä¸‹ä¸èƒ½ç”¨ï¼Œéšæœºé€‰å–ä¸€ä¸ªå°æ—¶çš„æ—¥å¿—ï¼š&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;wget https://cdnlog.cn-hangzhou.oss.aliyun-inc.com/streaming.lawulu.com/2017_03_04/streaming.lawulu.com_2017_03_04_1200_1300.gz?spm=5176.8232200.log.d10.83JI60&amp;amp;OSSAccessKeyId=xxxxx&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Pythonå¤„ç†åˆ†æä¸ºCSV&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;pat = ( r&#39;\[(.+)\]\s&#39; #datetime
           &#39;(\d+.\d+.\d+.\d+)\s&#39; #IP address
           &#39;-\s&#39; #proxy ip
            &#39;\d+\s&#39; #responseTime
           &#39;&amp;quot;.+&amp;quot;\s&#39; #referrer
           &#39;&amp;quot;(GET\s\S+mp4)&amp;quot;\s&#39;  # requested file
            &#39;\d+\s&#39; #status
           &#39;\d+\s&#39; #requestSIze
           &#39;(\d+)\s&#39; #responseSize
           &#39;\D+\s&#39; #HIT OR NOT
           &#39;&amp;quot;(.+)&amp;quot;\s&#39; #user agent
           &#39;&amp;quot;\S+&amp;quot;&#39; #contentType
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@see &lt;a href=&#34;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&#34;&gt;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å¯¼å…¥æ•°æ®åº“&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA INFILE &#39;cdnlogs.csv&#39; 
INTO TABLE test.cdn_log
FIELDS TERMINATED BY &#39;,&#39; 
LINES TERMINATED BY &#39;\n&#39; 
(date_time,ip,url,rsp_size,ua);
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;æ•°æ®æ ¡éªŒ&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;SELECT count(*)  FROM test.cdn_log 
&#39;206619&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cat streaming.lawulu.com_2017_03_04_1200_1300 | wc -l

206618
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ç”¨sqlåˆ†æ&#34;&gt;ç”¨Sqlåˆ†æ&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) =1 )a


&#39;110643&#39;,&#39;598635187514&#39;


select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) &amp;gt;1 )a


&#39;32670&#39;, &#39;440010318426&#39;

0.4236
select 440010318426/(598635187514+440010318426) from dual 

select sum(rsp_size) from  cdn_log

1038645505940

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ç»“è®º&#34;&gt;ç»“è®º&lt;/h2&gt;

&lt;p&gt;ç”±äºIP,UA,URLå¯ä»¥å”¯ä¸€ç¡®å®šä¸€æ¬¡UVï¼Œæˆ‘ä»¬å…¬å¸æœ‰30%çš„ç”¨æˆ·å ç”¨äº†40%çš„æµé‡ï¼Œå¯¹åŒä¸€ä¸ªè§†é¢‘ä¼šä¸åœçš„åå¤ä¸‹è½½ã€‚
æœ€åæŸ¥æ˜ï¼Œæ˜¯SDKçš„bugï¼Œæ­£åœ¨ä¸‹è½½è§†é¢‘æ—¶å€™ï¼Œå¦‚æœåˆ‡æ¢åˆ°åå°ï¼Œä¸‹è½½ä¼šç›´æ¥åœæ­¢ã€‚æ‰€ä»¥ä¼šåå¤ä¸‹è½½è§†é¢‘ï¼Œæµªè´¹æµé‡ã€‚&lt;/p&gt;

&lt;h2 id=&#34;æ ¡éªŒ&#34;&gt;æ ¡éªŒ&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;è®¡ç®—ResponseSizeä¹‹åå’ŒConsoleé‡Œé¢åŒä¸€æ—¶é—´æ®µçš„&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;
CURL  -i -X HEAD &#39;https://streaming.lawulu.com/default/256b9db1-a1ce-4983-8906-4568fa7a9c75-144.mp4&#39;
&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>å¹¿åçš„IPåº“åˆ°åº•å‡†ä¸ä½</title>
      <link>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</guid>
      <description>

&lt;h2 id=&#34;ç¼˜èµ·&#34;&gt;ç¼˜èµ·&lt;/h2&gt;

&lt;p&gt;å¯¹äºCPMå¹¿å‘Šï¼Œåœ°åŸŸå®šå‘æ˜¯éå¸¸é‡è¦çš„ã€‚å¹¿å‘Šåä¼šæä¾›äº†ipåº“ï¼Œæ–¹ä¾¿AdNetworkå’Œç¬¬ä¸‰æ–¹ç›‘æ§åšåœ°åŸŸå®šå‘ã€‚ç™¾åº¦é»˜è®¤ä»¥ip138åšä¸ºå…¶&amp;rdquo;IP&amp;rdquo;å…³é”®å­—çš„ç¬¬ä¸€ä¸ªå…¥å£ï¼Œæˆ‘ç›¸ä¿¡ip138çš„åº“æ˜¯æ¯”è¾ƒå‡†çš„ï¼Œé‚£å¹¿åçš„IPåº“å’Œip138çš„åº“å·®åˆ«å¤§å—ï¼Ÿ&lt;/p&gt;

&lt;h2 id=&#34;æ•°æ®åˆ†æ&#34;&gt;æ•°æ®åˆ†æ&lt;/h2&gt;

&lt;h3 id=&#34;éšæœºä»çº¿ä¸Šæå–10000ä¸ªip-å¹¶ä½¿ç”¨å¹¿ååº“è§£ææˆå¯¹åº”åŸå¸‚&#34;&gt;éšæœºä»çº¿ä¸Šæå–10000ä¸ªipï¼Œå¹¶ä½¿ç”¨å¹¿ååº“è§£ææˆå¯¹åº”åŸå¸‚&lt;/h3&gt;

&lt;p&gt;ä»æŸä¸ªæ—¶æ®µåŸå§‹Nginxæ—¥å¿—ä¸­çš„ip(300W+)éšæœºå–10000æ¡
ä½¿ç”¨pandas&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;csv = pd.read_csv(&#39;/tmp/ip.log&#39;)
np.savetxt(&#39;/tmp/ipSample.log&#39;,csv.sample(10000).values,fmt=&#39;%s&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;æ ¹æ®ipè·å–ip138å¯¹åº”çš„åŸå¸‚&#34;&gt;æ ¹æ®ipè·å–ip138å¯¹åº”çš„åŸå¸‚&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# get data from web
def get_data(query):
    url = &amp;quot;http://m.ip138.com/ip.asp?ip=&amp;quot; + query
    res = requests.get(url)
    print &#39;get data for &#39;+ query +&#39; result:&#39; + str(res.status_code)
    if(res.status_code != 200):
        return &#39;Failed Response&#39;
    else:
        return parse_html(res.content)

def parse_html(html):
    res = BeautifulSoup(html,&amp;quot;html.parser&amp;quot;).find_all(&amp;quot;p&amp;quot;, &amp;quot;result&amp;quot;)
    result =split(res[0])
    if len(res)&amp;gt;1:
        result=result+&amp;quot;,&amp;quot;+ split(res[1])
    return result

def split(str):
    try:
        splits=str.string.encode(&amp;quot;utf-8&amp;quot;).split(&#39;ï¼š&#39;)
        return splits[1]
    except RuntimeError:
        return &#39;Split Error:&#39;+str

with open(&#39;/tmp/ipSample.log&#39;) as source:
    with open(&#39;ip138Result.csv&#39;,&#39;aw&#39;) as result:
        for line in source:
            ip = line.strip(&#39;\n&#39;)
            ipStr=get_data(ip)
            time.sleep(0.1)
            result.write(ip+&#39;|&#39;+ipStr+&#39;\n&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;å–å…¨å›½60å¼ºåŸå¸‚-ç»Ÿè®¡ä¸¤ä¸ªç»“æœçš„å·®è·&#34;&gt;å–å…¨å›½60å¼ºåŸå¸‚ï¼Œç»Ÿè®¡ä¸¤ä¸ªç»“æœçš„å·®è·&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.mnw.cn/news/china/886245.html&#34;&gt;http://www.mnw.cn/news/china/886245.html&lt;/a&gt; 60å¼ºåŸå¸‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;citys=[&#39;ä¸Šæµ·&#39;,&#39;åŒ—äº¬&#39;,&#39;å¹¿å·&#39;,&#39;æ·±åœ³&#39;,&#39;æˆéƒ½&#39;,&#39;é‡åº†&#39;,&#39;æ­å·&#39;,&#39;å—äº¬&#39;,&#39;æ²ˆé˜³&#39;,&#39;è‹å·&#39;,&#39;å¤©æ´¥&#39;,&#39;æ­¦æ±‰&#39;,&#39;è¥¿å®‰&#39;,&#39;é•¿æ²™&#39;,&#39;å¤§è¿&#39;,&#39;æµå—&#39;,&#39;å®æ³¢&#39;,&#39;é’å²›&#39;,&#39;æ— é”¡&#39;,&#39;å¦é—¨&#39;,&#39;éƒ‘å·&#39;,&#39;é•¿æ˜¥&#39;,&#39;å¸¸å·&#39;,&#39;å“ˆå°”æ»¨&#39;,&#39;ç¦å·&#39;,&#39;æ˜†æ˜&#39;,&#39;åˆè‚¥&#39;,&#39;ä¸œè&#39;,&#39;çŸ³å®¶åº„&#39;,&#39;å‘¼å’Œæµ©ç‰¹&#39;,&#39;å—æ˜Œ&#39;,&#39;æ¸©å·&#39;,&#39;ä½›å±±&#39;,&#39;è´µé˜³&#39;,&#39;å—å®&#39;,&#39;æµ·å£&#39;,&#39;æ¹–å·&#39;,&#39;å”å±±&#39;,&#39;ä¸´æ²‚&#39;,&#39;å˜‰å…´&#39;,&#39;ç»å…´&#39;,&#39;å—é€š&#39;,&#39;å¾å·&#39;,&#39;æ³‰å·&#39;,&#39;å¤ªåŸ&#39;,&#39;çƒŸå°&#39;,&#39;ä¹Œé²æœ¨é½&#39;,&#39;æ½åŠ&#39;,&#39;ç æµ·&#39;,&#39;æ´›é˜³&#39;,&#39;ä¸­å±±&#39;,&#39;å…°å·&#39;,&#39;é‡‘å&#39;,&#39;æ·®å®‰&#39;,&#39;å‰æ—&#39;,&#39;å¨æµ·&#39;,&#39;æ·„åš&#39;,&#39;é“¶å·&#39;,&#39;æ‰¬å·&#39;,&#39;èŠœæ¹–&#39;,&#39;ç›åŸ&#39;,&#39;å®œæ˜Œ&#39;,&#39;è¥¿å®&#39;,&#39;è¥„é˜³&#39;,&#39;ç»µé˜³&#39;]
files =[&amp;quot;ip138Result.csv&amp;quot;,&amp;quot;ipResultV2.log&amp;quot;]
import subprocess, datetime, sys
def checkNums(file,city):
    p = subprocess.Popen(&amp;quot;&amp;quot;&amp;quot; cat &amp;quot;&amp;quot;&amp;quot; + file + &amp;quot;&amp;quot;&amp;quot;| grep &amp;quot;&amp;quot;&amp;quot;+ city +&amp;quot;&amp;quot;&amp;quot;  |wc -l &amp;quot;&amp;quot;&amp;quot;, shell=True, stdout=subprocess.PIPE );
    p.wait()
    out = p.stdout.readlines()[0]
    return str(out).strip()

for city in citys:
    result = city + &amp;quot;\t&amp;quot;
    for file in files:
        result = result +checkNums(file,city)+ &amp;quot;\t&amp;quot;
    print  result
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ç»“è®º&#34;&gt;ç»“è®º&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;åŒ—äº¬ç›¸å·®æ¯”è¾ƒå¤§ï¼Œç³»ç»Ÿåº“æ¯”Ip138åº“å¤šå‡ºä¸€å€&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å…¶ä»–åŸå¸‚ ä¸Šæµ· æ·±åœ³ å—äº¬ è‹å· å¤§è¿ å®æ³¢ç­‰ä¹Ÿç›¸å·®åœ¨20%ä»¥ä¸Šã€‚
å¯ä»¥è¯´è·ŸIP138å·®åˆ«è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ã€‚&lt;/p&gt;

&lt;p&gt;ä½†æ˜¯ä¸‰çº¿åŸå¸‚æ¯”è¾ƒå‡†ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scrapy</title>
      <link>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Thu, 15 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Scrapy%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;æœ€è¿‘å‡ºäºå…´è¶£çœ‹äº†ä¸€ä¸‹Scrapy.ç¬¬ä¸€æ¬¡ä»”ç»†ç ”ç©¶ä¸€ä¸ªPythonæ¡†æ¶,å…ˆè¯´ç»“è®º,çˆ¬è™«åˆ†ä¸ºä¸¤ç§,ä¸€ç§æ˜¯è´ªå©ªçˆ¬è™«,æœ€å¸¸ç”¨çš„å°±æ˜¯æœç´¢å¼•æ“,çœ‹è§Urlå°±çˆ¬.å¦å¤–ä¸€ç§æ˜¯ä¸“ä¸šçˆ¬è™«,ç±»ä¼¼äºç½‘ç»œå°æ•°å®šæœŸæ›´æ–°,ä»·æ ¼æ¯”å¯¹.Scrapyé€‚åˆåšç¬¬äºŒç§.Scrapyä¸“å¿ƒåšçˆ¬å–é€»è¾‘,å…¶ä¼˜åŠ¿æ˜¯æ¡†æ¶æ¯”è¾ƒæˆç†Ÿ,å„ç§æ’ä»¶æ¯”è¾ƒå…¨.å¦‚æœçœŸæ­£åšä¸€ä¸ªå·¥ä¸šåŒ–çš„äº§å“,Scrapyè¿œè¿œä¸å¤Ÿ,è¿˜è¦æ”’å¾ˆå¤šä¸œè¥¿è¿›æ¥.å…¶å®å¦‚æœå¯¹&lt;code&gt;requests&lt;/code&gt;å’Œ&lt;code&gt;beautifulsoup&lt;/code&gt;ç†Ÿæ‚‰,ç›´æ¥ä¸Šè¿™ä¿©å°±æŒºå¥½çš„.&lt;/p&gt;

&lt;h2 id=&#34;ç®€ä»‹&#34;&gt;ç®€ä»‹&lt;/h2&gt;

&lt;p&gt;Scrapyå…¶å®æ˜¯ä¸€ä¸ªå¾ˆè½»é‡çº§æˆ–è€…ç®€å•çš„æ¡†æ¶.å‡ ä¸ªæ¦‚å¿µï¼š
1. &lt;code&gt;Request&lt;/code&gt;,&lt;code&gt;Response&lt;/code&gt;,é¡¾åæ€ä¹‰,å¯¹åº”çš„å°±æ˜¯ç½‘ç»œçš„è¯·æ±‚å’Œå“åº”.
2. &lt;code&gt;Item&lt;/code&gt;æˆ–è€…python dict,å¯¹åº”çš„æ˜¯çˆ¬å–åˆ°çš„ç»“æœ.
3. &lt;code&gt;Pipeline&lt;/code&gt;å¯¹Itemçš„å¤„ç†.
4. &lt;code&gt;Selector&lt;/code&gt;å¯¹åº”çš„æ˜¯å¯¹HTMLçš„å¤„ç†
5. &lt;code&gt;Middleware&lt;/code&gt;ç›¸å½“äºFilter,åˆ†ä¸¤ç§,ä¸€ç§æ˜¯é’ˆå¯¹Spider,ä¸€ç§é’ˆå¯¹ç½‘ç»œè¿æ¥
6. &lt;code&gt;Scheduler&lt;/code&gt;ä¸€ä¸ªæ·±åº¦ä¼˜å…ˆçš„é˜Ÿåˆ—
7. &lt;code&gt;Feed&lt;/code&gt; å®˜æ–¹æä¾›çš„jsonæˆ–è€…json lineçš„è¾“å‡º
8. &lt;code&gt;meta&lt;/code&gt; å¯ä»¥åœ¨å¤šä¸ªSpiderä¹‹é—´ä¼ è¾“æ•°æ®&lt;/p&gt;

&lt;p&gt;å…¶å®æŠŠæ–‡æ¡£çœ‹ä¸€é,ä¸‹ä¸€ä¸ªdemo,å¾ˆå¿«å°±å¯ä»¥è·‘èµ·æ¥.&lt;/p&gt;

&lt;p&gt;çœ‹å›¾å…¶å®å¾ˆæ¸…æ¥šï¼š
&lt;img src=&#34;https://lawulu.github.io/iimg/scrapy.png&#34; alt=&#34;Scrapy Arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä»StartRequestå¼€å§‹,ä¸åœçš„è¿”å›Itemæˆ–è€…è¿”å›Request,å¦‚æœè¿”å›Item,å°±å‘åˆ°Pipelineå»å¤„ç†,è¿”å›Request,ç»è¿‡Callbackå¤„ç†ä¹‹å,ç›´åˆ°æ‰€æœ‰çš„Requestéƒ½å˜æˆItem.æ­£æ‰€è°“,ä¸–ç•Œæ˜¯å±äºRequest,å½’æ ¹ç»“åº•è¿˜æ˜¯å±äºItemçš„.&lt;/p&gt;

&lt;h2 id=&#34;ç®€å•ç¤ºä¾‹&#34;&gt;ç®€å•ç¤ºä¾‹&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import scrapy
class ZhihuSpider(scrapy.Spider):
    name = &amp;quot;zhihu&amp;quot;
    start_urls = [
        &#39;https://www.zhihu.com/explore&#39;,
    ]

    def parse(self, response):
        # self.logger.info(&amp;quot;------zzhonä¸­å›½&amp;quot;)

        for feed in response.css(&#39;.recommend-feed&#39;):
            link = response.urljoin(feed.css(&#39;a::attr(href)&#39;).extract_first());
            yield scrapy.Request(link,callback=self.parse_content)
           

##!!!  print response.css(&#39;#zh-question-detail &amp;gt; div&#39;).extract()[0].encode(&#39;utf-8&#39;)

    def parse_content(self,response):
        yield {
            &#39;title&#39;:response.css(&#39;#zh-question-title &amp;gt; h2 &amp;gt; a::text&#39;).extract_first(),
            &#39;content&#39;:response.css(&#39;#zh-question-detail &amp;gt; div::text&#39;).extract(),
            &#39;link&#39;:response.url
        }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å½“ç„¶è¿™ä¸ªç¤ºä¾‹è·‘èµ·æ¥ä¹‹å‰,éœ€è¦æ”¹ä¸€äº›é…ç½®æ–‡ä»¶å’Œè®¾ç½®User-agentâ€¦&lt;/p&gt;

&lt;h2 id=&#34;åæ§½&#34;&gt;åæ§½&lt;/h2&gt;

&lt;p&gt;Javaå‡ºèº«çš„ç¨‹åºå‘˜çš„åæ§½â€¦ ä¸ç”¨åœ¨æ„ğŸ˜œ&lt;/p&gt;

&lt;h3 id=&#34;æ—¥å¿—åªæ‰“å°unicodeé—®é¢˜&#34;&gt;æ—¥å¿—åªæ‰“å°Unicodeé—®é¢˜&lt;/h3&gt;

&lt;p&gt;Debugå’Œç›‘æ§èµ·æ¥éå¸¸ä¸æ–¹ä¾¿,æˆ‘çœ‹å®˜æ–¹æŠŠè¿™ä¸ªissueå½“åšwont fixäº†,å¼ºçƒˆæ¨èç›´æ¥ä¸ŠPython3.å¯¹éè‹±æ–‡ä¸–ç•Œæ¥è¯´,Python2ä¸‹scrapy shellç®€ç›´æ²¡äº†å­˜åœ¨çš„å¿…è¦â€¦&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2015-10-14 23:25:54 [scrapy] DEBUG: Scraped from &amp;lt;200 https://www.zhihu.com/question/29218955/answer/41797438&amp;gt; 
{&#39;content&#39;: [u&#39;\u666e\u6d31\u5728\u8fc7\u53bb\u5e94\u8be5\u662f\u4e0d\u4e0a\u53f0\u9762\u7684\u5427\uff1f\u4e5f\u6ca1\u95ee\u662f\u4e0d\u662f\u5c31\u95ee\u4e3a\u4ec0\u4e48\u4e86\uff0c\u5982\u679c\u95ee\u9898\u6709\u9519\u8bef\uff0c\u8bf7\u6307\u6b63&#39;], &#39;link&#39;: &#39;https://www.zhihu.com/question/19258955/answer/42797438&#39;, &#39;title&#39;: u&#39;\u666e\u6d31\u7531\u539f\u6765\u4e0d\u4e0a\u53f0\u9762\u7684\u8fb9\u9500\u8336\u5230\u5982\u4eca\u53d7\u5230\u5e7f\u5927\u8336\u53cb\u559c\u7231\u7684\u8336\u7c7b\uff0c\u9664\u4e86\u7092\u4f5c\u5916\uff0c\u666e\u6d31\u7684\u5de5\u827a\u6216\u8005\u8d28\u91cf\u6709\u4e86\u5f88\u5927\u8fdb\u6b65\u5417\uff1f&#39;} 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;schedulerå¤ªå¼±&#34;&gt;Schedulerå¤ªå¼±&lt;/h3&gt;

&lt;p&gt;è‡ªå¸¦çš„scheduleråªèƒ½æ·±åº¦ä¼˜å…ˆå’Œå¹¿åº¦ä¼˜å…ˆ,å®˜æ–¹æ¨èæ˜¯è‡ªå®¶çš„scrapyd,ç¨å¾®äº†è§£äº†ä¸‹,åº”è¯¥æ˜¯å¯åŠ¨æ—¶å€™æŠŠä»»åŠ¡åˆ†ç»™å¤šä¸ªè¿›ç¨‹,è¿›ç¨‹ä¹‹é—´å¥½åƒæ²¡æ³•äº¤äº’.&lt;/p&gt;

&lt;h3 id=&#34;spider-requestä¹‹é—´å¾ˆéš¾ååŒ&#34;&gt;Spider/Requestä¹‹é—´å¾ˆéš¾ååŒ&lt;/h3&gt;

&lt;p&gt;å¾ˆå¤šæ—¶å€™æˆ‘ä»¬éœ€è¦æŸå‡ ä¸ªRequeståº”è¯¥åœ¨ä¸€ä¸ªç»„é‡Œé¢,åŒä¸€ä¸ªç»„çš„Requestå¯ä»¥å…±ç”¨Cookie,å…±ç”¨ä»£ç†.è¿™ç±»çš„éœ€æ±‚å¯ä»¥é€šè¿‡metaæœºåˆ¶æ¥å®ç°â€¦ç”±äºScrapyçš„Requestæ˜¯æ— è„‘yieldå‡ºæ¥çš„,å®ç°å‡ºæ¥è‚¯å®šå¾ˆä¸‘é™‹.&lt;/p&gt;

&lt;h3 id=&#34;callback-hell&#34;&gt;Callback hell&lt;/h3&gt;

&lt;p&gt;é€šè¿‡Callbackæ¥ç»‘å®šå¤„ç†æ–¹æ³•,å…¥å£å’Œå¯æµ‹è¯•æ€§å¾ˆå·®,æ€ªä¸å¾—è¦åŠ å…¥Contractæœºåˆ¶&lt;/p&gt;

&lt;h3 id=&#34;å®¹é”™&#34;&gt;å®¹é”™&lt;/h3&gt;

&lt;p&gt;å®¹é”™æœºåˆ¶å¾ˆå°‘,éœ€è¦æŠŠTaskæŒä¹…åŒ–,å®˜æ–¹æä¾›çš„æŒä¹…åŒ–åˆ°diskçš„æ–¹æ¡ˆæ„Ÿè§‰å¾ˆåˆçº§å¾ˆç²—ç³™.&lt;/p&gt;

&lt;h2 id=&#34;splash&#34;&gt;Splash&lt;/h2&gt;

&lt;p&gt;Pyspideræ˜¯ç”¨çš„&lt;code&gt;PhantomJS&lt;/code&gt;,Scrapyæä¾›äº†ä¸€ä¸ªåŸºäºDockerå’ŒLuaçš„Splash.
è¿™è´§çš„é—®é¢˜æ˜¯Splashæ²¡æœ‰ä¿æŒæ¸²æŸ“çŠ¶æ€çš„æ–¹æ³•,å¯¹äºå¤æ‚çš„JSäº¤äº’,ä¸€èˆ¬çš„åšæ³•æ˜¯ä¸€ä¸ªluaè„šæœ¬æ‰§è¡Œå¾ˆå¤šæ¬¡,æœ€åè¿”å›æ‰€æœ‰çš„ç»“æœ.è¿™æ ·çˆ¬å–çš„é¢—ç²’æ¯”è¾ƒå¤§,å¦‚æœä¸­é—´å‡ºç°å¼‚å¸¸,ä¸å¥½å¤„ç†,è€Œä¸”è°ƒç”¨&lt;code&gt;splash::wait()&lt;/code&gt;æ€»æ„Ÿè§‰ä¸é è°±,æ²¡æœ‰æä¾›DomåŠ è½½ä¹‹åçš„å›è°ƒå—ï¼Ÿ&lt;/p&gt;

&lt;h3 id=&#34;åŸç†&#34;&gt;åŸç†&lt;/h3&gt;

&lt;p&gt;Splashå¯ä»¥æ‰§è¡ŒLuaè„šæœ¬,å®˜æ–¹å°è£…äº†Splashç±»,å¯ä»¥å’ŒJSäº¤äº’.è¿™æ ·,å°±å¯å¯ä»¥æ‰¾åˆ°Domä¸­çš„Button,ç›´æ¥è°ƒç”¨clickäº‹ä»¶,ç„¶åè¿”å›æŸäº›DOMä¸­çš„Htmlæˆ–è€…Json.&lt;/p&gt;

&lt;h3 id=&#34;lua&#34;&gt;Lua&lt;/h3&gt;

&lt;p&gt;Lua å¾ˆå€¼å¾—ä¸€å­¦,Nginx,æ¸¸æˆå¼•æ“,Rediséƒ½åœ¨ç”¨â€¦â€¦
å¿«é€Ÿå…¥é—¨ï¼š&lt;a href=&#34;http://tylerneylon.com/a/learn-lua/&#34;&gt;http://tylerneylon.com/a/learn-lua/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Luaç¨å¾®å¤æ‚å¹¶ä¸”å¯ä»¥ç©å‡ºèŠ±çš„å†…ç½®æœºåˆ¶å°±æ˜¯Metatableäº†,å¯ä»¥é‡è½½å¯¹å­—å…¸(å¯¹è±¡)å„ç§æ“ä½œç¬¦.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>