<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on 璐濒殉漂流记</title>
    <link>https://lawulu.github.io/post/feed/index.xml</link>
    <description>Recent content in Post-rsses on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 15 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lawulu.github.io/post/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>小陈与大数据的故事</title>
      <link>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</guid>
      <description>

&lt;p&gt;&lt;em&gt;本篇为公众号&lt;code&gt;码农翻身&lt;/code&gt;的约稿，最终想写4篇，分别为&lt;code&gt;陈余罗严&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;在学校&#34;&gt;在学校&lt;/h2&gt;

&lt;p&gt;做为一个科班出身的程序员，在小陈还不明白自己所学的的数据结构和操作系统这些专业课跟未来的工作有什么关系，甚至未来做什么也不清楚时候，小陈最喜欢做的事就是看电影，看完还舍不得删。那个时代流行的还是“为人不识武藤兰，阅遍电影也枉然”，电影还有很多是rm格式的，几百兆都算大的，但是架不住小陈对一衣带水邻国文化的热爱，只有120G的硬盘让他很快有了另一个爱好：整理硬盘，每次有新资源出来，小陈都得想办法腾点空间出来。&lt;/p&gt;

&lt;p&gt;后来教育网里一个叫Maze的软件在学校流行开来，这个软件可以在局域网内以P2P的方式完成资源共享。喜大普奔，小陈得意洋洋的宣布自己终于不用整理硬盘了，自己硬盘里面只存一部分经典的，想看什么电影从别人机器里面拖，哪怕自己临时要看一个美帝大片，删掉一部分电影，删掉之后都还能从别人机器里面拖回来。&lt;/p&gt;

&lt;p&gt;直到有一天，小陈临时删了一位老师的代表作，等想找回来时候，不知道是学校调整了网络路由还是因为高年级学生的离校，这部本来在Maze中非常常见的电影居然一个都找不到了。小陈被室友嘲笑之余，怒而卸载了Maze，并决定要开发一套更好的共享资源软件：要有一个元数据服务器，所有使用者的硬盘的读写都由元数据服务器来管理，元数据保证每份电影都至少存在2份以上，而不是像现在这样保存什么电影，全靠用户心照不宣的默契和自己的兴趣。&lt;/p&gt;

&lt;p&gt;不过小陈也面临着毕业，自然没有时间去搞这些乱七八糟的东西，为了找工作，开始恶补各种技能。这已经是2011年，大数据已经开始火起来了，小陈开始试着学习大数据的知识。搞Android培训的那波人摇身一变开始培训Hadoop了，小陈上了一次试听课之后，回来开始向室友生成，自己的创意被Google和Hadoop给剽窃了！大数据首先要解决的都是存储问问题，GFS，以及受之启发的Hadoop HDFS，完全就是照着自己的资源共享软件思路来的。&lt;/p&gt;

&lt;p&gt;其实小陈知道，Google在2003年就把GFS的论文给写出来，开发这个GFS时候武老师可能还没有出道，甚至BigData这个词还没有被清楚的定义。但是Google作为技术最领先，体量最大的搜索引擎公司，很早就面临和自己类似的问题:大量的数据(信息索引和网页快照等等)要怎么存储存储在哪里的问题。Google选择的是用大量的通用硬件组成一个可以横向扩展的集群去对存储和处理这些海量的数据。这种英雄所见略同的感觉，让小陈决定要从事大数据开发，毕竟这个越来越自我的世界，随着移动互联网的爆发，每个人制造的垃圾数据会越来越容易，而制造垃圾数据的人也会越来越多。&lt;/p&gt;

&lt;h2 id=&#34;第一份工作&#34;&gt;第一份工作&lt;/h2&gt;

&lt;p&gt;找大数据的工作并不顺利，心仪的公司嫌自己没有经验，给Offer的公司貌似没有多少数据，只是想趁着大数据热忽悠一把。阴错阳差，小陈找到一份Java开发的工作。这家公司专做某行业的软件，公司不大但发展很快，待遇还算不错。小陈第一周上班主要学习公司的技术框架，然后就懵逼了。面试时候非要分清POBOVO，现在项目所有的参数传递都是List&lt;Map&gt;；面试时候各种设计模式的问，结果这项目的大部分的业务逻辑都是写在SQL存储过程里面的，甚至JS里面也夹杂了很多SQL；自研的一个工作流框架对应的库表，各种预留字段，从field1，field2一直到field8。在小陈犹豫着要不要换家公司，技术总监的几句话给忽悠住了他：这个世界的所有事物，即所谓的对象，都是可以用List&lt;Map&gt;来表示；SQL是史上的最好的DSL，一定要充分利用，如果想写，用SQL实现一个Dijkstra算法都可以[注1]；这框架虽然不走寻常路，但是有PHP的开发速度，也利用JVM的性能和Java的生态系统。&lt;/p&gt;

&lt;p&gt;小陈不仅觉得总监的话很有道理，而且这些话跟大数据的思路也有几份相似。Google开启大数据时代最重要的就是三篇论文:Google File System,讲如何分布式存储大数据；Google MapReduce,讲如何分布式计算大数据；Google Bigtable,实现了一个数据库，以便在分布式下快速读写结构化数据。对应的开源实现是Hadoop Hdfs，Hadoop MapReduce，HBase。这个识万物为List&lt;Map&gt;的思路不就是MapReduce的思路吗，而那个预留字段的数据库设计其实有点Bigtable/HBase的风范，不管神似与否至少形似。工作一段时间，小陈感觉这糙快猛的框架确实还是很适合这个需求变化比较快的行业，对他自己来说，至少写SQL和实现业务需求的能力提高了不少。一年之后，小陈跳槽到一家三线互联网公司做大数据，凭借的就是这两项能力。&lt;/p&gt;

&lt;h2 id=&#34;大数据工作和两个领导&#34;&gt;大数据工作和两个领导&lt;/h2&gt;

&lt;p&gt;新公司工作主要基于Shell调用Hive，把数据从日志往Oracle里面导，经常还要给业务部门提数。所以，虽然在做大数据，工作内容只是从上家公司的Oracle PL/SQL换成了Hive SQL。Map-Reduce虽然直指大数据计算的本源，但是奈何这个世界太复杂，产品经理的需求太多，表达力有点欠缺。而且又很多通用的需求，例如数据之间的Join和Group，完全可以提炼出来，于是Facebook向开源社区捐献了Hive，用史上的最好的DSL作为查询语言交给熟悉SQL的人去使用，Hive引擎负责把SQL翻译成MapReduce。小陈得偿所愿进入大数据行业，但是很快就厌倦了：Shell管理起来太麻烦，跟其他系统交互起来不方便；Hive运行太慢，资源利用率低，有时候跑了十几分钟结果出来了发现自己的SQL有一个简单的错误；想推动HadoopV2的落地，让Yarn去合理的管理集群资源，领导和同事都不愿意承担风险。小陈隐约觉得自己做的东西并没有给公司带来应有的价值，只是让报表的维度增加一些。领导告诉小陈，大数据并不是解决一切问题的灵丹妙药，数据越来越多，增加更多的是无价值的数据，而大数据处理的日志等数据与业务数据相比，信息熵要低的多。&lt;/p&gt;

&lt;p&gt;就这么工作一年后，老板也对数据部门不满意，赶走了老领导，挖来一个大厂的架构师来主管整合所有数据相关业务。新领导上来就大刀阔斧，部门架构调整，把一部分数据相关的开发并了过来，整个部门一分为二，一部分往上跟业务和产品部门打交道，一部分下沉做大数据平台时候。小陈果断的选择了去做平台。&lt;/p&gt;

&lt;p&gt;如果说老领导强调的是数据完整性和正确性，数据对业务的支撑，新领导更强调数据对产品和决策的反馈，技术的先进性。按照新领导的意思，以做用户数据仓库为出发点，从数据收集、数据转换、数据模型、数据查询、数据可视化到任务调度和平台监控，要重新梳理现有每一个模块的技术架构。这个时候，大数据相关技术已经可以说是百花齐放百家争鸣，围绕着Hadoop，几乎每个领域都有一两个出色的解决方案。领导要求使用技术要向代表技术先进性，相关软件版本落后不能超过2年，鼓励使用3年以内的新项目。小陈很高兴，感觉可以大干一场了。因为态度积极，小陈很受新领导的器重，小陈先负责用其他SQL on Hadoop的如Impala/Presto来代替Hive的部分功能，提高查询效率，后来又负责引入实时计算框架Storm，虽然只是用来计算PV/UV，但好歹是实时的。总之是趟坑无数，背锅无数，忙得不亦乐乎。&lt;/p&gt;

&lt;p&gt;直到又过一年公司裁员，小陈才发现数据部门是重灾区。和领导吃散伙饭，小陈开始鸣不平，觉得咱们的平台做了这么多，大家加班这么辛苦，为什么就没人懂欣赏呢。领导却直言自己早就知道这个结局，部门折腾了一年，老板只看到数据部门人员和机器一直在增加，却经常收到其他部门各种抱怨，产品抱怨进度太慢啊，商务说数据不懂业务啊，研发说总是要配合数据做这做那啊，运营说报表没有老系统方便数据老出问题等等。相处这么久，领导已经把小陈当心腹了，直言一开始想把有些业务拿到数据这边统一处理，技术出身搞不定公司复杂的关系，阻力太大；埋头把平台理顺，而在强势老板的公司里面，自己还是没有抓住老板的需求和业务的痛点，所以对老板来说，确实没为公司带来什么价值。领导的这番肺腑之言对小陈触动很大，开始重新审视自己的过去的工作。不管怎样小陈这时候找下一份工作已经有了一份资本。&lt;/p&gt;

&lt;h2 id=&#34;在创业公司&#34;&gt;在创业公司&lt;/h2&gt;

&lt;p&gt;在全民创业的浪潮中，小陈来到一家创业公司，这里还没有什么&amp;rdquo;大数据&amp;rdquo;,但是CEO的数据要介入整个产品和运营中的承诺和希望打动了小陈，而且这里有机会从零开始有机会按照自己的思路规划整个公司的大数据业务。这个时候，大数据圈最火的是Spark，Spark以基于内存的DAG计算引擎为基础，提供了批量计算，实时计算，交互式查询，机器学习，基本上实现了One Stack To Rule Them All，而且Spark也有SparkSQL。小陈选择了围绕Spark来构建整个系统，并且拥抱云计算了，直接选择云服务，希望自己能专注于让数据产生价值。小陈工作分这么几部分:输出报表数据;给一个数据科学家打杂，帮他整理数据提取特征，实现一些算法;每天花一个小时看数据，解释数据和用数据去解释。小陈成了对公司业务和系统最了解的人，比产品和QA都要了解，并且提出了很多优化和改进。CEO很满意，年终时候大笔一挥给了更多的期权，然而没什么卵用，因为融资寒冬来的时候，公司还是倒了。&lt;/p&gt;

&lt;h2 id=&#34;在大厂&#34;&gt;在大厂&lt;/h2&gt;

&lt;p&gt;小陈已经拖家带口了，感觉自己不再年轻，在机构Offer之间犹豫了很久之后，选择了去一个大厂做螺丝钉，这里流程和工具都已经非常完善了，用的Hadoop都是自己定制甚至已经跟社区不兼容了。小陈业务时间乐于参加各种线下各种活动，积攒自己的人脉，坚持更新Blog，提升自己的名气。&lt;/p&gt;

&lt;p&gt;这个时候Google推出了Beam计算框架。其实Google抛出了三大论文之后，表面上很沉寂，自己内部的系统却一直在向前演进，例如老版的GFS已经被Colossus被代替。直到Google眼红AWS投身云计算时候，发现客户都更习惯开源的那套东西，甚至Google把自己的BigTable服务往外卖的时候，不得不兼容BigTable的山寨产品HBase的API。这就好比一个欧洲人找到一个中国建筑师建一个亚洲古代建筑，中国建筑师给建了一个唐朝风格的建筑，欧洲人说，你这建的不大对，我在日本看到的都是这样那样。Google为了争夺话语权，把自己最新的产品拿出来开源，要一统所有的计算框架，不管你底层是Storm还是Spark，按照Beam的API来处理数据，底层可以随意切换Spark和自家的Cloud Dataflow，甚至为了师出有因，大力提携Flink和Spark打擂台。小陈相信这个框架会一统江湖，为了自己的KPI和升级, 准备在公司推Apache Beam。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>程序员必备七种武器</title>
      <link>https://lawulu.github.io/post/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E4%B8%83%E7%A7%8D%E6%AD%A6%E5%99%A8/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E4%B8%83%E7%A7%8D%E6%AD%A6%E5%99%A8/</guid>
      <description>

&lt;p&gt;&lt;em&gt;本篇为公众号&lt;code&gt;码农翻身&lt;/code&gt;的约稿&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;科学上网&#34;&gt;科学上网&lt;/h2&gt;

&lt;p&gt;从服务器芯片到底层协议，从W3C技术标准到github各种开源项目，IT技术绝大部分都起源于墙外，而Google是检索这些第一手技术的最好的手段，没有之一。所以对码农来说，可以自由的浏览是非常有必要的。&lt;/p&gt;

&lt;p&gt;有很多种科学上网方式，笔者推荐自己买海外主机搭建,可以和亲近朋友一起使用一台。除了常用的SS之外，SSH的socks转发是最简单的方式了。&lt;/p&gt;

&lt;p&gt;例如笔者常用的一个shell：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
nohup ssh  -ND 3128 user@jumpserver &amp;amp;
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome  Chrome --proxy-server=&amp;quot;socks5://localhost:3128&amp;quot; --host-resolver-rules=&amp;quot;MAP * 0.0.0.0 , EXCLUDE localhost&amp;quot; --user-data-dir=/tmp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个shell在Mac下启动一个Chrome进程，该进程完全以jumpserver的网络去访问互联网，这样在这个Chrome内访问公司云主机内网的各种服务非常方便。&lt;/p&gt;

&lt;h2 id=&#34;剪贴板&#34;&gt;剪贴板&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.todamax.net/wp-content/uploads/2016/03/copy-paste-from-stack-overflow.jpeg&#34;&gt;http://blog.todamax.net/wp-content/uploads/2016/03/copy-paste-from-stack-overflow.jpeg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;相信大家最常用的快捷键就是复制和粘贴了。
假如你刚刚从Wiki的设计文档把一个API的URL复制下来准备写代码；发现经常聊天的一个技术群弹出一个消息，有个妹子问一个技术问题，凭借自己对技术的感觉很快在Stackoverflow搜到了答案，
赶紧把答案复制过去；突然又收到产品群发的一个邮件需要你去撕逼，写了几句之后，觉得某个措辞应该放到后面，刚剪切了第一句；经理在IM上让你把某个需求的Jira链接发给他..你如果经常遇到这种情况，首先考虑的应该是把所有的打扰都屏蔽掉，如果做不到，那就安装一个剪贴板工具吧。&lt;/p&gt;

&lt;p&gt;Windows下推荐ditto，Mac下推荐Alfred自带的。顺便提一下，类似Mac下的Alfred这种工具（Windows下的有TotalCommander），如果能研究透彻，会极大的提高工作效率。&lt;/p&gt;

&lt;h2 id=&#34;抓包&#34;&gt;抓包&lt;/h2&gt;

&lt;p&gt;现在很多产品和需求都需要通过网络与用户或其他服务打交道。抓包工具可以把自己交付物当做一个黑盒，去发现和定位问题。Windows下的Fiddler，Mac下推荐Charles都非常强大。这两者通过安装证书，甚至可以直接抓取Https的网络。&lt;/p&gt;

&lt;p&gt;基于Web的应用，也可以直接考虑浏览器的开发者工具。例如，Chrome的开发者工具就非常强大：可以直接找到元素的XPath，在Console运行JS代码，通过修改元素的文本来P图。&lt;/p&gt;

&lt;p&gt;如果想做更深层次的抓包，可以研究下Wireshark。&lt;/p&gt;

&lt;h2 id=&#34;文本编辑器&#34;&gt;文本编辑器&lt;/h2&gt;

&lt;p&gt;程序员经常要跟各种日志和数据打交道，一个熟悉的文本编辑器可以事半功倍。&lt;/p&gt;

&lt;p&gt;笔者比较常用的功能有列模式，例如把一些数据直接编辑成SQL；正则替换，例如从不太标准的日志中提取自己所需的内容。&lt;/p&gt;

&lt;p&gt;文本编辑器免费的有Sublime Text/Atom,商业软件有UltraEdit等。另外还有一种基于终端的文本编辑器，例如Nano/Vim/Emacs。如果从事Server开发，掌握这些编辑器的查找、修改、行号等功能是非常有必要的。&lt;/p&gt;

&lt;h2 id=&#34;笔记&#34;&gt;笔记&lt;/h2&gt;

&lt;p&gt;在这个信息爆炸的时代，搜索引擎有时候也捉襟见肘。子曰：&amp;rdquo;吾日三省吾身&amp;rdquo;，做笔记是一个非常好的反省自己的手段，对每天的收获进行记录和整理，也方便后续快速解决类似问题。市面上的笔记产品大同小异，笔者一直使用有道云笔记：支持MarkDown语法，免费，跨平台。
另外，思维导图也是一个非常好的整理自己知识的工具。&lt;/p&gt;

&lt;h2 id=&#34;脚本语言&#34;&gt;脚本语言&lt;/h2&gt;

&lt;p&gt;假如突然接到一个这样的临时需求：需要给客户端提供一个模拟服务器，根据请求内容的不同，返回相应的Json。这个需求用Python SimpleHTTPServer十几行代码就能实现。很多类似的临时需求，还有需要快速验证的想法，以及粘合多个系统，脚本语言是最合适的。在自己的主力开发语言之外熟悉一门脚本语言做很多事都可以事半功倍，也可以体验到另一种编程文化。&lt;/p&gt;

&lt;p&gt;有很多脚本语言（shell,Ruby,Perl等等）可以选择，笔者推荐Python。Python这些年在很多领域都重新受到追捧。Python有2和3两种，推荐直接上手3，虽然很多老版本服务器系统自带的还是2，但是现在基本上所有的主流库都已经支持Python3了。&lt;/p&gt;

&lt;p&gt;如果单纯的当做一个脚本语言，只需要了解如何处理字符串，常用数据结构，如何处理文件和网络IO,基本就满足日常需求了。&lt;/p&gt;

&lt;p&gt;另外推荐一下Jupyter Notebook，这是一个web版的Python编辑、运行和演示环境(也支持R等其他语言)，可以和shell等环境变量交互。很多人在服务器上直接运行notebook,然后在本地web端调试python程序。&lt;/p&gt;

&lt;h2 id=&#34;自己的codebase&#34;&gt;自己的CodeBase&lt;/h2&gt;

&lt;p&gt;将自己Coding经验提炼为一个CodeBase。经常需要的用模块，如Web框架、模板引擎、Http请求、单元测试以及Mock、Cache、调度、Metric、时间处理、安全、日志、XML/Excel解析等等，每一个模块都有三四种可以选择的技术，选择一个自己熟悉的，构建自己的软件开发栈，这样遇到各种需求都能快速基于自己的CodeBase的实现。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imply和Confluent破解</title>
      <link>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</guid>
      <description>

&lt;p&gt;其实这两个软件还是比较容易破解的。因为：&lt;/p&gt;

&lt;p&gt;首先开发者并没有把精力放到反破解上面，相关功能做的很简单;&lt;/p&gt;

&lt;p&gt;第二没有服务器交互的反破解还是很难的，尤其是对于Java这种容易反编译的软件来说。话说过来，即时是有服务器交互，例如Idea，也有人搞出各种License Server出来。&lt;/p&gt;

&lt;h2 id=&#34;imply-pivot&#34;&gt;Imply Pivot&lt;/h2&gt;

&lt;p&gt;搜索&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -ri &#39;Pivot evaluation license expired&#39; .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现有个文件&lt;code&gt;./node_modules/@implydata/im-auth/build/server/utils/license-manager/license-manager.js&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;里面有这个逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        var e = path.join(this.varDir, &amp;quot;.pivot-first-run&amp;quot;);
        return Q(fs.readFile(e, {encoding: &amp;quot;utf-8&amp;quot;})).then(function (e) {
            var r = new Date(e.trim());
            if (isNaN(r)) throw new Error(&amp;quot;invalid date&amp;quot;);
            return {created: false, firstRun: r}
        }).catch(function (r) {
            var t = new Date;
            return Q(fs.writeFile(e, t.toISOString())).then(function () {
                return {created: true, firstRun: t}
            })
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到文件&lt;code&gt;find . -name &amp;quot;.imply-first-run&amp;quot;&lt;/code&gt;
把里面日期改一下就行了&lt;/p&gt;

&lt;h2 id=&#34;confluent&#34;&gt;Confluent&lt;/h2&gt;

&lt;p&gt;Confluent的前端要求比较简单，是一个Java Web项目（静态资源也用jar打包），通过Guice来管理对象依赖。反编译&lt;code&gt;io.confluent.controlcenter&lt;/code&gt;相关jar。很快发现这段代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; return License.baseClaims(&amp;quot;demo&amp;quot;, bfa.creationTime().toMillis() + TimeUnit.DAYS.toMillis(30L), true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以其实修改该文件的createTime就行了..&lt;/p&gt;

&lt;p&gt;不过这个破解花了我半天时间..&lt;/p&gt;

&lt;p&gt;坑一：一开始搞反了..一直没报错 还以为没生效&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jar uf /service/app/confluent-4.0.0/share/java/confluent-control-center/control-center-4.0.0.jar io/confluent/controlcenter/license/LicenseModule.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;坑二：zsh ll不显示隐藏文件，习惯把ll = ls-al 而不是ls -ah&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Druid的一些实践</title>
      <link>https://lawulu.github.io/post/Druid%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Druid%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E8%B7%B5/</guid>
      <description>

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;h3 id=&#34;文档&#34;&gt;文档&lt;/h3&gt;

&lt;p&gt;参考 &lt;a href=&#34;https://docs.imply.io/on-premise/cluster&#34;&gt;https://docs.imply.io/on-premise/cluster&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;不同之处：
- 使用了多个ZK
- 修改&lt;code&gt;imply/conf/druid/middleManager/runtime.properties&lt;/code&gt;的&lt;code&gt;druid.worker.capacity&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;监控&#34;&gt;监控&lt;/h3&gt;

&lt;p&gt;官方提供两个扩展graphite-emitter和statsd-emitter，跟我们现在用的Zabbix+grafana不能无缝对接。需要调研&lt;/p&gt;

&lt;h3 id=&#34;日志滚动&#34;&gt;日志滚动&lt;/h3&gt;

&lt;p&gt;因为启动脚本是共用的，而不同的模块需要指定不同的路径，修改起来比较麻烦。所以直接使用logrotate&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/logrotate.d/druid

/root/imply/var/sv/historical.log
/root/imply/var/sv/middleManager.log
/root/imply/var/sv/broker.log
/root/imply/var/sv/coordinator.log
/root/imply/var/sv/overlord.log
{
    daily
    rotate 7
    copytruncate
    dateext
    compress
    delaycompress
    missingok
    notifempty
}


&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;h3 id=&#34;topn取代groupby&#34;&gt;TopN取代GroupBy&lt;/h3&gt;

&lt;p&gt;可以参考Imply生成的Json，基本都是用的TopN，默认是&lt;code&gt;max(1000, threshold)&lt;/code&gt;，一般来说，Top1000的话前900肯定是准确的.
- 为什么TopN会快？
&lt;a href=&#34;https://groups.google.com/forum/#!topic/druid-development/zUKGEkSAAMo&#34;&gt;https://groups.google.com/forum/#!topic/druid-development/zUKGEkSAAMo&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;使用kis而非tranquility&#34;&gt;使用KIS而非Tranquility&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Tranquility 时间窗口太死，不适合我们的场景&lt;/li&gt;
&lt;li&gt;架构/代码复杂&lt;/li&gt;
&lt;li&gt;测试有丢数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;利用hyperhyperlog&#34;&gt;利用hyperHyperLog&lt;/h3&gt;

&lt;p&gt;将UserId和其他Id拼在一起，完成group by userId, bizId&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;hyperUnique&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;uniUser&amp;quot;,
  &amp;quot;fieldName&amp;quot;: &amp;quot;userId&amp;quot;
},
{
  &amp;quot;type&amp;quot;: &amp;quot;hyperUnique&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;uniAppUser&amp;quot;,
  &amp;quot;fieldName&amp;quot;: &amp;quot;appUserId&amp;quot;
},
{
   &amp;quot;type&amp;quot;: &amp;quot;hyperUnique&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;uniCampaignUser&amp;quot;,
  &amp;quot;fieldName&amp;quot;: &amp;quot;campaignUserId&amp;quot;
},


&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;维护&#34;&gt;维护&lt;/h2&gt;

&lt;h3 id=&#34;废弃掉segement&#34;&gt;废弃掉Segement&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;update ad_druid2.druid_segments  set used = 0 
where id &amp;lt; &#39;ad_request_2017-09-27T08:00:00.000+08:00_2017-09-28T08:00:00.000+08:00_2017-09-27T00:00:06.479Z&#39;
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;迁移&#34;&gt;迁移&lt;/h2&gt;

&lt;p&gt;公司重新购买了Hadoop集群，硬盘不足的问题解决了。想把HDFS作为Deep Storage用起来。先把思路记录下来，等折腾完了再补充。&lt;/p&gt;

&lt;h3 id=&#34;mysql到druid&#34;&gt;Mysql到Druid&lt;/h3&gt;

&lt;h4 id=&#34;确定时间点&#34;&gt;确定时间点&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;按照UTC时间划分&lt;/li&gt;
&lt;li&gt;将Druid之前试跑数据废弃&lt;/li&gt;
&lt;li&gt;将Mysql数据按天生成Json文件，批量导入&lt;/li&gt;
&lt;li&gt;数据校验&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;影响&#34;&gt;影响&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;查询需要并两个dataSource&lt;/li&gt;
&lt;li&gt;Mysql数据没有Uniq相关的Metric&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nfs-based到hdfs-based&#34;&gt;NFS based到HDFS based&lt;/h3&gt;

&lt;h4 id=&#34;目标&#34;&gt;目标&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;尽可能快的迁移&lt;/li&gt;
&lt;li&gt;数据完整性和正确&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;方案一-基于offset&#34;&gt;方案一：基于Offset&lt;/h4&gt;

&lt;h5 id=&#34;步骤&#34;&gt;步骤：&lt;/h5&gt;

&lt;p&gt;①关掉旧Druid的KIS服务，并记录下消费的offset&lt;/p&gt;

&lt;p&gt;②修改Segments的名字&lt;/p&gt;

&lt;p&gt;③迁移Segements到HDFS&lt;/p&gt;

&lt;p&gt;④重新Index&lt;/p&gt;

&lt;p&gt;⑤在新Druid上面启动KIS，Offset设定&lt;/p&gt;

&lt;h5 id=&#34;问题&#34;&gt;问题：&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;KIS的Offset问题&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;保存在metadata的DruidTask中？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;startPartitions&amp;quot; : {
			&amp;quot;partitionOffsetMap&amp;quot; : {
				&amp;quot;11&amp;quot; : 3,
				&amp;quot;2&amp;quot; : 3,
				&amp;quot;5&amp;quot; : 3,
				&amp;quot;8&amp;quot; : 3
			},
			&amp;quot;topic&amp;quot; : &amp;quot;topic1&amp;quot;
			
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;没有找到直接的办法，复制任务风险太大，可能需要修改源代码&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;同一Segment的数据是两个集群生成的，合并起来的风险？例如segment的Id是递增的&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;方案二-基于时间划分&#34;&gt;方案二：基于时间划分&lt;/h4&gt;

&lt;h5 id=&#34;步骤-1&#34;&gt;步骤：&lt;/h5&gt;

&lt;p&gt;①在新集群上面启动KIS服务，两个集群同时跑一段时间&lt;/p&gt;

&lt;p&gt;②根据日期切割&lt;/p&gt;

&lt;h5 id=&#34;问题-1&#34;&gt;问题：&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;对kafka集群的压力double&lt;/li&gt;
&lt;li&gt;按照日期切割依旧存在合并时候Indexing问题&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;方案二的技术风险要小一些。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transform技术选型：Spark Structured Streaming Vs Pure Scala</title>
      <link>https://lawulu.github.io/post/Transform%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%EF%BC%9ASpark%20Vs%20Scala/</link>
      <pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Transform%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%EF%BC%9ASpark%20Vs%20Scala/</guid>
      <description>

&lt;h2 id=&#34;需求&#34;&gt;需求&lt;/h2&gt;

&lt;p&gt;公司要上实时，需要一个应用对原始日志进行转换，简单来说：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从Kafka读取数据，然后写回到Kafka&lt;/li&gt;
&lt;li&gt;对日志和数据进行合并&lt;/li&gt;
&lt;li&gt;完成归因、去重、真实点击等其他需求&lt;/li&gt;
&lt;li&gt;完成计费&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有这么几个技术需求：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Json的处理，FileBeat产生的In和Druid需要的Out都是Json形式。&lt;/li&gt;
&lt;li&gt;Kafka读写&lt;/li&gt;
&lt;li&gt;外部数据源，Mysql，Redis和MongoDB的读取&lt;/li&gt;
&lt;li&gt;逻辑处理&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;spark-or-scala&#34;&gt;Spark or Scala&lt;/h2&gt;

&lt;p&gt;所谓的Spark，即Spark Structured Streaming，所谓的Scala是Scala based on Play Framework
优先考虑的是Spark（Spark Structured Streaming），因为Spark可以做流处理和即席查询，某种程度上可以复用很多逻辑，并且可以接入Hive-serde表，与无缝与Hive切换。&lt;/p&gt;

&lt;h3 id=&#34;技术需求&#34;&gt;技术需求&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Json

&lt;ul&gt;
&lt;li&gt;Spark：问题不大，但需要调研，Scala貌似没什么好用的Json库&lt;/li&gt;
&lt;li&gt;Scala：Play自带Json库&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka

&lt;ul&gt;
&lt;li&gt;Spark：官方提供有类库，但是屏蔽的细节太多了。现在项目内没人清楚DStream/Spark.readStream是怎么个机制&lt;/li&gt;
&lt;li&gt;Scala：Kafka官方的Client是Java的，Akka Stream有对应的封装。如果不能用，解决起来也比较方便&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;外部DB

&lt;ul&gt;
&lt;li&gt;Spark：问题不大，需要调研如果精细的控制Partiton与连接之间的关系，还有Lookup问题&lt;/li&gt;
&lt;li&gt;Scala：Scala有各种Reactive的驱动&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;非技术需求&#34;&gt;非技术需求&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;监控&amp;amp;日志

&lt;ul&gt;
&lt;li&gt;用Spark的话，现有公司/阿里云的各种基础设施很难用的上&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;补数，关键是对Kafka的Offset做精细的控制

&lt;ul&gt;
&lt;li&gt;搜了一下，Spark这块放到Checkpoint了&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;TroubleShooting

&lt;ul&gt;
&lt;li&gt;Spark：？？？公司现在并没有用Spark跑大规模的应用&lt;/li&gt;
&lt;li&gt;Scala：Akka这块&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;所谓Spark的优势：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;以RDD/DataFrame/SQL形式提供的对数据处理的DSL，虽然比Hive效率更高，可以和Scala程序同时工作&lt;/li&gt;
&lt;li&gt;横向扩展以及容错&lt;/li&gt;
&lt;li&gt;Broadcast/Accumulator等原语&lt;/li&gt;
&lt;li&gt;可以有效利用HDFS对应的计算资源，并且做为一个Unified的平台，理论上可以重用很多逻辑&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Spark的问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Structured Streaming API 不稳定，我自己测试发现有Job莫名奇妙不运行的情况。&lt;/li&gt;
&lt;li&gt;任务调度和执行都是黑盒&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Spark并不是银弹，更倾向于用纯Java应用来做。Spark的优势在一个纯Transform的应用里面发挥不出来（上面三点：1显然不如直接写程序，对于2瓶颈在Kafka，3引入Redis可以解决类似问题。
）。后续可以研究一下Kafka Streams。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>数据需要收集哪些数据</title>
      <link>https://lawulu.github.io/post/%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E6%94%B6%E9%9B%86%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E6%94%B6%E9%9B%86%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE/</guid>
      <description>

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近线上遇到一些问题，分析起来还是要抓瞎，完全不知道怎么去定位。只能从代码去分析&amp;hellip;觉得之前的设计的数据收集太简单了，只是以报表需求为导向设计的。那我们还需要收集哪些数据呢？&lt;/p&gt;

&lt;h2 id=&#34;能解释业务的数据&#34;&gt;能解释业务的数据&lt;/h2&gt;

&lt;h3 id=&#34;所有业务要串起来&#34;&gt;所有业务要串起来&lt;/h3&gt;

&lt;p&gt;业务有三条线：
- Active
- Config
- Preload-&amp;gt;Impression-&amp;gt;Event-&amp;gt;Click-&amp;gt;Install&lt;/p&gt;

&lt;p&gt;改动：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;最核心的广告生命周期是否可以用一个Id呢？如果不能，每一步要讲上一步的id给带上来。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CDN的下载要带上一个preloadId，然后开启参数过滤，通过关联preloadId将下载流程也关联到核心业务上去。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重复Id上报的问题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;server日志&#34;&gt;Server日志&lt;/h3&gt;

&lt;p&gt;为了帮助运营快速的定位业务问题，如为什么某些App的填充低？为什么Erpm最低的某个Campaign还拿到了量？可以考虑将Serving的日志和业务关联起来。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK要用起来，亦可以调研一下阿里的日志服务&lt;/li&gt;
&lt;li&gt;规范Server输出，尤其是Filter广告时候，要将挑选广告失败的原因记录到日志里面&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;可追溯每个客户端的行为的数据&#34;&gt;可追溯每个客户端的行为的数据&lt;/h2&gt;

&lt;p&gt;最近几个痛点：流量端说我们的填充率对低，而我们报表的填充率非常高；点击广告主上报跟服务器上报总是对不上；点击率中有真实点击和误点击区分不开。所以要收集这些数据：
- APP对SDK的接口调用都要记录
- SDK和Server之外的所有交互（点击上报，CDN下载）都要记录
- 以上记录可以通过开关来控制
- EndCard的点击率要能区分点击区域，找到真实的点击率&lt;/p&gt;

&lt;h2 id=&#34;其他数据&#34;&gt;其他数据&lt;/h2&gt;

&lt;p&gt;抓友商的包，发现友商有一个SessionStart和SessionEnd的接口，这个要加上。貌似SessionEnd不准，因为每次进入后台，SDK没有机会发请求到Server。&lt;/p&gt;

&lt;h2 id=&#34;实施&#34;&gt;实施&lt;/h2&gt;

&lt;h3 id=&#34;ab测试准备&#34;&gt;AB测试准备&lt;/h3&gt;

&lt;p&gt;增加一个Header&lt;code&gt;x-clad-tag&lt;/code&gt;,在Config接口下发下去，给用户打标签，并贯彻到整个业务流程&lt;/p&gt;

&lt;h3 id=&#34;上报服务拆分&#34;&gt;上报服务拆分&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Impr,Click等事件上报因为只是记一个数字，要和Preload业务从逻辑和部署上都分开，避免互相干扰&lt;/li&gt;
&lt;li&gt;并且上报地址应该是Preload下发下去，而非写死，并且支持同一事件的多地址上报。这样易于控制，甚至以后可以作弊（按照某人的说法：&amp;rdquo;&lt;code&gt;我们不主动作恶，但是在恶劣的环境下，要有保护自己的手段。&lt;/code&gt;&amp;ldquo;）&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Preload接口和Play接口的兴衰史</title>
      <link>https://lawulu.github.io/post/Preload%E5%92%8CPlay%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B4%E8%A1%B0%E5%8F%B2/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Preload%E5%92%8CPlay%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B4%E8%A1%B0%E5%8F%B2/</guid>
      <description>

&lt;p&gt;通过对业务的深入理解和梳理，来优化架构。先说结论：&lt;strong&gt;Preload的本质是告诉SDK下一次Impression应该展示哪个广告&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;对于Native移动视频广告来说，一方面网络条件所限，另一方面要提升用户和广告主的体验，播放广告基本都是以预加载的形式提前把广告加载完成。这个特性让Ad Serving的流程大大不同。&lt;/p&gt;

&lt;h2 id=&#34;第一版的做法&#34;&gt;第一版的做法&lt;/h2&gt;

&lt;h3 id=&#34;提供了两个接口&#34;&gt;提供了两个接口:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload,用来告诉SDK有哪些广告可以下载，然后告诉SDK开始下载，一般是三到五条。&lt;/li&gt;
&lt;li&gt;Play，在SDK有曝光机会的时候，SDK将已经预加载完成的广告告诉Server，Server从中挑出可播的广告。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;问题&#34;&gt;问题&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload和Play两个接口时间上有间隔，在另一个时间点，各种条件已经变化了，需要重新挑选广告。所以同一次曝光机会，可能需要挑选两次。&lt;/li&gt;
&lt;li&gt;Play接口的响应时间要求很高，客户端的网络又不稳定。&lt;/li&gt;
&lt;li&gt;整个链路太长，又有些条件不可控，所以Ad Serving的逻辑很难优化。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;梳理业务-重新定义&#34;&gt;梳理业务，重新定义&lt;/h2&gt;

&lt;h3 id=&#34;其他部门角度&#34;&gt;其他部门角度&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;对商务来说，尾量可以接受，甚至非常正常。这意味着，各种定向控制并不需要那么严谨。&lt;/li&gt;
&lt;li&gt;对运营来说，CDN的消耗相比转化率的提高并不重要，更迫切需要一个可以快速干扰Ranking的办法。&lt;/li&gt;
&lt;li&gt;对产品来说，需要让事情简单化。我们的Preload的挑选广告就对应传统Ad产品中的那次挑选广告就OK。&lt;/li&gt;
&lt;li&gt;对数据来说，填充率的计算需要和流量方匹配。对应第一版来说，填充率，是用Play接口的成功返回/Play接口的调用。&lt;/li&gt;
&lt;li&gt;对技术来说，Play接口要求太高，优化起来太困难。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;那么&#34;&gt;那么&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload接口挑选广告，Play接口只做Impression的上报&lt;/li&gt;
&lt;li&gt;激进的来说，每次Preload只预加载一次，播完就删&lt;/li&gt;
&lt;li&gt;在应用启动和每次播放完成之后调用一次Preload&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Play的逻辑前置和合并到Preload里面，Play沦为一个上报接口&lt;/p&gt;

&lt;h2 id=&#34;推而广之&#34;&gt;推而广之&lt;/h2&gt;

&lt;p&gt;SDK模式下，每个接口对userId是否是合法用户的检查也没有意义。&lt;/p&gt;

&lt;p&gt;所谓SDK模式，因为SDK是自家的，流量和费用都是通过商务谈的。API的设计是建立在SDK可控和可以信任的基础之上的。现有架构之下，每次激活相当于获得了一个调用其他API的Token，而这个Token是保存在客户端本地的，所以，没必要去单独Check UserId，
即使检查不通过，SDK自动会去调用用户激活接口，又会得到一个已经激活UserId。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用Scala开发自己的DSL</title>
      <link>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91%E8%87%AA%E5%B7%B1%E7%9A%84DSL/</link>
      <pubDate>Tue, 30 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91%E8%87%AA%E5%B7%B1%E7%9A%84DSL/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;yinwang写了一篇关于DSL的文章：&lt;a href=&#34;http://www.yinwang.org/blog-cn/2017/05/25/dsl&#34;&gt;http://www.yinwang.org/blog-cn/2017/05/25/dsl&lt;/a&gt;
，认为大部分情况是不需要DSL的：&lt;code&gt;绝大部分 DSL 的存在，都是因为设计它的人没有理解问题的本质，没有意识到这问题其实不需要通过设计新的语言来解决。&lt;/code&gt;并且拿出Scala来当做反例，从某种程度上，我很支持yinwang的观点，因为最近看Scala的一些项目，经常会有一些奇怪的操作符，极大的增加了阅读的难度。&lt;/p&gt;

&lt;p&gt;yinwang认为因为大家已经熟悉了本语言，所以对库函数的认知会更容易一些。比如，大部分程序员已经习惯了C语言设定的DSL（比如for if while)，所以跨语言学习会减少了很多的难度。&lt;/p&gt;

&lt;p&gt;虽然yinwang反对，我们还是要好奇一下，怎么用Scala开发自己的DSL&lt;/p&gt;

&lt;h2 id=&#34;实现dsl&#34;&gt;实现DSL&lt;/h2&gt;

&lt;h3 id=&#34;scala为什么适合dsl&#34;&gt;Scala为什么适合DSL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;对于一个参数的方法可以省略点/括号&lt;/li&gt;
&lt;li&gt;可以使用特殊符号做方法&lt;/li&gt;
&lt;li&gt;函数式编程&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;定义&#34;&gt;定义&lt;/h3&gt;

&lt;p&gt;参考：&lt;a href=&#34;https://dzone.com/articles/rolling-your-own-dsl-in-scala&#34;&gt;https://dzone.com/articles/rolling-your-own-dsl-in-scala&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>公司整体架构上的一些思考</title>
      <link>https://lawulu.github.io/post/%E5%85%AC%E5%8F%B8%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E4%B8%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
      <pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%85%AC%E5%8F%B8%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E4%B8%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
      <description>

&lt;h2 id=&#34;server&#34;&gt;Server&lt;/h2&gt;

&lt;h3 id=&#34;性能&#34;&gt;性能&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;流量高峰时候遇到过响应时间过大

&lt;ul&gt;
&lt;li&gt;充分利用WEB服务器性能(例如OpenResty或者Nginx+NgixScirpt），将简单通用的业务逻辑（如签名校验和去重逻辑）前置，降低后端容器的负荷，提高并发和响应&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;客户端重试导致的服务器雪崩

&lt;ul&gt;
&lt;li&gt;部分接口静态化（例如config接口），服务降级，例如直接返回null
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;redis依赖&#34;&gt;Redis依赖&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;太过于依赖Redis

&lt;ul&gt;
&lt;li&gt;引入二级缓存，降低和隔离对Redis的依赖&lt;/li&gt;
&lt;li&gt;Redis数据分只读和读写两种，可以放到两个集群里面&lt;/li&gt;
&lt;li&gt;上Redis集群？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;阿里云Redis

&lt;ul&gt;
&lt;li&gt;考虑在某些场景用阿里云Redis
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;监控和日志&#34;&gt;监控和日志&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;现在已经有了基于Zabbix的机器性能的监控，需要业务层次的分布式监控系统（例如Cat），方便快速洞察和响应线上问题&lt;/li&gt;
&lt;li&gt;日志统一格式到Json&lt;/li&gt;
&lt;li&gt;ELK需要提到日程了&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;数据&#34;&gt;数据&lt;/h2&gt;

&lt;h3 id=&#34;数据质量&#34;&gt;数据质量&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;使用python grep按小时检查原始日志，来和Hive对比保证数据完整和正确性
&lt;code&gt;
p = subprocess.Popen(&amp;quot;&amp;quot;&amp;quot; cat &amp;quot;&amp;quot;&amp;quot; + file + &amp;quot;&amp;quot;&amp;quot;| grep -avF &#39;[2.3.0]&#39; | grep -aF &#39;[04]&#39;  |  wc -l &amp;quot;&amp;quot;&amp;quot;, shell=True, stdout=subprocess.PIPE );
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对某些阈值进行报警，例如Preload，没有转化，点击率过低，CDN消耗太低等情况。帮助运营及早洞察线上问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;任务调度&#34;&gt;任务调度&lt;/h3&gt;

&lt;p&gt;现在基于Crontab，缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没有任务依赖关系，通过较长的时间间隔来保证任务的先后顺序。&lt;/li&gt;
&lt;li&gt;重跑起来比较麻烦。&lt;/li&gt;
&lt;li&gt;报警实现起来有点难&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决方案：&lt;/p&gt;

&lt;p&gt;根据开源实现，将现有的任务（任务类型主要：shell,hive,sqoop）整合到任务调度系统中。
例如：&lt;a href=&#34;http://azkaban.github.io/azkaban/docs/latest/#new-hive-type&#34;&gt;http://azkaban.github.io/azkaban/docs/latest/#new-hive-type&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;查询引擎&#34;&gt;查询引擎&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;因为某些维度太细（手机型号，城市）报表查询慢

&lt;ul&gt;
&lt;li&gt;为报表预生成数据&lt;/li&gt;
&lt;li&gt;更换查询引擎，如Druid，Kylin？&lt;/li&gt;
&lt;li&gt;Ali RDS是否有优化Mysql（例如引擎）的可能&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;对Serving的支持：数据和Server的界限问题。例如ImprNum per Campaign per app的计数，这个应该放到data还是server？&lt;/li&gt;
&lt;li&gt;分层过多，如何快速响应业务变化？&lt;/li&gt;
&lt;li&gt;如何能快速定位问题？如为什么填充下降了？不填充的原因要列出来。如为什么有些erpm低的拿到的量大？(4G网络下面包体大于100M的不下发， 前面高erpm的经过两轮（第一轮是否在线，第二轮数据网络条件下面包体大小过滤）过滤掉， 正好轮到了1789下发)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;统计维度&#34;&gt;统计维度&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;如何才能将&lt;code&gt;产生转化的意愿和原因&lt;/code&gt;，可视化出来？有效点击（对应安装的点击）？用户观看数？&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SparkML-卡方检验</title>
      <link>https://lawulu.github.io/post/SparkML-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/SparkML-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/</guid>
      <description>

&lt;h2 id=&#34;spark例子&#34;&gt;Spark例子&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;//org.apache.spark.examples.ml.ChiSquareTestExample

val data = Seq(
      (0.0, Vectors.dense(0.5, 10.0)),
      (0.0, Vectors.dense(1.5, 20.0)),
      (1.0, Vectors.dense(1.5, 30.0)),
      (0.0, Vectors.dense(3.5, 30.0)),
      (0.0, Vectors.dense(3.5, 40.0)),
      (1.0, Vectors.dense(3.5, 40.0))
    )

val df = data.toDF(&amp;quot;label&amp;quot;, &amp;quot;features&amp;quot;)
val chi = ChiSquareTest.test(df, &amp;quot;features&amp;quot;, &amp;quot;label&amp;quot;).head
println(&amp;quot;pValues = &amp;quot; + chi.getAs[Vector](0))
println(&amp;quot;degreesOfFreedom = &amp;quot; + chi.getSeq[Int](1).mkString(&amp;quot;[&amp;quot;, &amp;quot;,&amp;quot;, &amp;quot;]&amp;quot;))
println(&amp;quot;statistics = &amp;quot; + chi.getAs[Vector](2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chi: org.apache.spark.sql.Row = [[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为Vector是两列，所以结果也是两列，即，需要检测每一个维度和label相关性。&lt;/p&gt;

&lt;h2 id=&#34;几个概念&#34;&gt;几个概念&lt;/h2&gt;

&lt;h3 id=&#34;卡方检验&#34;&gt;卡方检验&lt;/h3&gt;

&lt;p&gt;卡方检验两个用途：
- 检测一批样本是否符合某种分布(Goodeness of Fit Test)
- 检测两个变量是否独立（本文着重讨论相关性检测）&lt;/p&gt;

&lt;h3 id=&#34;卡方检验计算过程&#34;&gt;卡方检验计算过程&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;假设变量是独立的，没有相关性&lt;/li&gt;
&lt;li&gt;计算统计值：按照假设计算理论值，然后计算每一项的sqrt（实际值-理论值）/理论值，把所有的项都加起来&lt;/li&gt;
&lt;li&gt;自由度，（distinctCount(label)-1）x（distinctCount(feature)-1），可以用(行-1) x(列-1)来理解&lt;/li&gt;
&lt;li&gt;查表求p值，如果p值非常小（例如0.05），就拒绝假设&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;计算过程&#34;&gt;计算过程&lt;/h2&gt;

&lt;h3 id=&#34;举个例子&#34;&gt;举个例子&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt; val data = Seq(
      (0.0, Vectors.dense(0.5)),
      (0.0, Vectors.dense(0.5)),
      (1.0, Vectors.dense(1.5)),
      (0.0, Vectors.dense(3.5)),
      (2.0, Vectors.dense(3.5)),
      (1.0, Vectors.dense(1.5))
    )

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;统计每个维度取值和label的对应关系&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;label&lt;/th&gt;
&lt;th&gt;0.5&lt;/th&gt;
&lt;th&gt;1.5&lt;/th&gt;
&lt;th&gt;3.5&lt;/th&gt;
&lt;th&gt;总数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;合计&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;假设该维度和label是独立的，那么无论维度如何取值，取到的label为0的数目都是总数的1/2。&lt;/p&gt;

&lt;p&gt;维度取0.5的一共为2个，则理论上，0.5对应的label数目为1. 做出对应的表格如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;label&lt;/th&gt;
&lt;th&gt;0.5&lt;/th&gt;
&lt;th&gt;1.5&lt;/th&gt;
&lt;th&gt;3.5&lt;/th&gt;
&lt;th&gt;总数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;合计&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;计算统计值为8&lt;/p&gt;

&lt;h3 id=&#34;验证统计值&#34;&gt;验证统计值&lt;/h3&gt;

&lt;p&gt;用Python验证：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
from scipy import stats
import fractions
obs = [2, 0, 1, 0, 2, 0, 0, 0, 1]
obs = [2, 0, 1, 0, 2, 0]
# 0, 2, 0, 0, 0, 1]
exp = [1, 1, 1,
       fractions.Fraction(2,3),fractions.Fraction(2,3), fractions.Fraction(2,3),
       fractions.Fraction(1,3),fractions.Fraction(1,3), fractions.Fraction(1,3)]

exp = [1, 1, 1,
       fractions.Fraction(2,3),fractions.Fraction(2,3), fractions.Fraction(2,3)]
       # fractions.Fraction(1,3),fractions.Fraction(1,3), fractions.Fraction(1,3)]

print(stats.chisquare(obs, f_exp = exp ,ddof=4))


obs= [0, 0, 1]
#exp= [fractions.Fraction(1,3),fractions.Fraction(1,3), fractions.Fraction(1,3)]
exp= [0.33333, 0.33333, 0.33333]

print(stats.chisquare(obs, f_exp = exp ))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果正确&lt;/p&gt;

&lt;h3 id=&#34;计算p值&#34;&gt;计算P值&lt;/h3&gt;

&lt;p&gt;//TODO&lt;/p&gt;

&lt;h3 id=&#34;验证结论&#34;&gt;验证结论&lt;/h3&gt;

&lt;p&gt;假设数据是这样子的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; val data = Seq(
      (0.0, Vectors.dense(0.5)),
      (0.0, Vectors.dense(0.5)),
      (1.0, Vectors.dense(1.5)),
      (1.0, Vectors.dense(1.5)),
      (2.0, Vectors.dense(3.5)),
      (2.0, Vectors.dense(3.5))
    )

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;肉眼可查，feature和label相关性很高。计算的P值为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pValues = [0.017351265236664526]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样理解：假如feature和label独立不相关的，只有1.7%的可能取到像样本这样的数据。所以假设不成立。&lt;/p&gt;

&lt;p&gt;参考：
&lt;a href=&#34;http://www.statisticshowto.com/probability-and-statistics/chi-square/&#34;&gt;http://www.statisticshowto.com/probability-and-statistics/chi-square/&lt;/a&gt;
&lt;a href=&#34;https://segmentfault.com/a/1190000003719712&#34;&gt;https://segmentfault.com/a/1190000003719712&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>元数据管理</title>
      <link>https://lawulu.github.io/post/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;引子&#34;&gt;引子&lt;/h2&gt;

&lt;h3 id=&#34;业务元数据&#34;&gt;业务元数据&lt;/h3&gt;

&lt;p&gt;最近公司新来一个气场强大的产品，发现有些内部（研发和商务之间经过多次讨论之后形成的）常用的一些话术，他已经在改变对应的说法了。例如广告，因为数据库叫Campaign，所以技术这边都叫Campaign，商务本来已经要从Ad改成了Campaign了，现在有多了个说法叫offer；追踪链接的叫法本来要按照商务的来了，现在愈发混乱了。同一个含义对应的不同的表达，在需求到研发落地和数据流转时候，很容易出问题。&lt;/p&gt;

&lt;p&gt;项目刚起时候，制定一个词典/元数据非常重要，并且要随着业务的发展及时更新。业务元数据可以极大的减少沟通成本。&lt;/p&gt;

&lt;h3 id=&#34;一个悲剧&#34;&gt;一个悲剧&lt;/h3&gt;

&lt;p&gt;IMEI在电信手机上，取到的是一个字符串而不是一个数字。Server发现这个问题之后，没有及时的同步到数据，结果被这个问题又坑了一次。&lt;/p&gt;

&lt;h2 id=&#34;元数据是什么&#34;&gt;元数据是什么&lt;/h2&gt;

&lt;p&gt;瓦利哥（神策数据的创始人）在知乎有一个非常好的专栏（&lt;a href=&#34;https://zhuanlan.zhihu.com/sangwf/20622902&#34;&gt;https://zhuanlan.zhihu.com/sangwf/20622902&lt;/a&gt;
）。有一章专门讲百度的元数据服务的演进。
所谓的元数据服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Schema：表字段的定义&lt;/li&gt;
&lt;li&gt;数据就绪状态&lt;/li&gt;
&lt;li&gt;对外服务的API&lt;/li&gt;
&lt;li&gt;数据审计/变更&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;瓦利哥推荐了HCatalog，有空打算尝试一下，大概看了一下，是强制要求遵循某种规范。&lt;/p&gt;

&lt;h2 id=&#34;业界解决方案&#34;&gt;业界解决方案&lt;/h2&gt;

&lt;h3 id=&#34;wherehows&#34;&gt;Wherehows&lt;/h3&gt;

&lt;p&gt;LinkedIn开源了一个数据发现和血缘管理工具，叫做Wherehows，开发团队全华人。解决的是类似的问题，不过并不是强制要求开发团队生产数据时候主动和metadata绑定，而是通过分析Scheduler log和Repo(如Azkaban和Hadoop job)，爬取其他数据（如关系数据库和HDFS)来建立元数据，并分析彼此的血缘关系。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal&#34;&gt;https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;google-goods&#34;&gt;Google Goods&lt;/h3&gt;

&lt;p&gt;Google最近写了一篇相关论文，《Goods: Organizing Google’s Datasets》。论文中的Overview图和wherehows特别相似。看来big campany的big data都有相似的需求。Google公开的（可以被公司所有工程师查看的）datasets有26B，20多种格式，每天有很多数据在被删除..总之很有挑战性，但是依然只是一个论文。
Goods支持用户对数据的标注。&lt;/p&gt;

&lt;h2 id=&#34;实践&#34;&gt;实践&lt;/h2&gt;

&lt;p&gt;上面两个对我们现在的公司太重了。现在公司所有的元数据都是通过Excel和Jira来管理，主要还是口头沟通。两个努力的方向：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;考虑想一个jar的方式把日志metadata（字段含义和顺序）给确定下来，日志的输出、收集和处理都依赖于这个jar，以版本号管理。甚至，Hive表结构变更也要依赖于metadata。嗯，首先要把日志统一由CIF(Api Gateway)来输出。&lt;/li&gt;
&lt;li&gt;对于容易出错的东西，一定要拿出来公开讨论，例如公司SDK上报的Platform字段，IPad和Iphone不一样。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>CDN流量日志简单分析</title>
      <link>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;公司运营的很大一个成本就是CDN耗费。对比Impression和CDN消耗量就会很奇怪，不该有这么多的CDN消耗的。&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;下载并处理cdn原始日志&#34;&gt;下载并处理CDN原始日志&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;由于官方提供的合并下载日志工具在Mac下不能用，随机选取一个小时的日志：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;wget https://cdnlog.cn-hangzhou.oss.aliyun-inc.com/streaming.lawulu.com/2017_03_04/streaming.lawulu.com_2017_03_04_1200_1300.gz?spm=5176.8232200.log.d10.83JI60&amp;amp;OSSAccessKeyId=xxxxx&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Python处理分析为CSV&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;pat = ( r&#39;\[(.+)\]\s&#39; #datetime
           &#39;(\d+.\d+.\d+.\d+)\s&#39; #IP address
           &#39;-\s&#39; #proxy ip
            &#39;\d+\s&#39; #responseTime
           &#39;&amp;quot;.+&amp;quot;\s&#39; #referrer
           &#39;&amp;quot;(GET\s\S+mp4)&amp;quot;\s&#39;  # requested file
            &#39;\d+\s&#39; #status
           &#39;\d+\s&#39; #requestSIze
           &#39;(\d+)\s&#39; #responseSize
           &#39;\D+\s&#39; #HIT OR NOT
           &#39;&amp;quot;(.+)&amp;quot;\s&#39; #user agent
           &#39;&amp;quot;\S+&amp;quot;&#39; #contentType
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@see &lt;a href=&#34;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&#34;&gt;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;导入数据库&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA INFILE &#39;cdnlogs.csv&#39; 
INTO TABLE test.cdn_log
FIELDS TERMINATED BY &#39;,&#39; 
LINES TERMINATED BY &#39;\n&#39; 
(date_time,ip,url,rsp_size,ua);
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;数据校验&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;SELECT count(*)  FROM test.cdn_log 
&#39;206619&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cat streaming.lawulu.com_2017_03_04_1200_1300 | wc -l

206618
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;用sql分析&#34;&gt;用Sql分析&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) =1 )a


&#39;110643&#39;,&#39;598635187514&#39;


select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) &amp;gt;1 )a


&#39;32670&#39;, &#39;440010318426&#39;

0.4236
select 440010318426/(598635187514+440010318426) from dual 

select sum(rsp_size) from  cdn_log

1038645505940

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;由于IP,UA,URL可以唯一确定一次UV，我们公司有30%的用户占用了40%的流量，对同一个视频会不停的反复下载。
最后查明，是SDK的bug，正在下载视频时候，如果切换到后台，下载会直接停止。所以会反复下载视频，浪费流量。&lt;/p&gt;

&lt;h2 id=&#34;校验&#34;&gt;校验&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;计算ResponseSize之后和Console里面同一时间段的&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;
CURL  -i -X HEAD &#39;https://streaming.lawulu.com/default/256b9db1-a1ce-4983-8906-4568fa7a9c75-144.mp4&#39;
&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>广协的IP库到底准不住</title>
      <link>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;对于CPM广告，地域定向是非常重要的。广告协会提供了ip库，方便AdNetwork和第三方监控做地域定向。百度默认以ip138做为其&amp;rdquo;IP&amp;rdquo;关键字的第一个入口，我相信ip138的库是比较准的，那广协的IP库和ip138的库差别大吗？&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;随机从线上提取10000个ip-并使用广协库解析成对应城市&#34;&gt;随机从线上提取10000个ip，并使用广协库解析成对应城市&lt;/h3&gt;

&lt;p&gt;从某个时段原始Nginx日志中的ip(300W+)随机取10000条
使用pandas&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;csv = pd.read_csv(&#39;/tmp/ip.log&#39;)
np.savetxt(&#39;/tmp/ipSample.log&#39;,csv.sample(10000).values,fmt=&#39;%s&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;根据ip获取ip138对应的城市&#34;&gt;根据ip获取ip138对应的城市&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# get data from web
def get_data(query):
    url = &amp;quot;http://m.ip138.com/ip.asp?ip=&amp;quot; + query
    res = requests.get(url)
    print &#39;get data for &#39;+ query +&#39; result:&#39; + str(res.status_code)
    if(res.status_code != 200):
        return &#39;Failed Response&#39;
    else:
        return parse_html(res.content)

def parse_html(html):
    res = BeautifulSoup(html,&amp;quot;html.parser&amp;quot;).find_all(&amp;quot;p&amp;quot;, &amp;quot;result&amp;quot;)
    result =split(res[0])
    if len(res)&amp;gt;1:
        result=result+&amp;quot;,&amp;quot;+ split(res[1])
    return result

def split(str):
    try:
        splits=str.string.encode(&amp;quot;utf-8&amp;quot;).split(&#39;：&#39;)
        return splits[1]
    except RuntimeError:
        return &#39;Split Error:&#39;+str

with open(&#39;/tmp/ipSample.log&#39;) as source:
    with open(&#39;ip138Result.csv&#39;,&#39;aw&#39;) as result:
        for line in source:
            ip = line.strip(&#39;\n&#39;)
            ipStr=get_data(ip)
            time.sleep(0.1)
            result.write(ip+&#39;|&#39;+ipStr+&#39;\n&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;取全国60强城市-统计两个结果的差距&#34;&gt;取全国60强城市，统计两个结果的差距&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.mnw.cn/news/china/886245.html&#34;&gt;http://www.mnw.cn/news/china/886245.html&lt;/a&gt; 60强城市&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;citys=[&#39;上海&#39;,&#39;北京&#39;,&#39;广州&#39;,&#39;深圳&#39;,&#39;成都&#39;,&#39;重庆&#39;,&#39;杭州&#39;,&#39;南京&#39;,&#39;沈阳&#39;,&#39;苏州&#39;,&#39;天津&#39;,&#39;武汉&#39;,&#39;西安&#39;,&#39;长沙&#39;,&#39;大连&#39;,&#39;济南&#39;,&#39;宁波&#39;,&#39;青岛&#39;,&#39;无锡&#39;,&#39;厦门&#39;,&#39;郑州&#39;,&#39;长春&#39;,&#39;常州&#39;,&#39;哈尔滨&#39;,&#39;福州&#39;,&#39;昆明&#39;,&#39;合肥&#39;,&#39;东莞&#39;,&#39;石家庄&#39;,&#39;呼和浩特&#39;,&#39;南昌&#39;,&#39;温州&#39;,&#39;佛山&#39;,&#39;贵阳&#39;,&#39;南宁&#39;,&#39;海口&#39;,&#39;湖州&#39;,&#39;唐山&#39;,&#39;临沂&#39;,&#39;嘉兴&#39;,&#39;绍兴&#39;,&#39;南通&#39;,&#39;徐州&#39;,&#39;泉州&#39;,&#39;太原&#39;,&#39;烟台&#39;,&#39;乌鲁木齐&#39;,&#39;潍坊&#39;,&#39;珠海&#39;,&#39;洛阳&#39;,&#39;中山&#39;,&#39;兰州&#39;,&#39;金华&#39;,&#39;淮安&#39;,&#39;吉林&#39;,&#39;威海&#39;,&#39;淄博&#39;,&#39;银川&#39;,&#39;扬州&#39;,&#39;芜湖&#39;,&#39;盐城&#39;,&#39;宜昌&#39;,&#39;西宁&#39;,&#39;襄阳&#39;,&#39;绵阳&#39;]
files =[&amp;quot;ip138Result.csv&amp;quot;,&amp;quot;ipResultV2.log&amp;quot;]
import subprocess, datetime, sys
def checkNums(file,city):
    p = subprocess.Popen(&amp;quot;&amp;quot;&amp;quot; cat &amp;quot;&amp;quot;&amp;quot; + file + &amp;quot;&amp;quot;&amp;quot;| grep &amp;quot;&amp;quot;&amp;quot;+ city +&amp;quot;&amp;quot;&amp;quot;  |wc -l &amp;quot;&amp;quot;&amp;quot;, shell=True, stdout=subprocess.PIPE );
    p.wait()
    out = p.stdout.readlines()[0]
    return str(out).strip()

for city in citys:
    result = city + &amp;quot;\t&amp;quot;
    for file in files:
        result = result +checkNums(file,city)+ &amp;quot;\t&amp;quot;
    print  result
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;北京相差比较大，系统库比Ip138库多出一倍&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其他城市 上海 深圳 南京 苏州 大连 宁波等也相差在20%以上。
可以说跟IP138差别还是比较大的。&lt;/p&gt;

&lt;p&gt;但是三线城市比较准。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Databricks:Spark knowledge base</title>
      <link>https://lawulu.github.io/post/Databricks:Spark%20knowledge%20base/</link>
      <pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Databricks:Spark%20knowledge%20base/</guid>
      <description>

&lt;p&gt;内容来自：
&lt;a href=&#34;https://databricks.gitbooks.io/databricks-spark-knowledge-base/&#34;&gt;https://databricks.gitbooks.io/databricks-spark-knowledge-base/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;最佳实践&#34;&gt;最佳实践&lt;/h2&gt;

&lt;h3 id=&#34;避免使用groupbykey&#34;&gt;避免使用GroupByKey&lt;/h3&gt;

&lt;p&gt;在使用&lt;code&gt;groupByKey&lt;/code&gt;时候，Spark引擎并不清楚分组之后要做什么，所以无法在数据交换之前在Partition内部提前运算，而&lt;code&gt;reduceByKey&lt;/code&gt;不同，可以在分区内部提前调用&lt;code&gt;reduceByKey&lt;/code&gt;的&lt;code&gt;func&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;不要将大数据量copy到driver中&#34;&gt;不要将大数据量Copy到Driver中&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;myVeryLargeRDD.collect()&lt;/code&gt;这句话会试图将所有数据都Copy到Driver中，导致内存溢出和程序崩溃。为查看数据，可以使用&lt;code&gt;take&lt;/code&gt;或者&lt;code&gt;takeSample&lt;/code&gt;，或者将数据写到文件或者数据库。&lt;/p&gt;

&lt;h3 id=&#34;优雅的处理错误数据&#34;&gt;优雅的处理错误数据&lt;/h3&gt;

&lt;p&gt;使用&lt;code&gt;map&lt;/code&gt;、&lt;code&gt;filter&lt;/code&gt;、&lt;code&gt;flatMap&lt;/code&gt;来过滤错误数据&lt;/p&gt;

&lt;h2 id=&#34;常规故障处理&#34;&gt;常规故障处理&lt;/h2&gt;

&lt;h3 id=&#34;task-not-serializable&#34;&gt;Task not serializable&lt;/h3&gt;

&lt;p&gt;这是因为堆对象只有在被序列化时候，才能在节点之间传递。解决办法:
- 实现序列化
- Declare the instance only within the lambda function passed in map.
- 将实例声明为static,这样在每个节点都会重新初始化
- 使用&lt;code&gt;rdd.forEachPartition&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rdd.forEachPartition(iter -&amp;gt; {
     NotSerializable notSerializable = new NotSerializable();
   
     // ...Now process iter
   });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;缺失依赖&#34;&gt;缺失依赖&lt;/h3&gt;

&lt;p&gt;解决办法：在 Maven 打包的时候创建 shaded 或 uber 任务，并将Spark的lib设为provided&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/missing_dependencies_in_jar_files.html&#34;&gt;https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/missing_dependencies_in_jar_files.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;mac-error-running-start-all-sh-connection-refused&#34;&gt;Mac：Error running start-all.sh Connection refused&lt;/h3&gt;

&lt;p&gt;开启 “远程登录” 功能。进入 系统偏好设置 &amp;mdash;&amp;gt; 共享 勾选打开 远程登录。&lt;/p&gt;

&lt;h3 id=&#34;spark组件的网络连接问题&#34;&gt;Spark组件的网络连接问题&lt;/h3&gt;

&lt;h2 id=&#34;性能和优化&#34;&gt;性能和优化&lt;/h2&gt;

&lt;h3 id=&#34;rdd有多少个分区&#34;&gt;RDD有多少个分区？&lt;/h3&gt;

&lt;h4 id=&#34;使用-ui-查看在分区上执行的任务数&#34;&gt;使用 UI 查看在分区上执行的任务数&lt;/h4&gt;

&lt;h4 id=&#34;使用-ui-查看storage&#34;&gt;使用 UI 查看Storage&lt;/h4&gt;

&lt;h4 id=&#34;编程查看-rdd-分区&#34;&gt;编程查看 RDD 分区&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val someRDD = sc.parallelize(1 to 100, 30)
someRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; someRDD.partitions.size
res0: Int = 30
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据本地性&#34;&gt;数据本地性&lt;/h3&gt;

&lt;p&gt;任务应该在离数据尽可能近的地方执行(例如最少的数据传输)。&lt;/p&gt;

&lt;h4 id=&#34;检查本地性&#34;&gt;检查本地性&lt;/h4&gt;

&lt;p&gt;注意UI中的&lt;code&gt;Locality Level&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;调整本地性&#34;&gt;调整本地性&lt;/h4&gt;

&lt;p&gt;You can adjust how long Spark will wait before it times out on each of the phases of data locality (data local &amp;ndash;&amp;gt; process local &amp;ndash;&amp;gt; node local &amp;ndash;&amp;gt; rack local &amp;ndash;&amp;gt; Any)&lt;/p&gt;

&lt;h3 id=&#34;spark-streaming&#34;&gt;Spark Streaming&lt;/h3&gt;

&lt;h4 id=&#34;error-oneforonestrategy&#34;&gt;ERROR OneForOneStrategy&lt;/h4&gt;

&lt;p&gt;If you enable checkpointing in Spark Streaming, then objects used in a function called in forEachRDD should be Serializable. Otherwise, there will be an &amp;ldquo;ERROR OneForOneStrategy: &amp;hellip; java.io.NotSerializableException:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>白山新乱根源在南宫家解在天地峰</title>
      <link>https://lawulu.github.io/post/%E7%99%BD%E5%B1%B1%E6%96%B0%E4%B9%B1%E6%A0%B9%E6%BA%90%E5%9C%A8%E5%8D%97%E5%AE%AB%E5%AE%B6%E8%A7%A3%E5%9C%A8%E5%A4%A9%E5%9C%B0%E5%B3%B0/</link>
      <pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E7%99%BD%E5%B1%B1%E6%96%B0%E4%B9%B1%E6%A0%B9%E6%BA%90%E5%9C%A8%E5%8D%97%E5%AE%AB%E5%AE%B6%E8%A7%A3%E5%9C%A8%E5%A4%A9%E5%9C%B0%E5%B3%B0/</guid>
      <description>

&lt;p&gt;&lt;code&gt;《修真门派掌门路》剧情的一些看法&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;白山新乱没有幕后黑手是不可能把这么多家串连起来的，齐休绕了一大圈，没找到幕后黑手，但其实幕后黑手已经呼之欲出了，就是刚和三楚和好的南宫家。只有南宫家才有这么大的手笔，只有南宫家才能让灵木离火这么疯狂。
南宫家图的什么？
自然一切都是围绕着南宫木的渡劫。&lt;/p&gt;

&lt;p&gt;南宫已经拿到了渡劫法器，但是南宫木必须死，齐滑头才有腾挪的空间，就像当年罗家魏家器符城的金丹们挂的只剩下祁无霜之后，齐休才拿到罗家旧地。&lt;/p&gt;

&lt;p&gt;南宫木显然感受到了作者的恶意，不，大周书院两派的恶意。甚至有可能被极东城城主阴一把。
为了渡劫成功，南宫木还在寻找外力。
有两个选择，一个是红裳不灭元婴，另一个是齐休的命演术。
这两个有损人道，在此界都不好下手，只有将二人收入南宫家族，才能随意下手。
对红裳出手的是南宫止
对齐休是威逼利诱，南宫梦出面。
红裳有三代的保护，如果躲在南楚地宫，基本很难下手。齐休明面上的楚秦盟主，也不会入赘南宫家。
所以利用白山化神转世，幻剑并盟，合欢宗独大，五行去一的白山乱势，造了这么个局。&lt;/p&gt;

&lt;p&gt;眼下过年，貌似势均力敌，但是南宫家肯定有后手，搞不好丹盟或者合欢宗也会出事。&lt;/p&gt;

&lt;p&gt;最后解局的应该是天地峰派姜家解围。
大胆脑洞一下，白山的境况应该是天地峰行自己大道意志的一个试验田。南宫家插手白山是决不允许的。
而且，天地峰跟三楚有旧，出面也情有可原。&lt;/p&gt;

&lt;p&gt;天地峰之恩，正如当年双楚思过山帮楚秦硬悍灵木。
可怜齐休，又要被天地峰逼着做脏事了，估计不在盗婴之下。&lt;/p&gt;

&lt;p&gt;天地峰走的也是命运大道，齐休对之有用，不然不会两次送齐休淬体决
甚至神通邀请齐休去齐云结硬估计也是楚震安排的。&lt;/p&gt;

&lt;p&gt;想摆脱天地峰，就看贾长庚和老狮子了&lt;/p&gt;

&lt;h1 id=&#34;脑洞一下掌门路中的白山&#34;&gt;脑洞一下掌门路中的白山&lt;/h1&gt;

&lt;p&gt;先说白山结婴，说起来为了结婴，一上白山就失去自由是个非常苛刻的条件。虽然可以油灯降临可以照顾下门派，但连对门派鞠躬尽瘁的老齐就一直嘀咕这个条件，其他人可想而知了。&lt;/p&gt;

&lt;p&gt;而且白山这么多年其他家宗门竟然都乖乖选择上白山，不去找其他地方的路子，这就太奇怪了。想象一下，假如柴冠偷偷去外海结婴，灵木早把丹盟给灭了。&lt;/p&gt;

&lt;p&gt;白山究竟能许下多大的好处亦或是威逼利诱？&lt;/p&gt;

&lt;p&gt;威逼利诱，三代才第一代的燕南行完全不应该害怕，而且白山显然对山下宗门的控制力没有太高。&lt;/p&gt;

&lt;p&gt;那白山能给各位身家丰厚一门之主的金丹后期们什么？
我猜是结婴失败之后的转世。
修士是最惜命的一批人，修真本来目的，就是为了延寿，长生。
为了长生或者说灵魂不灭我识不死，也可以去做鬼修，或者傀儡之术，但都为此界不容。大概白山转生是唯一被大周书院允许的办法了。&lt;/p&gt;

&lt;p&gt;这么看来，天地峰要在白山盗婴其实是盗的结婴失败的金丹转世。
寻找转世之人是非常难的（南林找贾长庚就没找到），并且未必有上一世的记忆。
本质上，转生还是一种夺舍，只不过是在母体内夺舍。
天地峰盗婴的目的是完成了白山的承诺，将上一世的记忆激活，然后通过黑手之类的组织控制这些人为其卖命。&lt;/p&gt;

&lt;p&gt;具体办事的楚夺估计并不知道其中内情
龙凤胎应该不是转世，所以被楚神通给留下了
现在楚家因为楚震已死的原因没资格做了，办这事的估计是姜家了。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>