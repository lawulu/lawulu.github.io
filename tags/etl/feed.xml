<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>璐濒殉漂流记</title>
    <link>https://lawulu.github.io/tags/etl/feed/index.xml</link>
    <description>Recent content on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/tags/etl/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>使用SparkSQL替代Hive做ETL</title>
      <link>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</guid>
      <description>

&lt;h2 id=&#34;spark的优势&#34;&gt;Spark的优势&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;性能，比基于Tez的Hive要快&lt;/li&gt;
&lt;li&gt;编程友好：支持更多数据源格式的读和写；由于Spark可以做流处理和即席查询，某种程度上可以复用很多逻辑。&lt;/li&gt;
&lt;li&gt;可以接入Hive-serde表，与无缝与Hive切换&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;可以忽略错误的输入或者格式&#34;&gt;可以忽略错误的输入或者格式&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;spark.sql.files.ignoreCorruptFiles = true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;处理文本(json/csv)可以有三种模式：&lt;code&gt;PERMISSIVE DROPMALFORMED  FAILFAST&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PERMISSIVE: 将数据放到某一列里面&lt;/p&gt;

&lt;p&gt;DROPMALFORMED：丢弃错误记录&lt;/p&gt;

&lt;p&gt;FAILFAST: 抛出异常&lt;/p&gt;

&lt;h3 id=&#34;复杂结构的支持&#34;&gt;复杂结构的支持&lt;/h3&gt;

&lt;p&gt;SELECT EXISTS(values, e -&amp;gt; e &amp;gt; 30) AS v FROM tbl_nested&lt;/p&gt;

&lt;h3 id=&#34;函数&#34;&gt;函数&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;标准函数，UDF，不过UDF对于Spark引擎来说，是一个黑盒，建议不要用&lt;/li&gt;
&lt;li&gt;agg函数&lt;/li&gt;
&lt;li&gt;窗口函数，和hive类似&lt;/li&gt;
&lt;li&gt;rank&lt;/li&gt;
&lt;li&gt;lag&lt;/li&gt;
&lt;li&gt;lead&lt;/li&gt;
&lt;li&gt;其他&lt;/li&gt;
&lt;li&gt;broadcast 利用了broadcast原语做join
```
##貌似不用的话，会优化成用broadcast?
val left = Seq((0, &amp;ldquo;aa&amp;rdquo;), (0, &amp;ldquo;bb&amp;rdquo;)).toDF(&amp;ldquo;id&amp;rdquo;, &amp;ldquo;token&amp;rdquo;).as[(Int, String)]
val right = Seq((&amp;ldquo;aa&amp;rdquo;, 0.99), (&amp;ldquo;bb&amp;rdquo;, 0.57)).toDF(&amp;ldquo;token&amp;rdquo;, &amp;ldquo;prob&amp;rdquo;).as[(String, Double)]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;left.join(broadcast(right), &amp;ldquo;token&amp;rdquo;).explain(extended = true)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### 例子
注意，如果是二维数组来说，从外往内是先列后行。和Java类似`        int[][][] a=new int[2][2][4];  
`对应的是` [[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0]]]  
`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;val df = Seq(1 -&amp;gt; 2).toDF(&amp;ldquo;i&amp;rdquo;, &amp;ldquo;j&amp;rdquo;)
   val query = df.groupBy(&amp;lsquo;i)
     .agg(max(&amp;lsquo;j).as(&amp;ldquo;aggOrdering&amp;rdquo;))
     .orderBy(sum(&amp;lsquo;j))
     .as[(Int, Int)]
   query.collect contains (1, 2) // true&lt;/p&gt;

&lt;p&gt;val df = Seq((1, 1), (-1, 1)).toDF(&amp;ldquo;key&amp;rdquo;, &amp;ldquo;value&amp;rdquo;)
   df.createOrReplaceTempView(&amp;ldquo;src&amp;rdquo;)
   scala&amp;gt; sql(&amp;ldquo;SELECT IF(a &amp;gt; 0, a, 0) FROM (SELECT key a FROM src) temp&amp;rdquo;).show
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |(IF((a &amp;gt; 0), a, 0))|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |                  1|
   |                  0|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&#34;迁移中遇到的问题&#34;&gt;迁移中遇到的问题&lt;/h2&gt;

&lt;h3 id=&#34;兼容问题&#34;&gt;兼容问题&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Hive UDF用到了SimpleDateFormat，Spark中会有问题&lt;/li&gt;
&lt;li&gt;Spark 2.1 不支持MapJoin Hint&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>