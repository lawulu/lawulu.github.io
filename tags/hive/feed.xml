<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>璐濒殉漂流记</title>
    <link>https://lawulu.github.io/tags/hive/feed/index.xml</link>
    <description>Recent content on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="https://lawulu.github.io/tags/hive/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>使用SparkSQL替代Hive做ETL</title>
      <link>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</guid>
      <description>

&lt;h2 id=&#34;spark的优势&#34;&gt;Spark的优势&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;性能，比基于Tez的Hive要快&lt;/li&gt;
&lt;li&gt;编程友好：支持更多数据源格式的读和写；由于Spark可以做流处理和即席查询，某种程度上可以复用很多逻辑。&lt;/li&gt;
&lt;li&gt;可以接入Hive-serde表，与无缝与Hive切换&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;可以忽略错误的输入或者格式&#34;&gt;可以忽略错误的输入或者格式&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;spark.sql.files.ignoreCorruptFiles = true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;处理文本(json/csv)可以有三种模式：&lt;code&gt;PERMISSIVE DROPMALFORMED  FAILFAST&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PERMISSIVE: 将数据放到某一列里面&lt;/p&gt;

&lt;p&gt;DROPMALFORMED：丢弃错误记录&lt;/p&gt;

&lt;p&gt;FAILFAST: 抛出异常&lt;/p&gt;

&lt;h3 id=&#34;复杂结构的支持&#34;&gt;复杂结构的支持&lt;/h3&gt;

&lt;p&gt;SELECT EXISTS(values, e -&amp;gt; e &amp;gt; 30) AS v FROM tbl_nested&lt;/p&gt;

&lt;h3 id=&#34;函数&#34;&gt;函数&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;标准函数，UDF，不过UDF对于Spark引擎来说，是一个黑盒，建议不要用&lt;/li&gt;
&lt;li&gt;agg函数&lt;/li&gt;
&lt;li&gt;窗口函数，和hive类似&lt;/li&gt;
&lt;li&gt;rank&lt;/li&gt;
&lt;li&gt;lag&lt;/li&gt;
&lt;li&gt;lead&lt;/li&gt;
&lt;li&gt;其他&lt;/li&gt;
&lt;li&gt;broadcast 利用了broadcast原语做join
```
##貌似不用的话，会优化成用broadcast?
val left = Seq((0, &amp;ldquo;aa&amp;rdquo;), (0, &amp;ldquo;bb&amp;rdquo;)).toDF(&amp;ldquo;id&amp;rdquo;, &amp;ldquo;token&amp;rdquo;).as[(Int, String)]
val right = Seq((&amp;ldquo;aa&amp;rdquo;, 0.99), (&amp;ldquo;bb&amp;rdquo;, 0.57)).toDF(&amp;ldquo;token&amp;rdquo;, &amp;ldquo;prob&amp;rdquo;).as[(String, Double)]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;left.join(broadcast(right), &amp;ldquo;token&amp;rdquo;).explain(extended = true)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### 例子
注意，如果是二维数组来说，从外往内是先列后行。和Java类似`        int[][][] a=new int[2][2][4];  
`对应的是` [[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0]]]  
`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;val df = Seq(1 -&amp;gt; 2).toDF(&amp;ldquo;i&amp;rdquo;, &amp;ldquo;j&amp;rdquo;)
   val query = df.groupBy(&amp;lsquo;i)
     .agg(max(&amp;lsquo;j).as(&amp;ldquo;aggOrdering&amp;rdquo;))
     .orderBy(sum(&amp;lsquo;j))
     .as[(Int, Int)]
   query.collect contains (1, 2) // true&lt;/p&gt;

&lt;p&gt;val df = Seq((1, 1), (-1, 1)).toDF(&amp;ldquo;key&amp;rdquo;, &amp;ldquo;value&amp;rdquo;)
   df.createOrReplaceTempView(&amp;ldquo;src&amp;rdquo;)
   scala&amp;gt; sql(&amp;ldquo;SELECT IF(a &amp;gt; 0, a, 0) FROM (SELECT key a FROM src) temp&amp;rdquo;).show
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |(IF((a &amp;gt; 0), a, 0))|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |                  1|
   |                  0|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&#34;迁移中遇到的问题&#34;&gt;迁移中遇到的问题&lt;/h2&gt;

&lt;h3 id=&#34;兼容问题&#34;&gt;兼容问题&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Hive UDF用到了SimpleDateFormat，Spark中会有问题&lt;/li&gt;
&lt;li&gt;Spark 2.1 不支持MapJoin Hint&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>招人很难</title>
      <link>https://lawulu.github.io/post/%E6%8B%9B%E4%BA%BA%E5%BE%88%E9%9A%BE/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E6%8B%9B%E4%BA%BA%E5%BE%88%E9%9A%BE/</guid>
      <description>

&lt;h2 id=&#34;其实要求很简单&#34;&gt;其实要求很简单：&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;会Hive,能用Hive完成基本需求。真正的接触过一个线上项目。&lt;/li&gt;
&lt;li&gt;有责任心,靠谱,因为要独当一面,能保证数据的完整性和准确性。&lt;/li&gt;
&lt;li&gt;能以数据的思维来提供业务。&lt;/li&gt;
&lt;li&gt;加分项:日志收集/Spark/机器学习。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;招人很难&#34;&gt;招人很难：&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;跟着培训班或者看视频学了一些皮毛，或者在数据部门打杂，问起各种原理工具，也答得可以，但是一让解决问题，连思路都没有。&lt;/li&gt;
&lt;li&gt;真正做过的要价很高，所以我觉得还不如从Java培养，毕竟我们这有相当一部分工作是用Java做Batch&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;我总结的一些发问点&#34;&gt;我总结的一些发问点：&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hadoop常用的组件，相关软件，namenode原理，为什么要有Yarn？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hadoop相关组件，Zookeeper，Sqoop，Hbase等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用过哪些dfs命令？Hadoop常用端口？文件block？如何计算集群所需的硬盘大小？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hive Sql原理  &lt;a href=&#34;http://tech.meituan.com/hive-sql-to-mapreduce.html&#34;&gt;http://tech.meituan.com/hive-sql-to-mapreduce.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hive UDF UDAF区别，Hive 行转列怎么实现？ concat_ws collection&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;HiveClient和Beeline区别&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据倾斜问题&lt;a href=&#34;http://www.cnblogs.com/ggjucheng/archive/2013/01/03/2842860.html&#34;&gt;http://www.cnblogs.com/ggjucheng/archive/2013/01/03/2842860.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hive Metadata作用，Hive 分区表，新增一个字段，老的日志不认，显示为空，如何处理？&lt;a href=&#34;http://blog.csdn.net/lxpbs8851/article/details/17118841&#34;&gt;http://blog.csdn.net/lxpbs8851/article/details/17118841&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spark的优势？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;流计算Storm Trident API，如何保证有且只有一次
&lt;a href=&#34;http://www.flyne.org/article/222&#34;&gt;http://www.flyne.org/article/222&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;任务调度：
重试，幂等性
如何保证数据的完整性和准确性？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;点击去重
例如：logtime,userId,adId,actionType, 1小时内的点击只能算一次。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hive表存储的方式？哪些常用的文件存储格式？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;simpledateformat的多线程问题 MR中会有问题吗？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hive提高Map的数量，Hive小文件CombinedInputFormat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可视化，Ad hoc&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据仓库相关，如何分层，建模？拉链表&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hive引擎往Solr写数据</title>
      <link>https://lawulu.github.io/post/Hive%E5%BC%95%E6%93%8E%E5%BE%80Solr%E5%86%99%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Hive%E5%BC%95%E6%93%8E%E5%BE%80Solr%E5%86%99%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;p&gt;公司有一批结果是通过Hive跑SQL跑出来的，但是需要最终写到Solr里面，方面别的服务使用。
灵感来自
&lt;a href=&#34;https://chimpler.wordpress.com/2013/03/20/playing-with-apache-hive-and-solr/&#34;&gt;https://chimpler.wordpress.com/2013/03/20/playing-with-apache-hive-and-solr/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Fork之后，做了一些修改：主要是针对Hive0.12和Solr新版本做的一些升级。&lt;a href=&#34;https://github.com/lawulu/hive-solr/&#34;&gt;https://github.com/lawulu/hive-solr/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>