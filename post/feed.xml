<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on 璐濒殉漂流记</title>
    <link>https://lawulu.github.io/post/feed/index.xml</link>
    <description>Recent content in Post-rsses on 璐濒殉漂流记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 15 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lawulu.github.io/post/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>小陈与大数据的故事</title>
      <link>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B0%8F%E9%99%88%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%85%E4%BA%8B/</guid>
      <description>

&lt;p&gt;&lt;em&gt;本篇为公众号&lt;code&gt;码农翻身&lt;/code&gt;的约稿，最终想写4篇，所谓陈余罗严&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;在学校&#34;&gt;在学校&lt;/h2&gt;

&lt;p&gt;做为一个科班出身的程序员，在小陈还不明白自己所学的的数据结构和操作系统这些专业课跟未来的工作有什么关系，甚至未来做什么也不清楚时候，小陈最喜欢做的事就是看电影，看完还舍不得删。那个时代流行的还是“为人不识武藤兰，阅遍电影也枉然”，电影还有很多是rm格式的，几百兆都算大的，但是架不住小陈对一衣带水邻国文化的热爱，只有120G的硬盘让他很快有了另一个爱好：整理硬盘，每次有新资源出来，小陈都得想办法腾点空间出来。&lt;/p&gt;

&lt;p&gt;后来教育网里一个叫Maze的软件在我校流行开来，这个软件可以在局域网内以P2P的方式完成资源共享。喜大普奔，小陈得意洋洋的宣布自己终于不用整理硬盘了，自己硬盘里面只存一部分经典的，想看什么电影从别人机器里面拖，哪怕自己临时要看一个美帝大片，删掉一部分电影，删掉之后都还能从别人机器里面拖回来。&lt;/p&gt;

&lt;p&gt;直到有一天，小陈临时删了一位老师的代表作，等想找回来时候，不知道是学校调整了网络路由还是因为高年级学生的离校，这部本来在Maze中非常常见的电影居然一个都找不到了。小陈被室友嘲笑之余，怒而卸载了Maze，并决定要开发一套更好的共享资源软件：要有一个元数据服务器，所有使用者的硬盘的读写都由元数据服务器来管理，元数据保证每份电影都至少存在2份以上，而不是像现在这样保存什么电影，全靠用户心照不宣的默契和自己的兴趣。&lt;/p&gt;

&lt;p&gt;不过小陈也面临着毕业，自然没有时间去搞这些乱七八糟的东西，为了找工作，开始恶补各种技能。这已经是2011年，大数据已经开始火起来了，小陈开始试着学习大数据的知识。搞Android培训的那波人摇身一变开始培训Hadoop了，小陈上了一次试听课之后，回来开始向室友生成，自己的创意被Google和Hadoop给剽窃了！大数据首先要解决的都是存储问问题，GFS，以及受之启发的Hadoop HDFS，完全就是照着自己的资源共享软件思路来的。&lt;/p&gt;

&lt;p&gt;其实小陈知道，Google在2003年就把GFS的论文给写出来，开发这个GFS时候武老师可能还没有出道，甚至BigData这个词还没有被清楚的定义。但是Google作为技术最领先，体量最大的搜索引擎公司，很早就面临和自己类似的问题:大量的数据(信息索引和网页快照等等)要怎么存储存储在哪里的问题。Google选择的是用大量的通用硬件组成一个可以横向扩展的集群去对存储和处理这些海量的数据。这种英雄所见略同的感觉，让小陈决定要从事大数据开发，毕竟这个越来越自我的世界，随着移动互联网的爆发，每个人制造的垃圾数据会越来越容易，而制造垃圾数据的人也会越来越多。&lt;/p&gt;

&lt;h2 id=&#34;第一份工作&#34;&gt;第一份工作&lt;/h2&gt;

&lt;p&gt;找大数据的工作并不顺利，心仪的公司嫌自己没有经验，给Offer的公司貌似没有多少数据，只是想趁着大数据热忽悠一把。阴错阳差，小陈找到一份Java开发的工作。这家公司专做某行业的软件，公司不大但发展很快，待遇还算不错。小陈第一周上班主要学习公司的技术框架，然后就懵逼了。面试时候非要分清POBOVO，现在项目所有的参数传递都是List&lt;Map&gt;；面试时候各种设计模式的问，结果这项目的大部分的业务逻辑都是写在SQL存储过程里面的，甚至JS里面也夹杂了很多SQL；自研的一个工作流框架对应的库表，各种预留字段，从field1，field2一直到field8。在小陈犹豫着要不要换家公司，技术总监的几句话给忽悠住了他：这个世界的所有事物，即所谓的对象，都是可以用List&lt;Map&gt;来表示；SQL是史上的最好的DSL，一定要充分利用，如果想写，用SQL实现一个Dijkstra算法都可以[注1]；这框架虽然不走寻常路，但是有PHP的开发速度，也利用JVM的性能和Java的生态系统。&lt;/p&gt;

&lt;p&gt;小陈不仅觉得总监的话很有道理，而且这些话跟大数据的思路也有几份相似。Google开启大数据时代最重要的就是三篇论文:Google File System,讲如何分布式存储大数据；Google MapReduce,讲如何分布式计算大数据；Google Bigtable,实现了一个数据库，以便在分布式下快速读写结构化数据。对应的开源实现是Hadoop Hdfs，Hadoop MapReduce，HBase。这个识万物为List&lt;Map&gt;的思路不就是MapReduce的思路吗，而那个预留字段的数据库设计其实有点Bigtable/HBase的风范，不管神似与否至少形似。工作一段时间，小陈感觉这糙快猛的框架确实还是很适合这个需求变化比较快的行业，对他自己来说，至少写SQL和实现业务需求的能力提高了不少。一年之后，小陈跳槽到一家三线互联网公司做大数据，凭借的就是这两项能力。&lt;/p&gt;

&lt;h2 id=&#34;大数据工作和两个领导&#34;&gt;大数据工作和两个领导&lt;/h2&gt;

&lt;p&gt;新公司工作主要基于Shell调用Hive，把数据从日志往Oracle里面导，经常还要给业务部门提数。所以，虽然在做大数据，工作内容只是从上家公司的Oracle PL/SQL换成了Hive SQL。Map-Reduce虽然直指大数据计算的本源，但是奈何这个世界太复杂，产品经理的需求太多，表达力有点欠缺。而且又很多通用的需求，例如数据之间的Join和Group，完全可以提炼出来，于是Facebook向开源社区捐献了Hive，用史上的最好的DSL作为查询语言交给熟悉SQL的人去使用，Hive引擎负责把SQL翻译成MapReduce。小陈得偿所愿进入大数据行业，但是很快就厌倦了：Shell管理起来太麻烦，跟其他系统交互起来不方便；Hive运行太慢，资源利用率低，有时候跑了十几分钟结果出来了发现自己的SQL有一个简单的错误；想推动HadoopV2的落地，让Yarn去合理的管理集群资源，领导和同事都不愿意承担风险。小陈隐约觉得自己做的东西并没有给公司带来应有的价值，只是让报表的维度增加一些。领导告诉小陈，大数据并不是解决一切问题的灵丹妙药，数据越来越多，增加更多的是无价值的数据，而大数据处理的日志等数据与业务数据相比，信息熵要低的多。&lt;/p&gt;

&lt;p&gt;就这么工作一年后，老板也对数据部门不满意，赶走了老领导，挖来一个大厂的架构师来主管整合所有数据相关业务。新领导上来就大刀阔斧，部门架构调整，把一部分数据相关的开发并了过来，整个部门一分为二，一部分往上跟业务和产品部门打交道，一部分下沉做大数据平台时候。小陈果断的选择了去做平台。&lt;/p&gt;

&lt;p&gt;如果说老领导强调的是数据完整性和正确性，数据对业务的支撑，新领导更强调数据对产品和决策的反馈，技术的先进性。按照新领导的意思，以做用户数据仓库为出发点，从数据收集、数据转换、数据模型、数据查询、数据可视化到任务调度和平台监控，要重新梳理现有每一个模块的技术架构。这个时候，大数据相关技术已经可以说是百花齐放百家争鸣，围绕着Hadoop，几乎每个领域都有一两个出色的解决方案。领导要求使用技术要向代表技术先进性，相关软件版本落后不能超过2年，鼓励使用3年以内的新项目。小陈很高兴，感觉可以大干一场了。因为态度积极，小陈很受新领导的器重，小陈先负责用其他SQL on Hadoop的如Impala/Presto来代替Hive的部分功能，提高查询效率，后来又负责引入实时计算框架Storm，虽然只是用来计算PV/UV，但好歹是实时的。总之是趟坑无数，背锅无数，忙得不亦乐乎。&lt;/p&gt;

&lt;p&gt;直到又过一年公司裁员，小陈才发现数据部门是重灾区。和领导吃散伙饭，小陈开始鸣不平，觉得咱们的平台做了这么多，大家加班这么辛苦，为什么就没人懂欣赏呢。领导却直言自己早就知道这个结局，部门折腾了一年，老板只看到数据部门人员和机器一直在增加，却经常收到其他部门各种抱怨，产品抱怨进度太慢啊，商务说数据不懂业务啊，研发说总是要配合数据做这做那啊，运营说报表没有老系统方便数据老出问题等等。相处这么久，领导已经把小陈当心腹了，直言一开始想把有些业务拿到数据这边统一处理，技术出身搞不定公司复杂的关系，阻力太大；埋头把平台理顺，而在强势老板的公司里面，自己还是没有抓住老板的需求和业务的痛点，所以对老板来说，确实没为公司带来什么价值。领导的这番肺腑之言对小陈触动很大，开始重新审视自己的过去的工作。不管怎样小陈这时候找下一份工作已经有了一份资本。&lt;/p&gt;

&lt;h2 id=&#34;在创业公司&#34;&gt;在创业公司&lt;/h2&gt;

&lt;p&gt;在全民创业的浪潮中，小陈来到一家创业公司，这里还没有什么&amp;rdquo;大数据&amp;rdquo;,但是CEO的数据要介入整个产品和运营中的承诺和希望打动了小陈，而且这里有机会从零开始有机会按照自己的思路规划整个公司的大数据业务。这个时候，大数据圈最火的是Spark，Spark以基于内存的DAG计算引擎为基础，提供了批量计算，实时计算，交互式查询，机器学习，基本上实现了One Stack To Rule Them All，而且Spark也有SparkSQL。小陈选择了围绕Spark来构建整个系统，并且拥抱云计算了，直接选择云服务，希望自己能专注于让数据产生价值。小陈工作分这么几部分:输出报表数据;给一个数据科学家打杂，帮他整理数据提取特征，实现一些算法;每天花一个小时看数据，解释数据和用数据去解释。小陈成了对公司业务和系统最了解的人，比产品和QA都要了解，并且提出了很多优化和改进。CEO很满意，年终时候大笔一挥给了更多的期权，然而没什么卵用，因为融资寒冬来的时候，公司还是倒了。&lt;/p&gt;

&lt;h2 id=&#34;在大厂&#34;&gt;在大厂&lt;/h2&gt;

&lt;p&gt;小陈已经拖家带口了，感觉自己不再年轻，在机构Offer之间犹豫了很久之后，选择了去一个大厂做螺丝钉，这里流程和工具都已经非常完善了，用的Hadoop都是自己定制甚至已经跟社区不兼容了。小陈业务时间乐于参加各种线下各种活动，积攒自己的人脉，坚持更新Blog，提升自己的名气。&lt;/p&gt;

&lt;p&gt;这个时候Google推出了Beam计算框架。其实Google抛出了三大论文之后，表面上很沉寂，自己内部的系统却一直在向前演进，例如老版的GFS已经被Colossus被代替。直到Google眼红AWS投身云计算时候，发现客户都更习惯开源的那套东西，甚至Google把自己的BigTable服务往外卖的时候，不得不兼容BigTable的山寨产品HBase的API。这就好比一个欧洲人找到一个中国建筑师建一个亚洲古代建筑，中国建筑师给建了一个唐朝风格的建筑，欧洲人说，你这建的不大对，我在日本看到的都是这样那样。Google为了争夺话语权，把自己最新的产品拿出来开源，要一统所有的计算框架，不管你底层是Storm还是Spark，按照Beam的API来处理数据，底层可以随意切换Spark和自家的Cloud Dataflow，甚至为了师出有因，大力提携Flink和Spark打擂台。小陈相信这个框架会一统江湖，为了自己的KPI和升级, 准备在公司推Apache Beam。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>程序员必备七种武器</title>
      <link>https://lawulu.github.io/post/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E4%B8%83%E7%A7%8D%E6%AD%A6%E5%99%A8/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E4%B8%83%E7%A7%8D%E6%AD%A6%E5%99%A8/</guid>
      <description>

&lt;p&gt;&lt;em&gt;本篇为公众号&lt;code&gt;码农翻身&lt;/code&gt;的约稿&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;科学上网&#34;&gt;科学上网&lt;/h2&gt;

&lt;p&gt;从服务器芯片到底层协议，从W3C技术标准到github各种开源项目，IT技术绝大部分都起源于墙外，而Google是检索这些第一手技术的最好的手段，没有之一。所以对码农来说，可以自由的浏览是非常有必要的。&lt;/p&gt;

&lt;p&gt;有很多种科学上网方式，笔者推荐自己买海外主机搭建,可以和亲近朋友一起使用一台。除了常用的SS之外，SSH的socks转发是最简单的方式了。&lt;/p&gt;

&lt;p&gt;例如笔者常用的一个shell：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
nohup ssh  -ND 3128 user@jumpserver &amp;amp;
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome  Chrome --proxy-server=&amp;quot;socks5://localhost:3128&amp;quot; --host-resolver-rules=&amp;quot;MAP * 0.0.0.0 , EXCLUDE localhost&amp;quot; --user-data-dir=/tmp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个shell在Mac下启动一个Chrome进程，该进程完全以jumpserver的网络去访问互联网，这样在这个Chrome内访问公司云主机内网的各种服务非常方便。&lt;/p&gt;

&lt;h2 id=&#34;剪贴板&#34;&gt;剪贴板&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.todamax.net/wp-content/uploads/2016/03/copy-paste-from-stack-overflow.jpeg&#34;&gt;http://blog.todamax.net/wp-content/uploads/2016/03/copy-paste-from-stack-overflow.jpeg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;相信大家最常用的快捷键就是复制和粘贴了。
假如你刚刚从Wiki的设计文档把一个API的URL复制下来准备写代码；发现经常聊天的一个技术群弹出一个消息，有个妹子问一个技术问题，凭借自己对技术的感觉很快在Stackoverflow搜到了答案，
赶紧把答案复制过去；突然又收到产品群发的一个邮件需要你去撕逼，写了几句之后，觉得某个措辞应该放到后面，刚剪切了第一句；经理在IM上让你把某个需求的Jira链接发给他..你如果经常遇到这种情况，首先考虑的应该是把所有的打扰都屏蔽掉，如果做不到，那就安装一个剪贴板工具吧。&lt;/p&gt;

&lt;p&gt;Windows下推荐ditto，Mac下推荐Alfred自带的。顺便提一下，类似Mac下的Alfred这种工具（Windows下的有TotalCommander），如果能研究透彻，会极大的提高工作效率。&lt;/p&gt;

&lt;h2 id=&#34;抓包&#34;&gt;抓包&lt;/h2&gt;

&lt;p&gt;现在很多产品和需求都需要通过网络与用户或其他服务打交道。抓包工具可以把自己交付物当做一个黑盒，去发现和定位问题。Windows下的Fiddler，Mac下推荐Charles都非常强大。这两者通过安装证书，甚至可以直接抓取Https的网络。&lt;/p&gt;

&lt;p&gt;基于Web的应用，也可以直接考虑浏览器的开发者工具。例如，Chrome的开发者工具就非常强大：可以直接找到元素的XPath，在Console运行JS代码，通过修改元素的文本来P图。&lt;/p&gt;

&lt;p&gt;如果想做更深层次的抓包，可以研究下Wireshark。&lt;/p&gt;

&lt;h2 id=&#34;文本编辑器&#34;&gt;文本编辑器&lt;/h2&gt;

&lt;p&gt;程序员经常要跟各种日志和数据打交道，一个熟悉的文本编辑器可以事半功倍。&lt;/p&gt;

&lt;p&gt;笔者比较常用的功能有列模式，例如把一些数据直接编辑成SQL；正则替换，例如从不太标准的日志中提取自己所需的内容。&lt;/p&gt;

&lt;p&gt;文本编辑器免费的有Sublime Text/Atom,商业软件有UltraEdit等。另外还有一种基于终端的文本编辑器，例如Nano/Vim/Emacs。如果从事Server开发，掌握这些编辑器的查找、修改、行号等功能是非常有必要的。&lt;/p&gt;

&lt;h2 id=&#34;笔记&#34;&gt;笔记&lt;/h2&gt;

&lt;p&gt;在这个信息爆炸的时代，搜索引擎有时候也捉襟见肘。子曰：&amp;rdquo;吾日三省吾身&amp;rdquo;，做笔记是一个非常好的反省自己的手段，对每天的收获进行记录和整理，也方便后续快速解决类似问题。市面上的笔记产品大同小异，笔者一直使用有道云笔记：支持MarkDown语法，免费，跨平台。
另外，思维导图也是一个非常好的整理自己知识的工具。&lt;/p&gt;

&lt;h2 id=&#34;脚本语言&#34;&gt;脚本语言&lt;/h2&gt;

&lt;p&gt;假如突然接到一个这样的临时需求：需要给客户端提供一个模拟服务器，根据请求内容的不同，返回相应的Json。这个需求用Python SimpleHTTPServer十几行代码就能实现。很多类似的临时需求，还有需要快速验证的想法，以及粘合多个系统，脚本语言是最合适的。在自己的主力开发语言之外熟悉一门脚本语言做很多事都可以事半功倍，也可以体验到另一种编程文化。&lt;/p&gt;

&lt;p&gt;有很多脚本语言（shell,Ruby,Perl等等）可以选择，笔者推荐Python。Python这些年在很多领域都重新受到追捧。Python有2和3两种，推荐直接上手3，虽然很多老版本服务器系统自带的还是2，但是现在基本上所有的主流库都已经支持Python3了。&lt;/p&gt;

&lt;p&gt;如果单纯的当做一个脚本语言，只需要了解如何处理字符串，常用数据结构，如何处理文件和网络IO,基本就满足日常需求了。&lt;/p&gt;

&lt;p&gt;另外推荐一下Jupyter Notebook，这是一个web版的Python编辑、运行和演示环境(也支持R等其他语言)，可以和shell等环境变量交互。很多人在服务器上直接运行notebook,然后在本地web端调试python程序。&lt;/p&gt;

&lt;h2 id=&#34;自己的codebase&#34;&gt;自己的CodeBase&lt;/h2&gt;

&lt;p&gt;将自己Coding经验提炼为一个CodeBase。经常需要的用模块，如Web框架、模板引擎、Http请求、单元测试以及Mock、Cache、调度、Metric、时间处理、安全、日志、XML/Excel解析等等，每一个模块都有三四种可以选择的技术，选择一个自己熟悉的，构建自己的软件开发栈，这样遇到各种需求都能快速基于自己的CodeBase的实现。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imply和Confluent破解</title>
      <link>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Imply%E5%92%8CConfluent%E7%A0%B4%E8%A7%A3/</guid>
      <description>

&lt;p&gt;其实这两个软件还是比较容易破解的。因为：&lt;/p&gt;

&lt;p&gt;首先开发者并没有把精力放到反破解上面，相关功能做的很简单;&lt;/p&gt;

&lt;p&gt;第二没有服务器交互的反破解还是很难的，尤其是对于Java这种容易反编译的软件来说。话说过来，即时是有服务器交互，例如Idea，也有人搞出各种License Server出来。&lt;/p&gt;

&lt;h2 id=&#34;imply-pivot&#34;&gt;Imply Pivot&lt;/h2&gt;

&lt;p&gt;搜索&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -ri &#39;Pivot evaluation license expired&#39; .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现有个文件&lt;code&gt;./node_modules/@implydata/im-auth/build/server/utils/license-manager/license-manager.js&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;里面有这个逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        var e = path.join(this.varDir, &amp;quot;.pivot-first-run&amp;quot;);
        return Q(fs.readFile(e, {encoding: &amp;quot;utf-8&amp;quot;})).then(function (e) {
            var r = new Date(e.trim());
            if (isNaN(r)) throw new Error(&amp;quot;invalid date&amp;quot;);
            return {created: false, firstRun: r}
        }).catch(function (r) {
            var t = new Date;
            return Q(fs.writeFile(e, t.toISOString())).then(function () {
                return {created: true, firstRun: t}
            })
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到文件&lt;code&gt;find . -name &amp;quot;.imply-first-run&amp;quot;&lt;/code&gt;
把里面日期改一下就行了&lt;/p&gt;

&lt;h2 id=&#34;confluent&#34;&gt;Confluent&lt;/h2&gt;

&lt;p&gt;坑一：一开始搞反了..一直没报错 还以为没生效&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jar uf /service/app/confluent-4.0.0/share/java/confluent-control-center/control-center-4.0.0.jar io/confluent/controlcenter/license/LicenseModule.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;坑二：zsh ll不显示隐藏文件，习惯把ll = ls-al 而不是ls -ah&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; return License.baseClaims(&amp;quot;demo&amp;quot;, bfa.creationTime().toMillis() + TimeUnit.DAYS.toMillis(30L), true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以其实修改该文件的createTime就行了..&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>数据需要收集哪些数据</title>
      <link>https://lawulu.github.io/post/%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E6%94%B6%E9%9B%86%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E6%94%B6%E9%9B%86%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE/</guid>
      <description>

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近线上遇到一些问题，分析起来还是要抓瞎，完全不知道怎么去定位。只能从代码去分析&amp;hellip;觉得之前的设计的数据收集太简单了，只是以报表需求为导向设计的。那我们还需要收集哪些数据呢？&lt;/p&gt;

&lt;h2 id=&#34;能解释业务的数据&#34;&gt;能解释业务的数据&lt;/h2&gt;

&lt;h3 id=&#34;所有业务要串起来&#34;&gt;所有业务要串起来&lt;/h3&gt;

&lt;p&gt;业务有三条线：
- Active
- Config
- Preload-&amp;gt;Impression-&amp;gt;Event-&amp;gt;Click-&amp;gt;Install&lt;/p&gt;

&lt;p&gt;改动：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;最核心的广告生命周期是否可以用一个Id呢？如果不能，每一步要讲上一步的id给带上来。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CDN的下载要带上一个preloadId，然后开启参数过滤，通过关联preloadId将下载流程也关联到核心业务上去。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重复Id上报的问题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;server日志&#34;&gt;Server日志&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ELK要用起来，亦可以调研一下阿里的日志服务&lt;/li&gt;
&lt;li&gt;规范Server输出，尤其是Filter广告时候，要将挑选广告失败的原因记录到日志里面&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;可追溯每个客户端的行为的数据&#34;&gt;可追溯每个客户端的行为的数据&lt;/h2&gt;

&lt;p&gt;最近几个痛点：流量端说我们的填充率对低，而我们报表的填充率非常高；点击广告主上报跟服务器上报总是对不上；点击率中有真实点击和误点击区分不开。所以要收集这些数据：
- APP对SDK的接口调用都要记录
- SDK和Server之外的所有交互（点击上报，CDN下载）都要记录
- 以上记录可以通过开关来控制
- EndCard的点击率要能区分点击区域，找到真实的点击率&lt;/p&gt;

&lt;h2 id=&#34;其他数据&#34;&gt;其他数据&lt;/h2&gt;

&lt;p&gt;抓友商的包，发现友商有一个SessionStart和SessionEnd的接口，这个要加上。貌似SessionEnd不准，因为每次进入后台，SDK没有机会发请求到Server。&lt;/p&gt;

&lt;h2 id=&#34;实施&#34;&gt;实施&lt;/h2&gt;

&lt;h3 id=&#34;ab测试准备&#34;&gt;AB测试准备&lt;/h3&gt;

&lt;p&gt;增加一个Header&lt;code&gt;x-clad-tag&lt;/code&gt;,在Config接口下发下去，给用户打标签，并贯彻到整个业务流程&lt;/p&gt;

&lt;h3 id=&#34;上报服务拆分&#34;&gt;上报服务拆分&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Impr,Click等事件上报因为只是记一个数字，要和Preload业务从逻辑和部署上都分开，避免互相干扰&lt;/li&gt;
&lt;li&gt;并且上报地址应该是Preload下发下去，而非写死，并且支持同一事件的多重上报。这样易于控制，甚至以后可以作弊（按照某人的说法：&amp;rdquo;&lt;code&gt;我们不主动作恶，但是在恶劣的环境下，要有保护自己的手段。&lt;/code&gt;&amp;ldquo;）&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Preload接口和Play接口的兴衰史</title>
      <link>https://lawulu.github.io/post/Preload%E5%92%8CPlay%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B4%E8%A1%B0%E5%8F%B2/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Preload%E5%92%8CPlay%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B4%E8%A1%B0%E5%8F%B2/</guid>
      <description>

&lt;p&gt;对业务理解的深入和梳理，极大的方便了重构。先说结论：&lt;strong&gt;Preload的本质是告诉SDK下一次Impression应该展示哪个广告&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;对于Native移动视频广告来说，一方面网络条件所限，另一方面要提升用户和广告主的体验，播放广告基本都是以预加载的形式提前把广告加载完成。这个特性让Ad Serving的流程大大不同。&lt;/p&gt;

&lt;h2 id=&#34;第一版的做法&#34;&gt;第一版的做法&lt;/h2&gt;

&lt;h3 id=&#34;提供了两个接口&#34;&gt;提供了两个接口:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload,用来告诉SDK有哪些广告可以下载，然后告诉SDK开始下载，一般是三到五条。&lt;/li&gt;
&lt;li&gt;Play，在SDK有曝光机会的时候，SDK将已经预加载完成的广告告诉Server，Server从中挑出可播的广告。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;问题&#34;&gt;问题&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload和Play两个接口时间上有间隔，在另一个时间点，各种条件已经变化了，需要重新挑选广告。所以同一次曝光机会，可能需要挑选两次。&lt;/li&gt;
&lt;li&gt;Play接口的响应时间要求很高，客户端的网络又不稳定。&lt;/li&gt;
&lt;li&gt;整个链路太长，又有些条件不可控，所以Ad Serving的逻辑很难优化。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;梳理业务-重新定义&#34;&gt;梳理业务，重新定义&lt;/h2&gt;

&lt;h3 id=&#34;其他部门角度&#34;&gt;其他部门角度&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;对商务来说，尾量可以接受，甚至非常正常。这意味着，各种定向控制并不需要那么严谨。&lt;/li&gt;
&lt;li&gt;对运营来说，CDN的消耗相比转化率的提高并不重要，更迫切需要一个可以快速干扰Ranking的办法。&lt;/li&gt;
&lt;li&gt;对产品来说，需要让事情简单化。我们的Preload的挑选广告就对应传统Ad产品中的那次挑选广告就OK。&lt;/li&gt;
&lt;li&gt;对数据来说，填充率的计算需要和流量方匹配。对应第一版来说，填充率，是用Play接口的成功返回/Play接口的调用。&lt;/li&gt;
&lt;li&gt;对技术来说，Play接口要求太高，优化起来太困难。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;那么&#34;&gt;那么&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Preload接口挑选广告，Play接口只做Impression的上报&lt;/li&gt;
&lt;li&gt;激进的来说，每次Preload只预加载一次，播完就删&lt;/li&gt;
&lt;li&gt;在应用启动和每次播放完成之后调用一次Preload&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Play的逻辑前置到Preload里面，Play沦为一个上报接口&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>元数据管理</title>
      <link>https://lawulu.github.io/post/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;引子&#34;&gt;引子&lt;/h2&gt;

&lt;h3 id=&#34;业务元数据&#34;&gt;业务元数据&lt;/h3&gt;

&lt;p&gt;最近公司新来一个气场强大的产品，发现有些内部（研发和商务之间经过多次讨论之后形成的）常用的一些话术，他已经在改变对应的说法了。例如广告，因为数据库叫Campaign，所以技术这边都叫Campaign，商务本来已经要从Ad改成了Campaign了，现在有多了个说法叫offer；追踪链接的叫法本来要按照商务的来了，现在愈发混乱了。同一个含义对应的不同的表达，在需求到研发落地和数据流转时候，很容易出问题。&lt;/p&gt;

&lt;p&gt;项目刚起时候，制定一个词典/元数据非常重要，并且要随着业务的发展及时更新。业务元数据可以极大的减少沟通成本。&lt;/p&gt;

&lt;h3 id=&#34;一个悲剧&#34;&gt;一个悲剧&lt;/h3&gt;

&lt;p&gt;IMEI在电信手机上，取到的是一个字符串而不是一个数字。Server发现这个问题之后，没有及时的同步到数据，结果被这个问题又坑了一次。&lt;/p&gt;

&lt;h2 id=&#34;元数据是什么&#34;&gt;元数据是什么&lt;/h2&gt;

&lt;p&gt;瓦利哥（神策数据的创始人）在知乎有一个非常好的专栏（&lt;a href=&#34;https://zhuanlan.zhihu.com/sangwf/20622902&#34;&gt;https://zhuanlan.zhihu.com/sangwf/20622902&lt;/a&gt;
）。有一章专门讲百度的元数据服务的演进。
所谓的元数据服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Schema：表字段的定义&lt;/li&gt;
&lt;li&gt;数据就绪状态&lt;/li&gt;
&lt;li&gt;对外服务的API&lt;/li&gt;
&lt;li&gt;数据审计/变更&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;瓦利哥推荐了HCatalog，有空打算尝试一下，大概看了一下，是强制要求遵循某种规范。&lt;/p&gt;

&lt;h2 id=&#34;业界解决方案&#34;&gt;业界解决方案&lt;/h2&gt;

&lt;h3 id=&#34;wherehows&#34;&gt;Wherehows&lt;/h3&gt;

&lt;p&gt;LinkedIn开源了一个数据发现和血缘管理工具，叫做Wherehows，开发团队全华人。解决的是类似的问题，不过并不是强制要求开发团队生产数据时候主动和metadata绑定，而是通过分析Scheduler log和Repo(如Azkaban和Hadoop job)，爬取其他数据（如关系数据库和HDFS)来建立元数据，并分析彼此的血缘关系。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal&#34;&gt;https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;google-goods&#34;&gt;Google Goods&lt;/h3&gt;

&lt;p&gt;Google最近写了一篇相关论文，《Goods: Organizing Google’s Datasets》。论文中的Overview图和wherehows特别相似。看来big campany的big data都有相似的需求。Google公开的（可以被公司所有工程师查看的）datasets有26B，20多种格式，每天有很多数据在被删除..总之很有挑战性，但是依然只是一个论文。
Goods支持用户对数据的标注。&lt;/p&gt;

&lt;h2 id=&#34;实践&#34;&gt;实践&lt;/h2&gt;

&lt;p&gt;上面两个对我们现在的公司太重了。现在公司所有的元数据都是通过Excel和Jira来管理，主要还是口头沟通。两个努力的方向：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;考虑想一个jar的方式把日志metadata（字段含义和顺序）给确定下来，日志的输出、收集和处理都依赖于这个jar，以版本号管理。甚至，Hive表结构变更也要依赖于metadata。嗯，首先要把日志统一由CIF(Api Gateway)来输出。&lt;/li&gt;
&lt;li&gt;对于容易出错的东西，一定要拿出来公开讨论，例如公司SDK上报的Platform字段，IPad和Iphone不一样。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>CDN流量日志简单分析</title>
      <link>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/CDN%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;公司运营的很大一个成本就是CDN耗费。对比Impression和CDN消耗量就会很奇怪，不该有这么多的CDN消耗的。&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;下载并处理cdn原始日志&#34;&gt;下载并处理CDN原始日志&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;由于官方提供的合并下载日志工具在Mac下不能用，随机选取一个小时的日志：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;wget https://cdnlog.cn-hangzhou.oss.aliyun-inc.com/streaming.lawulu.com/2017_03_04/streaming.lawulu.com_2017_03_04_1200_1300.gz?spm=5176.8232200.log.d10.83JI60&amp;amp;OSSAccessKeyId=xxxxx&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Python处理分析为CSV&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;pat = ( r&#39;\[(.+)\]\s&#39; #datetime
           &#39;(\d+.\d+.\d+.\d+)\s&#39; #IP address
           &#39;-\s&#39; #proxy ip
            &#39;\d+\s&#39; #responseTime
           &#39;&amp;quot;.+&amp;quot;\s&#39; #referrer
           &#39;&amp;quot;(GET\s\S+mp4)&amp;quot;\s&#39;  # requested file
            &#39;\d+\s&#39; #status
           &#39;\d+\s&#39; #requestSIze
           &#39;(\d+)\s&#39; #responseSize
           &#39;\D+\s&#39; #HIT OR NOT
           &#39;&amp;quot;(.+)&amp;quot;\s&#39; #user agent
           &#39;&amp;quot;\S+&amp;quot;&#39; #contentType
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@see &lt;a href=&#34;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&#34;&gt;https://github.com/richardasaurus/nginx-access-log-parser/blob/master/main.py&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;导入数据库&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA INFILE &#39;cdnlogs.csv&#39; 
INTO TABLE test.cdn_log
FIELDS TERMINATED BY &#39;,&#39; 
LINES TERMINATED BY &#39;\n&#39; 
(date_time,ip,url,rsp_size,ua);
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;数据校验&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;SELECT count(*)  FROM test.cdn_log 
&#39;206619&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cat streaming.lawulu.com_2017_03_04_1200_1300 | wc -l

206618
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;用sql分析&#34;&gt;用Sql分析&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) =1 )a


&#39;110643&#39;,&#39;598635187514&#39;


select count(*), sum(rsp_size) from (

SELECT ip,ua,url,sum(rsp_size) as rsp_size FROM test.cdn_log group by ip,ua,url having count(*) &amp;gt;1 )a


&#39;32670&#39;, &#39;440010318426&#39;

0.4236
select 440010318426/(598635187514+440010318426) from dual 

select sum(rsp_size) from  cdn_log

1038645505940

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;由于IP,UA,URL可以唯一确定一次UV，我们公司有30%的用户占用了40%的流量，对同一个视频会不停的反复下载。
最后查明，是SDK的bug，正在下载视频时候，如果切换到后台，下载会直接停止。所以会反复下载视频，浪费流量。&lt;/p&gt;

&lt;h2 id=&#34;校验&#34;&gt;校验&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;计算ResponseSize之后和Console里面同一时间段的&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;
CURL  -i -X HEAD &#39;https://streaming.lawulu.com/default/256b9db1-a1ce-4983-8906-4568fa7a9c75-144.mp4&#39;
&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>广协的IP库到底准不住</title>
      <link>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B9%BF%E5%8D%8F%E7%9A%84IP%E5%BA%93%E5%88%B0%E5%BA%95%E5%87%86%E4%B8%8D%E4%BD%8F/</guid>
      <description>

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;对于CPM广告，地域定向是非常重要的。广告协会提供了ip库，方便AdNetwork和第三方监控做地域定向。百度默认以ip138做为其&amp;rdquo;IP&amp;rdquo;关键字的第一个入口，我相信ip138的库是比较准的，那广协的IP库和ip138的库差别大吗？&lt;/p&gt;

&lt;h2 id=&#34;数据分析&#34;&gt;数据分析&lt;/h2&gt;

&lt;h3 id=&#34;随机从线上提取10000个ip-并使用广协库解析成对应城市&#34;&gt;随机从线上提取10000个ip，并使用广协库解析成对应城市&lt;/h3&gt;

&lt;p&gt;从某个时段原始Nginx日志中的ip(300W+)随机取10000条
使用pandas&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;csv = pd.read_csv(&#39;/tmp/ip.log&#39;)
np.savetxt(&#39;/tmp/ipSample.log&#39;,csv.sample(10000).values,fmt=&#39;%s&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;根据ip获取ip138对应的城市&#34;&gt;根据ip获取ip138对应的城市&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# get data from web
def get_data(query):
    url = &amp;quot;http://m.ip138.com/ip.asp?ip=&amp;quot; + query
    res = requests.get(url)
    print &#39;get data for &#39;+ query +&#39; result:&#39; + str(res.status_code)
    if(res.status_code != 200):
        return &#39;Failed Response&#39;
    else:
        return parse_html(res.content)

def parse_html(html):
    res = BeautifulSoup(html,&amp;quot;html.parser&amp;quot;).find_all(&amp;quot;p&amp;quot;, &amp;quot;result&amp;quot;)
    result =split(res[0])
    if len(res)&amp;gt;1:
        result=result+&amp;quot;,&amp;quot;+ split(res[1])
    return result

def split(str):
    try:
        splits=str.string.encode(&amp;quot;utf-8&amp;quot;).split(&#39;：&#39;)
        return splits[1]
    except RuntimeError:
        return &#39;Split Error:&#39;+str

with open(&#39;/tmp/ipSample.log&#39;) as source:
    with open(&#39;ip138Result.csv&#39;,&#39;aw&#39;) as result:
        for line in source:
            ip = line.strip(&#39;\n&#39;)
            ipStr=get_data(ip)
            time.sleep(0.1)
            result.write(ip+&#39;|&#39;+ipStr+&#39;\n&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;取全国60强城市-统计两个结果的差距&#34;&gt;取全国60强城市，统计两个结果的差距&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.mnw.cn/news/china/886245.html&#34;&gt;http://www.mnw.cn/news/china/886245.html&lt;/a&gt; 60强城市&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;citys=[&#39;上海&#39;,&#39;北京&#39;,&#39;广州&#39;,&#39;深圳&#39;,&#39;成都&#39;,&#39;重庆&#39;,&#39;杭州&#39;,&#39;南京&#39;,&#39;沈阳&#39;,&#39;苏州&#39;,&#39;天津&#39;,&#39;武汉&#39;,&#39;西安&#39;,&#39;长沙&#39;,&#39;大连&#39;,&#39;济南&#39;,&#39;宁波&#39;,&#39;青岛&#39;,&#39;无锡&#39;,&#39;厦门&#39;,&#39;郑州&#39;,&#39;长春&#39;,&#39;常州&#39;,&#39;哈尔滨&#39;,&#39;福州&#39;,&#39;昆明&#39;,&#39;合肥&#39;,&#39;东莞&#39;,&#39;石家庄&#39;,&#39;呼和浩特&#39;,&#39;南昌&#39;,&#39;温州&#39;,&#39;佛山&#39;,&#39;贵阳&#39;,&#39;南宁&#39;,&#39;海口&#39;,&#39;湖州&#39;,&#39;唐山&#39;,&#39;临沂&#39;,&#39;嘉兴&#39;,&#39;绍兴&#39;,&#39;南通&#39;,&#39;徐州&#39;,&#39;泉州&#39;,&#39;太原&#39;,&#39;烟台&#39;,&#39;乌鲁木齐&#39;,&#39;潍坊&#39;,&#39;珠海&#39;,&#39;洛阳&#39;,&#39;中山&#39;,&#39;兰州&#39;,&#39;金华&#39;,&#39;淮安&#39;,&#39;吉林&#39;,&#39;威海&#39;,&#39;淄博&#39;,&#39;银川&#39;,&#39;扬州&#39;,&#39;芜湖&#39;,&#39;盐城&#39;,&#39;宜昌&#39;,&#39;西宁&#39;,&#39;襄阳&#39;,&#39;绵阳&#39;]
files =[&amp;quot;ip138Result.csv&amp;quot;,&amp;quot;ipResultV2.log&amp;quot;]
import subprocess, datetime, sys
def checkNums(file,city):
    p = subprocess.Popen(&amp;quot;&amp;quot;&amp;quot; cat &amp;quot;&amp;quot;&amp;quot; + file + &amp;quot;&amp;quot;&amp;quot;| grep &amp;quot;&amp;quot;&amp;quot;+ city +&amp;quot;&amp;quot;&amp;quot;  |wc -l &amp;quot;&amp;quot;&amp;quot;, shell=True, stdout=subprocess.PIPE );
    p.wait()
    out = p.stdout.readlines()[0]
    return str(out).strip()

for city in citys:
    result = city + &amp;quot;\t&amp;quot;
    for file in files:
        result = result +checkNums(file,city)+ &amp;quot;\t&amp;quot;
    print  result
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;北京相差比较大，系统库比Ip138库多出一倍&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其他城市 上海 深圳 南京 苏州 大连 宁波等也相差在20%以上。
可以说跟IP138差别还是比较大的。&lt;/p&gt;

&lt;p&gt;但是三线城市比较准。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用SparkSQL替代Hive做ETL</title>
      <link>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E4%BD%BF%E7%94%A8SparkSQL%E6%9B%BF%E4%BB%A3Hive%E5%81%9AETL/</guid>
      <description>

&lt;h2 id=&#34;spark的优势&#34;&gt;Spark的优势&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;性能，比基于Tez的Hive要快&lt;/li&gt;
&lt;li&gt;编程友好：支持更多数据源格式的读和写；由于Spark可以做流处理和即席查询，某种程度上可以复用很多逻辑。&lt;/li&gt;
&lt;li&gt;可以接入Hive-serde表，与无缝与Hive切换&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;可以忽略错误的输入或者格式&#34;&gt;可以忽略错误的输入或者格式&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;spark.sql.files.ignoreCorruptFiles = true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;处理文本(json/csv)可以有三种模式：&lt;code&gt;PERMISSIVE DROPMALFORMED  FAILFAST&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PERMISSIVE: 将数据放到某一列里面&lt;/p&gt;

&lt;p&gt;DROPMALFORMED：丢弃错误记录&lt;/p&gt;

&lt;p&gt;FAILFAST: 抛出异常&lt;/p&gt;

&lt;h3 id=&#34;复杂结构的支持&#34;&gt;复杂结构的支持&lt;/h3&gt;

&lt;p&gt;SELECT EXISTS(values, e -&amp;gt; e &amp;gt; 30) AS v FROM tbl_nested&lt;/p&gt;

&lt;h3 id=&#34;函数&#34;&gt;函数&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;标准函数，UDF，不过UDF对于Spark引擎来说，是一个黑盒，建议不要用&lt;/li&gt;
&lt;li&gt;agg函数&lt;/li&gt;
&lt;li&gt;窗口函数，和hive类似&lt;/li&gt;
&lt;li&gt;rank&lt;/li&gt;
&lt;li&gt;lag&lt;/li&gt;
&lt;li&gt;lead&lt;/li&gt;
&lt;li&gt;其他&lt;/li&gt;
&lt;li&gt;broadcast 利用了broadcast原语做join
```
##貌似不用的话，会优化成用broadcast?
val left = Seq((0, &amp;ldquo;aa&amp;rdquo;), (0, &amp;ldquo;bb&amp;rdquo;)).toDF(&amp;ldquo;id&amp;rdquo;, &amp;ldquo;token&amp;rdquo;).as[(Int, String)]
val right = Seq((&amp;ldquo;aa&amp;rdquo;, 0.99), (&amp;ldquo;bb&amp;rdquo;, 0.57)).toDF(&amp;ldquo;token&amp;rdquo;, &amp;ldquo;prob&amp;rdquo;).as[(String, Double)]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;left.join(broadcast(right), &amp;ldquo;token&amp;rdquo;).explain(extended = true)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### 例子
注意，如果是二维数组来说，从外往内是先列后行。和Java类似`        int[][][] a=new int[2][2][4];  
`对应的是` [[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0]]]  
`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;val df = Seq(1 -&amp;gt; 2).toDF(&amp;ldquo;i&amp;rdquo;, &amp;ldquo;j&amp;rdquo;)
   val query = df.groupBy(&amp;lsquo;i)
     .agg(max(&amp;lsquo;j).as(&amp;ldquo;aggOrdering&amp;rdquo;))
     .orderBy(sum(&amp;lsquo;j))
     .as[(Int, Int)]
   query.collect contains (1, 2) // true&lt;/p&gt;

&lt;p&gt;val df = Seq((1, 1), (-1, 1)).toDF(&amp;ldquo;key&amp;rdquo;, &amp;ldquo;value&amp;rdquo;)
   df.createOrReplaceTempView(&amp;ldquo;src&amp;rdquo;)
   scala&amp;gt; sql(&amp;ldquo;SELECT IF(a &amp;gt; 0, a, 0) FROM (SELECT key a FROM src) temp&amp;rdquo;).show
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |(IF((a &amp;gt; 0), a, 0))|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
   |                  1|
   |                  0|
   +&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&#34;迁移中遇到的问题&#34;&gt;迁移中遇到的问题&lt;/h2&gt;

&lt;h3 id=&#34;兼容问题&#34;&gt;兼容问题&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Hive UDF用到了SimpleDateFormat，Spark中会有问题&lt;/li&gt;
&lt;li&gt;Spark 2.1 不支持MapJoin Hint&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>广告的Tracking(AdNetwork&#39;s Point of view)</title>
      <link>https://lawulu.github.io/post/%E5%B9%BF%E5%91%8A%E7%9A%84Tracking/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E5%B9%BF%E5%91%8A%E7%9A%84Tracking/</guid>
      <description>

&lt;p&gt;&lt;em&gt;&lt;strong&gt;广告新人的一些感想&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在移动互联网烧钱大潮中，（传统来说）有流量变现能力只有广告了。当每一次点击都与钱相关时候，机会和伴随机会而来的混乱就出现了。&lt;/p&gt;

&lt;h2 id=&#34;商机&#34;&gt;商机&lt;/h2&gt;

&lt;p&gt;对于移动强CPA广告来说，最大的痛点就是转化的核算和归因。各种第三方平台（友盟，TalkingData，热云……），各种不同的规范……，能否成立的一家Tracking平台，帮助AdNetwork来统一追踪。甚至有了各家的数据之后（因为各家的为了Tracking，都要把DevieId通过平台透传）：可以收钱做优化，因为平台（从整个宏观数据可以看出）知道哪类流量适合哪类推广；也可以卖数据甚至卖流量。其实类似想统一混乱的公司有Appcoach,Affiliate，mediation。。简单写一下我们用的FC.&lt;/p&gt;

&lt;h3 id=&#34;fc的做法&#34;&gt;FC的做法&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;为每一个AdNetwork建立一个Tracking Link，在Dashboard中配置对应的宏和Postback Url。&lt;/li&gt;
&lt;li&gt;提供报表做到小时级别的统一，方便了不同时区的用户。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;归因是怎么做的&#34;&gt;归因是怎么做的&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;渠道包，因为（国内）Android并没有一个强势如AppStore的（混乱之源之一）应用商店，所以就会有渠道包这种东西。之前公司每次上线Android，有专门的应用分发脚本，给不同渠道的包不一样。这样就可以知道各个渠道的推广情况。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根据设备id归因：对于Android来说国内是IMEI和AndroidId，国外是GAID。iOS比较规范，一般使用IDFA，不过新版iOS用户据说可以关闭广告追踪。详细流程再下面讲。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fingerprint：大的平台会基于设备的零散数据例如ip,ua，通过数据积累，当遇到没有设备id信息的归因问题时，就用这些零散的数据作为唯一标示确认唯一性。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Referrer来源：如果应用商店支持，可以传递参数表明自己的渠道信息。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;tracking流程&#34;&gt;Tracking流程&lt;/h2&gt;

&lt;h3 id=&#34;tracking&#34;&gt;Tracking&lt;/h3&gt;

&lt;p&gt;对AdNetwork来说，由于无法知道是否安装完成，所以要通过第三方的平台来完成计费。所以，Tracking一般分为Click上报和Install的Postback。即当用户要下载一个App时候，AdNetwork马上告诉广告主（或者代表广告主的检测平台），快看，我这有个用户点击了，他要安装了！！然后就开始默默的等待..广告主收到一个安装，就会根据安装时候的信息反过来去找是谁上报的，一般会根据设备的信息（混乱之源之一)来匹配。所以上报时候一定要把设备信息给带上。上报分302让客户端上报和S2S对接两种，按下不表。&lt;/p&gt;

&lt;h3 id=&#34;宏替换&#34;&gt;宏替换&lt;/h3&gt;

&lt;p&gt;由于AdNetwork要对接的第三方平台太多，而各家Tracking时候需要的参数是不一样的。（为什么没有一个广告协会之类规范一下？！），所以就会有宏替换这么一个奇特而又奇妙的东西。举一个例子：&lt;/p&gt;

&lt;p&gt;假如，大家需要一个参数DeviceId，A家需要的上报是&lt;a href=&#34;http://a.com/tracking?did=XX,而B家需要的上报是http://b.com/tracking?device=XX，这时候客户需要让AdNetwork知道应该怎么上报，AdNetwork提供一批可供替换的变量，让客户自己填。例如DeviceId的变量是shebeihao，那么上面两家分别填写：http://a.com/tracking?did={shebeihao}，和http://a.com/tracking?device={shebeihao}，等上报时候，将这些信息都替换正确。等归因完成，Postback时候，也可以走同样的流程。&#34;&gt;http://a.com/tracking?did=XX,而B家需要的上报是http://b.com/tracking?device=XX，这时候客户需要让AdNetwork知道应该怎么上报，AdNetwork提供一批可供替换的变量，让客户自己填。例如DeviceId的变量是shebeihao，那么上面两家分别填写：http://a.com/tracking?did={shebeihao}，和http://a.com/tracking?device={shebeihao}，等上报时候，将这些信息都替换正确。等归因完成，Postback时候，也可以走同样的流程。&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AngularJS初窥</title>
      <link>https://lawulu.github.io/post/AngularJS%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/AngularJS%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;无数人吐槽过前端框架的层出不穷,眼看前端开发要脱离浏览器兼容性的苦海,又要落入框架碎片的深坑.从语言本身到编译到打包到开发框架各种框架层出不穷&amp;hellip;是终端用户需求太多还是前端大牛们野心太大? 不管怎样,报G家大腿选择Angular似乎没什么问题,另一个大腿的Reactive本身不是一个完备的框架.&lt;/p&gt;

&lt;h2 id=&#34;angular简介&#34;&gt;Angular简介&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在Web开发中需要JS做什么？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;与服务器交互&lt;/li&gt;
&lt;li&gt;操作DOM&lt;/li&gt;
&lt;li&gt;简单的逻辑控制&lt;/li&gt;
&lt;li&gt;输入验证&lt;/li&gt;
&lt;li&gt;路由&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Angular做了什么改变？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;引入了指令,ng开头,来扩展HTML&lt;/li&gt;
&lt;li&gt;数据绑定,通过对HTML元素增加指令ng-model&lt;/li&gt;
&lt;li&gt;使用directive来自定义指令&lt;/li&gt;
&lt;li&gt;Scope,提供了HTML视图和JS控制器之间的纽带&lt;/li&gt;
&lt;li&gt;Controller,可以视为一个$Scope的构造函数&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据绑定 即将Dom的变化绑定到数据的变化上面去,因为用的1,所以这个绑定是双向的,有时候想只改变页面,会影响到数据&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;使用一周的感受&#34;&gt;使用一周的感受&lt;/h2&gt;

&lt;h3 id=&#34;一些坑&#34;&gt;一些坑:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;架构很恢弘，学习曲线很陡峭&lt;/li&gt;
&lt;li&gt;http是异步,要在.function中处理,为避免因为异步未完成引起的页面问题，课可以统一在配置里面设置&lt;code&gt;async : true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;怎么Lazy初始化？没有找到办法，可能需要自定义Directive&lt;/li&gt;
&lt;li&gt;一些自定义Directive封装的不彻底，各种问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;controller之间的交互&#34;&gt;Controller之间的交互&lt;/h3&gt;

&lt;p&gt;两种办法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/29467339/how-to-call-a-function-from-another-controller-in-angularjs&#34;&gt;http://stackoverflow.com/questions/29467339/how-to-call-a-function-from-another-controller-in-angularjs&lt;/a&gt;
本质是绑定到RootScope里面,通过事件来响应&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/findsafety/article/details/50403242&#34;&gt;http://blog.csdn.net/findsafety/article/details/50403242&lt;/a&gt;
原生dom与js的处理办法,可能会版本之间不兼容,本质是通过dom找到Ng-controller的方法.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;    var appElement = document.querySelector(&#39;[ng-controller=&amp;quot;EndCardEditCtrl as ctrl&amp;quot;]&#39;);
    var editCtrl = angular.element(appElement);
    
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;与jquery-js之间的交互&#34;&gt;与Jquery/JS之间的交互&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;有时候Jquery绑定Function时候,Dom并没有生成,所以需要替换成Angular的&lt;code&gt;ng-click&lt;/code&gt;事件,或者需要绑定到顶级上面,冒泡绑定.&lt;/li&gt;
&lt;li&gt;JS中可以通过元素来找的其绑定的scope
&lt;code&gt;
window.angular.element(&amp;quot;#divId&amp;quot;).scope().doSth()
&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;angular的分支循环&#34;&gt;Angular的分支循环&lt;/h3&gt;

&lt;p&gt;Angular提供了Ng-show等逻辑判断,来控制页面的变化.
提供了Switch和For来做分支和循环.
简单来说,基本可以对Html的显示进行编程.&lt;/p&gt;

&lt;h3 id=&#34;scope-apply&#34;&gt;$scope.$apply()&lt;/h3&gt;

&lt;p&gt;点击按钮之后,为什么会绑定成功?奥秘就在于Angular把&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Events =&amp;gt; ng-click
Timeouts =&amp;gt; $timeout
jQuery.ajax() =&amp;gt; $http
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都Wrapper了,只要这些事件被触发,自动调用&lt;code&gt;$scope.$apply()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;问题是很多时候(在$scope之外的操作)需要自己手动调用, 但是Angular又保证此方法不能调用两次.
所以官方推荐使用Angular自己那套($Resource,$Route,service之类&amp;hellip;).恩,有种上了贼船的感觉.&lt;/p&gt;

&lt;h3 id=&#34;resource&#34;&gt;$Resource&lt;/h3&gt;

&lt;p&gt;一个对HttpRest的封装,简单的CRUD确实很方便,但是遇到复杂的需求就要抓瞎,很多时候需要调整后端..所以有很多人说,Angular适合做简单的CRUD应用.&lt;/p&gt;

&lt;h3 id=&#34;ng-change&#34;&gt;Ng-change&lt;/h3&gt;

&lt;p&gt;ng-change requires ng-model, without it will not work&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: [$compile:ctreq] http://errors.angularjs.org/1.4.5/$compile/ctreq?p0=ngModel&amp;amp;p1=ngChange

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外 ng-change对File不生效&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/angular/angular.js/issues/1375&#34;&gt;https://github.com/angular/angular.js/issues/1375&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;单元测试&#34;&gt;单元测试&lt;/h3&gt;

&lt;p&gt;Angular轮了一套Service,Factory之流,自然也少不了单元测试..不知道有多少人会为页面写这玩意.&lt;/p&gt;

&lt;h2 id=&#34;angular自定义directive和ng-repeat的问题&#34;&gt;Angular自定义Directive和Ng-repeat的问题&lt;/h2&gt;

&lt;h3 id=&#34;现象&#34;&gt;现象&lt;/h3&gt;

&lt;p&gt;同事自定义了一个&lt;code&gt;directive&lt;/code&gt;，模仿&lt;a href=&#34;http://www.cnblogs.com/e50000/p/5663806.html&#34;&gt;这里&lt;/a&gt;做的一个日期控件。这个&lt;code&gt;directive&lt;/code&gt;需要input域里面含有一个id。另外一个同事想在 &lt;code&gt;np-repeat&lt;/code&gt;里面使用这个控件，为了保证id唯一性，写成了这样，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &amp;lt;input type=&amp;quot;text&amp;quot; ng-model=&amp;quot;statDatas[$index].endDate&amp;quot; class=&amp;quot;overviews_date&amp;quot; placeholder=&amp;quot;结束时间&amp;quot; &amp;quot;id=&amp;quot;{{&#39;startDateInput&#39;+$index}}&amp;quot;  ng-change=&amp;quot;statDateChanged($index)&amp;quot; readonly  def-laydate&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果一直报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TypeError: Cannot read property &#39;tagName&#39; of undefined
    at Object.Dates.run (http://localhost:8081/resources/js/laydate.js:171:42)
    at win.laydate (http://localhost:8081/resources/js/laydate.js:34:11)
    at link (http://localhost:8081/resources/platformJs/ad_angular.js:592:20)
    at http://localhost:8081/resources/angular/angular.min.js:72:493
    at $ (http://localhost:8081/resources/angular/angular.min.js:73:46)
    at L (http://localhost:8081/resources/angular/angular.min.js:61:495)
    at g (http://localhost:8081/resources/angular/angular.min.js:54:326)
    at g (http://localhost:8081/resources/angular/angular.min.js:54:349)
    at g (http://localhost:8081/resources/angular/angular.min.js:54:349)
    at g (http://localhost:8081/resources/angular/angular.min.js:54:349) &amp;lt;input type=&amp;quot;text&amp;quot; ng-model=&amp;quot;statDatas[$index].startDate&amp;quot; class=&amp;quot;overviews_date ng-pristine ng-untouched ng-valid ng-isolate-scope&amp;quot; placeholder=&amp;quot;开始时间&amp;quot; id=&amp;quot;{{&#39;startDateInput&#39;+$index}}&amp;quot; ng-change=&amp;quot;statDateChanged($index,ad.id)&amp;quot; readonly=&amp;quot;&amp;quot; def-laydate=&amp;quot;&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分析&#34;&gt;分析&lt;/h3&gt;

&lt;p&gt;一开始就怀疑，是因为执行这个&lt;code&gt;directive&lt;/code&gt;时候&lt;code&gt;np-repeat&lt;/code&gt;还没有执行完，即这个dom还没有渲染完毕。那怎么解决呢？
1. 监听&lt;code&gt;np-repeat&lt;/code&gt;,等结束之后，再去动态的执行自定义&lt;code&gt;directive&lt;/code&gt;。例如这篇文章里面：&lt;a href=&#34;http://www.tuicool.com/articles/Fb2um2e&#34;&gt;利用angular指令监听ng-repeat渲染完成后执行脚本&lt;/a&gt;
2. 貌似有个Priority的属性，然而，搜了一下&lt;code&gt;np-repeat&lt;/code&gt;的Priority非常大，为1000，而自定义的priority为0.
3. 仔细看报错日志，感觉像是laydate里面拿到的document是angular渲染之前的，所以找不到对应的Input域。这时候搜索已经没有意义了，应该仔细弄清楚&lt;code&gt;directive&lt;/code&gt;各行代码的意义。
4. scope的问题？&lt;/p&gt;

&lt;h3 id=&#34;解决办法&#34;&gt;解决办法&lt;/h3&gt;

&lt;p&gt;用原始的值传递和&lt;code&gt;$scope.$apply&lt;/code&gt;来交互数据，弃掉自定义控件&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phantomjs和Casperjs初窥</title>
      <link>https://lawulu.github.io/post/Phantomjs%E5%92%8CCasperjs%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/Phantomjs%E5%92%8CCasperjs%E5%88%9D%E7%AA%A5/</guid>
      <description>

&lt;p&gt;在这个浏览器最大的时代，JS越来越重的时代，做爬虫其实最好的办法是找一个Headless浏览器，必须要支持执行JS，这样做爬虫就不用去分析HTTP和JS了。我大Java有HtmlUnit，最近尝试的是传说中的PhantomJs。话说为什么没有基于Chrome dev tools的爬虫框架呢，或许可以尝试下油猴子。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;PhantomJs提供了最原始的API，Casperjs是对PhantomJs的包装，最方便的是不用不停的写if/else（成功了执行下一步），而是用then/wait等方法增加代码的可读性和可写性。不过，Casperjs官方文档直接给出，遇到问题，应该使用PhantomJs的方式先去调用，有问题也要去PhantomJs那里去提…所以，看文档还是直接上PhantomJs吧。&lt;/p&gt;

&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;

&lt;p&gt;casperjs提供了&lt;code&gt;echo&lt;/code&gt;、&lt;code&gt;waitfor&lt;/code&gt;、&lt;code&gt;click&lt;/code&gt;等原语，所以代码大概是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;casper.thenClick(&#39;li.pg_next&#39;);

casper.then(function () {
    this.waitForSelector(&#39;li.pg_next&#39;,function () {
         this.echo(this.getHTML(&#39;.post-list&#39;));
    });
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比较苦逼的是，只有通过文件和console来跟外界交互…casperjs提供了一个server示例，直接标注说有内存溢出风险，phantomjs直接把webserver标注为不可在生产上用。&lt;/p&gt;

&lt;h2 id=&#34;debug&#34;&gt;Debug&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;casperjs --ignore-ssl-errors=yes --ssl-protocol=any  sample.js  --remote-debugger-port=9000 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候，可以在Safari里面打开，在Scripts里面设置断点，就可以Debug了。看似简单，但是很多坑在里面。&lt;/p&gt;

&lt;p&gt;首先，要在console里面执行__run()，但是如果&lt;code&gt;sample.js&lt;/code&gt;如果太复杂的话，会直接把PhantomJs搞Crash。所以可以在代码里面加上&lt;code&gt;Debugger&lt;/code&gt;，这样直接能跳过Crash的地方。
另外，PhantomJs是基于Webkit的，Chrome虽然有Webkit血统，但是只使用了起渲染HTML和CSS这一层，自己搞了一套V8去执行JS，所以，最新版本的Chrome是无法Debug PhantomJs的，最好使用血缘更近的Safari。另外再吐槽一下，PhantomJs的Bug真多，动不动就Crash。&lt;/p&gt;

&lt;p&gt;而且Console实在是不方便，所以，我觉得，最好的Debug还是用capture截图和多用log输出，例如监听资源。&lt;/p&gt;

&lt;h2 id=&#34;监听资源&#34;&gt;监听资源&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; var utils = require(&#39;utils&#39;);

casper.options.onResourceReceived = function(casperObject, res) {
     var contentType = res.contentType;
     if(contentType.indexOf(&amp;quot;json&amp;quot;)!=-1){
        console.log(utils.dump(res));

     }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以打印出所有header为&lt;code&gt;application/json&lt;/code&gt;的请求，然而没什么卵用，因为一般来说，body都被Gzip了，这里比较适合的是打印出url。&lt;/p&gt;

&lt;p&gt;另外一个坑是，PhantomJs退出时候，可能有很多Ajax请求还没有完成，所以这里要手动Wait……,不然很多请求抓不到，再所以，PhantomJs抓起来就比较慢，适合一些临时的小任务。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>闲言碎语·老人与孩</title>
      <link>https://lawulu.github.io/post/%E8%80%81%E4%BA%BA%E4%B8%8E%E5%AD%A9/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E8%80%81%E4%BA%BA%E4%B8%8E%E5%AD%A9/</guid>
      <description>&lt;p&gt;国庆前打车从西站回家，路遇一个炒房大叔司机，一路叨叨他们去河北买房的事情，中间有句话印象很深，是说和儿子商量怎么理财的事情：老人有老人的经验，年轻人有年轻人的闯劲。&lt;/p&gt;

&lt;p&gt;我对本地人最羡慕的一点就是，一直生活在生活的传承之下。不仅是生活的一些小窍门，还有很多为人处世的言传身教。&lt;/p&gt;

&lt;p&gt;这么多年没有和父母在一起生活，不管是生活理念还是人生观，因为缺少磨合和沟通，差距很大，如果按照一直以来连岳的说法，不要试图去改变他们。之前去郑州，父亲一直说我们办事情考虑问题不全面，没有把生孩子这件事给重视起来，万一某个环节出了问题，要后悔终生的。&lt;/p&gt;

&lt;p&gt;我想起了另一件事，儿子刚出生时候，因为住院，睡成了偏头，一个耳朵有点招风，然后又折腾母乳过敏，所以招风耳这事，一直没有太重视。岳母曾经说过几次说要用胶布粘住，妻子觉得会把皮肤粘疼，也向我转述过这种想法，我下意识的把这事当成了笑话来听，说不用，招风耳就当听逆言了。直到上次因为偏头去保健科（去晚了），才听说耳朵长不回来了，只有月子里面粘住耳朵有用，才开始着急，力图让妻子赶紧淘宝买一个。因为这事，差点吵了一架。&lt;/p&gt;

&lt;p&gt;另外，母亲最近在北京看孩子，向我展示了她的耳朵，我才知道她小时候耳朵也被压了，两个耳朵不对称。同样的教训竟然发生在一个家庭两代人身上！之前因为为尊者讳的原因，我从来不知道！幸好，现在头发长了，基本看不出来。&lt;/p&gt;

&lt;p&gt;我相信网络和权威（医院），但是网络和权威并没有给一个完全性的指南。遇上问题再去搜索，肯定要晚了。&lt;/p&gt;

&lt;p&gt;现在父母在这里看孩子，下意识的还是觉得父母的做法有问题，不如网上随便一个论坛里面的只言片语。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>知乎回答·河北高考分数高意味着什么</title>
      <link>https://lawulu.github.io/post/%E6%B2%B3%E5%8C%97%E9%AB%98%E8%80%83%E5%88%86%E6%95%B0%E7%BA%BF/</link>
      <pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/%E6%B2%B3%E5%8C%97%E9%AB%98%E8%80%83%E5%88%86%E6%95%B0%E7%BA%BF/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/31762126/answer/107369501&#34;&gt;https://www.zhihu.com/question/31762126/answer/107369501&lt;/a&gt;
利益相关:十几年前的河南考生&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;河南山西江西的考生的应试能力差不多,河北考生的应试能力完全碾压其他几省. 但这事是河北考生的悲哀而不是骄傲.&lt;/p&gt;

&lt;p&gt;上面很多答案已经说了是衡水中学的影响, 有兴趣的可以研究一下衡水中学跟河北整体分数的相关性.
我上学时候,貌似从来没有听说过河北考生的分数这么高,大概是衡水中学起来之后,河北高考分数越来越变态的.2016年河北700分以上达到了恐怖的168人,比去年多了90+..&lt;/p&gt;

&lt;p&gt;这对高中生来说是好事还是坏事?&lt;/p&gt;

&lt;p&gt;可能是好事,&lt;strong&gt;如果通过高中的训练,能有能力把一件事做的比全国其他同龄人更好,或许以后人生在其他竞争中应该更容易胜出.&lt;/strong&gt;
但是,我觉得肯定大部分家长不愿意自己孩子在高强度压力和竞争之下,需要比全国其他同龄人花费更多的时间和精力去拼命学习,去刷题(天赋一般的话理科考高分基本靠刷题),去练字(可以搜下衡水中学的语文英语作文字体). 而对于没有进入超级中学,或者进入超级中学而高考失败的考生,更是残酷.&lt;/p&gt;

&lt;p&gt;说说我老家的情况,豫南某县城. 县城两所高中,一个重点一个普通,我高考时候重点高中班级前十基本可以进一本线,而现在一个班前三名才有可能进一本线.
跟我们那时候比,一个是生源差了,初中按片升学,没有重点初中,中考招生,教委要求重点高中在某个分数线之后需要和普通高中轮流录取,换句话来说,重点高中不重点了,想想90年代时候,母校每个年级还有重点班,也被教委给取消了;另外就是很多好老师都去大城市了,也招不来好老师了.事实上,跨县高中之间竞争不大,升学率跟校长和县教委政绩挂钩不大,教委和校长没有动力去高压学生.&lt;/p&gt;

&lt;p&gt;高考不公平,单纯讨论分数没有意义. 因为各省市的情况不一样. 假如全国一套卷子一个录取线,相信我,不出三年,帝都人大附学生的考分绝对不比衡水差. 横向比较一本率(考虑到很多省内211招取了很多本地考生,或许用985录取率)才有意义.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML-决策树和回归</title>
      <link>https://lawulu.github.io/post/ML-%E5%9B%9E%E5%BD%92%E5%92%8C%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://lawulu.github.io/post/ML-%E5%9B%9E%E5%BD%92%E5%92%8C%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>

&lt;h2 id=&#34;决策树&#34;&gt;决策树&lt;/h2&gt;

&lt;h3 id=&#34;理论基础&#34;&gt;理论基础&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;信息熵：-p(x)*lnp(x)的和,如果信息熵高的话，说明不稳定，不确定性大。例如一个0.5vs0.5的硬币和一个0.99vs0.01的硬币。&lt;/li&gt;
&lt;li&gt;条件熵：H(Y|X)=&lt;/li&gt;
&lt;li&gt;信息增益：互信息&lt;/li&gt;
&lt;li&gt;信息增益率&lt;/li&gt;
&lt;li&gt;基尼系数（两个定义）
ID3,C4.5,CART&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;建立决策树，核心的问题是如何选择特征值作为根节点？
找到不确定性降低最快的，即构建一个信息熵下降最快的树&lt;/p&gt;

&lt;p&gt;决策树的评价：&lt;/p&gt;

&lt;p&gt;随机森林，对M个样本，是随机选出N个样本，并且放回，构造L颗树，然后投票选出分类，这是一种集中的思想，每一个树不完美，然后一起就很完美。（分类器不一定用决策树，也可能用SVM、LR之类，但是效果不好，因为是强分类器，对噪声比较敏感）
事实上，回归也可以使用集中这种精神。&lt;/p&gt;

&lt;p&gt;也有可能是在特征上加入了随机性，只取某些特征。降低了某些特征的影响。&lt;/p&gt;

&lt;p&gt;怎么确定树的个数？
这个是一个超参数，和lambda也一样，拍脑袋，然后验证。
很多实践中，N=M,因为有1-1/e的概率会重复，会去掉捣乱分子，然后防止过拟合。各个大学的特征脸，加完除以n之后就会把不太好的特征给抹掉&lt;/p&gt;

&lt;p&gt;平方根大法，每次平方根个样本&lt;/p&gt;

&lt;h3 id=&#34;决策树也可以用来拟合&#34;&gt;决策树也可以用来拟合&lt;/h3&gt;

&lt;p&gt;即每一段当做一个连续值&lt;/p&gt;

&lt;p&gt;投票机制，可以加权。
例如，电影加权评分调节冷门和热门比赛。
拉普拉斯平滑，例如中韩比赛前四次都输了，计算最新比赛的胜率，本来应该是0/4，但是为了平滑，按照1/4+1来算。&lt;/p&gt;

&lt;h2 id=&#34;回归&#34;&gt;回归&lt;/h2&gt;

&lt;h3 id=&#34;数学理论&#34;&gt;数学理论&lt;/h3&gt;

&lt;h4 id=&#34;中心极限定理&#34;&gt;中心极限定理&lt;/h4&gt;

&lt;h4 id=&#34;e的含义&#34;&gt;e的含义&lt;/h4&gt;

&lt;p&gt;a的x次方，只有a=e时候的导数是一致的，对应现实是利息，利息将时间考虑了进来。&lt;/p&gt;

&lt;h3 id=&#34;回归分析&#34;&gt;回归分析&lt;/h3&gt;

&lt;h4 id=&#34;目标函数&#34;&gt;目标函数&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;假设误差是IID高斯分布，对真个样本空间来说，观察到所有样本的联合分布是所有的误差分布乘起来，对之求对数然后求导，得到均方误差公式，某种意义上来说，样本的因变量Y也是符合高斯分布的。&lt;/li&gt;
&lt;li&gt;更一般意义的，均方误差公式其实对应的是欧氏距离。&lt;/li&gt;
&lt;li&gt;均方误差公式可以写成矩阵相乘的形式，θX-Y&lt;/li&gt;
&lt;li&gt;扰动，就类似于θ的值非常大的话，函数在样本空间之外的空间就可能出现过拟合，所以要防止θ过大，增加一个lamdaI矩阵，这个lamda叫做超常数，可以是指定的，例如=0.0001,根据训练数据&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>